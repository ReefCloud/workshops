[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ReefCloud R/Statistics resources",
    "section": "",
    "text": "This tutorial series should be considered reference and background materials in support of preparation for more advanced statistical analyses with R. The topics covered progress from introductory and foundational through to more advanced and as such, the tutorials are intended to be consumed and worked through largely in the order laid out in the sidebar menu to the left.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#r-code",
    "href": "index.html#r-code",
    "title": "ReefCloud R/Statistics resources",
    "section": "2.1 R code",
    "text": "2.1 R code\nThroughout the tutorials, R code snippets will be presented in a block such as:\n\n## construct variables\nx &lt;- rep(1:5, times = 2)\ng &lt;- gl(n = 2, k = 5, labels = c('High', 'Low'))\ny &lt;- 2 + (3 * x) + (as.numeric(g) - 1) + rnorm(10, mean = 0, sd = 1)\n## compile into a data frame\ndat &lt;- data.frame(x, g, y)\n## print out data frame\ndat\n\n   x    g         y\n1  1 High  6.652339\n2  2 High  7.452546\n3  3 High 11.906414\n4  4 High 15.053998\n5  5 High 15.811673\n6  1  Low  6.872689\n7  2  Low  8.971635\n8  3  Low 10.230413\n9  4  Low 14.336794\n10 5  Low 17.082725\n\n\nThis format is partially reminiscent of the layout of code editors, albeit with a very opinionated color scheme.\n\nthe R code appears as the text over the faint yellow background.\nany output appears below the code (white background) and in red font\nthe light gray numbers in the left hand gutter represent the line numbers. These can be useful when trying to draw attention to a particular line of code\nthe light gray text in the R code block beginning with two hash symbols ‘##’ are comments\nin the upper right hand corner there is a clipboard symbol. Clicking on this symbol will copy the code to the clipboard to help you transfer the code to your own R session.\n\nOccasionally (and particularly within tables) code snippets may alternatively be presented without the line number gutter. In such cases, there tends to only be a single line of code and there are substantial space savings if the gutter is removed.\n\nx &lt;- rep(1:5, times = 2)",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#bash-code",
    "href": "index.html#bash-code",
    "title": "ReefCloud R/Statistics resources",
    "section": "2.2 Bash code",
    "text": "2.2 Bash code"
  },
  {
    "objectID": "index.html#plain-text",
    "href": "index.html#plain-text",
    "title": "ReefCloud R/Statistics resources",
    "section": "2.3 Plain text",
    "text": "2.3 Plain text\nThe contents of plain text files will similarly be presented either as:\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit. \nMaecenas sed metus congue risus sagittis viverra. Etiam \nhendrerit orci arcu, et vehicula libero vulputate nec. \nAliquam placerat lacinia ex sit amet varius. Suspendisse \npotenti. Nam tristique fringilla lacus id tincidunt. Donec \nquis turpis tempus leo pharetra malesuada. Vivamus consequat \na quam nec vestibulum.\n\nor when necessary to save space:\n\nLorem ipsum dolor sit amet, consectetur adipiscing elit.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "01_introduction_to_r.html",
    "href": "01_introduction_to_r.html",
    "title": "Introduction to R",
    "section": "",
    "text": "The latest version of an R installation binary (or source code) can be downloaded from one of the Comprehensive R Archive Network (or CRAN) mirrors. Having selected one of the (Australian) mirrors, follow one of the sets of instructions below (depending on your operating system).\n\nWindowsMacOSxLinux\n\n\n\nDownload R:\n\nGo to the CRAN R-project website https://cran.r-project.org/ and click on “Download R for Windows”.\nSelect the “base” subdirectory\nSelect the “Download R-X.X.X for Windows” option (where X.X.X are a series of version and release numbers) to download.\n\nRun the installer: Double-click the downloaded .exe file and follow the installation wizard. Accept the default settings unless you have specific needs.\nOptional: Set R as the default: Check the checkbox to set R as the default for R scripts during installation. This allows you to run R scripts by double-clicking them.\nVerify installation:\n\nOpen a new command prompt (Start &gt; Run &gt; cmd) and type R. If the R console opens, the installation was successful.\nAlternatively, search for R in the Start menu\n\n\n\n\n\nDownload R:\n\nGo to the CRAN R-project website (https://cran.r-project.org/) and click on “Download R for macOS”.\nChoose the latest stable version that is appropriate for your architecture.\n\nOpen the disk image: Double-click the downloaded .pkg file and drag the R application icon to your Applications folder.\nVerify installation:\n\nOpen Terminal: Go to Applications &gt; Utilities and open Terminal.\nType R in the Terminal window. If the R console opens, the installation was successful.\n\n\n\n\n\nOpen Terminal: You can access Terminal through your application launcher or search bar.\nInstall R: The commands vary slightly depending on your Linux distribution. Here are common examples:\n\nDebian/Ubuntu: sudo apt install r-base\nFedora/CentOS: sudo yum install R\nArch Linux: sudo pacman -S R\n\nVerify installation: Type R in the Terminal window. If the R console opens, the installation was successful.",
    "crumbs": [
      "R basics",
      "Introduction to R"
    ]
  },
  {
    "objectID": "01_introduction_to_r.html#the-r-environment-and-command-line",
    "href": "01_introduction_to_r.html#the-r-environment-and-command-line",
    "title": "Introduction to R",
    "section": "2.1 The R environment and command line",
    "text": "2.1 The R environment and command line\nUpon opening R, you are presented with the R Console along with the command prompt (&gt;). R is a command driven application (as opposed to a ‘point-and-click’ application) and despite the steep learning curve, there are many very good reasons for this.\nCommands that you type are evaluated once the Enter key has been pressed\nEnter the following command (5+1) at the command prompt (&gt;);\n\n5+1\n\n[1] 6\n\n\n\n\n\n\n\n\nNote\n\n\n\nI have suppressed the command prompt (&lt;) from almost all code blocks throughout these workshop and tutorial series to make it easier for you to cut and paste code into your own scripts or directly into R.\n\n\n\n\n\n\n\n\nTip\n\n\n\nIn this tutorial series, the R code to be entered appears to the right hand side of the vertical bar. The number of the left side of the bar is a line number. For single line code snippets, such as the example above, line numbers are not necessary. However, for multi-line code snippets, line numbers help for identifying and describing different parts of the code.\n\n\nThe above R code evaluates the command five plus one and returns the result (six).. The [1] before the 6 indicates that the object immediately to its right is the first element in the returned object. In this case there is only one object returned. However, when a large set of objects (e.g. numbers) are returned, each row will start with an index number thereby making it easier to count through the elements.\n\n\n\n\n\n\nImportant definitions\n\n\n\n\n\n\nObject\n\nAs an object oriented language, everything in R is an object. Data, functions even output are objects.\n\nVector\n\nA collection of one or more objects of the same type (e.g. all numbers or all characters).\n\nFunction\n\nA set of instructions carried out on one or more objects. Functions are typically wrappers for a sequence of instructions that perform specific and common tasks.\n\nParameter\n\nThe kind of information passed to a function.\n\nArgument\n\nThe specific information passed to a function.\n\nOperator\n\nA symbol that has a pre-defined meaning. Familiar operators include + - * and /.\n\nAssignment operators\n\n&lt;- Assigning a name to an object (left to right)\n\n\n-&gt; Assigning a name to an object (right to left)\n\n\n= Used when defining and specifying function arguments\n\nLogical operators (return TRUE or FALSE)\n\n&lt; Less than\n\n\n&gt; Greater than\n\n\n&lt;= Less than or equal\n\n\n&gt;= Greater than or equal\n\n\n== Is the left hand side equal to the right hand side (a query)\n\n\n!= Is the left hand side NOT equal to the right hand side (a query)\n\n\n&& Are BOTH left hand and right hand conditions TRUE\n\n\n|| Are EITHER the left hand OR right hand conditions TRUE\n\nPipe operator\n\n|&gt; piping the output of one operation to the input of the next\n\n\n\n\n\n\n2.1.1 Expressions, Assignment and Arithmetic\nInstead of evaluating a statement and printing the result directly to the console, the results of evaluations can be stored in an object via a process called ‘Assignment’. Assignment assigns a name to an object and stores the result of an evaluation in that object. The contents of an object can be viewed (printed) by typing the name of the object at the command prompt and hitting Enter.\n\nvar1 &lt;- 2 + 3\nvar1\n\n[1] 5\n\n\nOn line 1 above, the name var1 was assigned to the result of the sum of 2 and 3. On line 2, the contents of this object are printed to the screen.\nA single command (statement) can spread over multiple lines. If the Enter key is pressed before R considers the statement complete, the next line in the console will begin with the prompt + indicating that the statement is not complete. For this example, I will include the command prompt in order to demonstrate the above point.\n\n&gt; var2 &lt;-\n+   2 + 3\n&gt; var2\n\n[1] 5\n\n\nWhen the contents of an object are numbers, standard arithmetic applies;\n\nvar2 - 1\n\n[1] 4\n\nans1 &lt;- var1 * var2\nans1\n\n[1] 25\n\n\n\n\n\n\n\n\nTip\n\n\n\nGenerally, spaces are ignored in R. Hence, the above and the following are both equally valid.\n\nans1&lt;-var1*var2\nans1\n\n[1] 25\n\n\nNevertheless, the former version (with spaces) is much more readable.\n\n\nCompatible objects can be concatenated (joined together) to create objects with multiple entries. Object concatenation can be performed using the c() function.\n\nc(1, 2, 6)\n\n[1] 1 2 6\n\nc(var1, var2)\n\n[1] 5 5\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn both examples above, objects were not assigned names. As a result, the expressions were evaluated and directly printed to the consol without being stored in any way. Doing so is useful for experimenting, however as the results are not stored, they cannot be used in subsequent actions.\n\n\nIn addition to the typical addition, subtraction, multiplication and division operators, there are a number of special operators, the simplest of which are the quotient or integer divide operator (%/%) and the remainder or modulus operator (%%).\n\n7 / 3\n\n[1] 2.333333\n\n7 %/% 3\n\n[1] 2\n\n7 %% 3\n\n[1] 1\n\n\n\n\n2.1.2 Operator precedence\nThe rules of operator precedence are listed (highest to lowest) in the following table. Additionally, expressions within parentheses ‘()’ always have highest precedence.\n\n\n\n\n\n\n\nOperator\nDescription\n\n\n\n\n[ [[\nindexing\n\n\n::\nnamespace\n\n\n$\ncomponent\n\n\n^\nexponentiation (evaluated right to left)\n\n\n-\n+ sign (unary)\n\n\n:\nsequence\n\n\n%special%\nspecial operators (e.g. %/%, %%, %*%, %in%)\n\n\n* /\nmultiplication and division\n\n\n+\n- addition and subtraction\n\n\n&gt; &lt; &gt;= &lt;= == !=\nordering and comparison\n\n\n!\nlogical negation (not)\n\n\n& &&\nlogical AND\n\n\n| ||\nlogical OR\n\n\n~\nformula\n\n\n-&gt; -&gt;&gt;\nassignment (left to right)\n\n\n=\nargument assignment (right to left)\n\n\n&lt;- &lt;&lt;-\nassignment (right to left)\n\n\n?\nhelp\n\n\n\n\n\n2.1.3 Command history\nEach time a command is entered at the R command prompt, the command is also added to a list known as the command history. The up and down arrow keys scroll backward and forward respectively through the session’s command history list and place the top most command at the current R command prompt. Scrolling through the command history enables previous commands to be rapidly re-executed, reviewed or modified and executed.\n\n\n2.1.4 Object names\nEverything created within R are objects. Objects are programming constructs that not only store values (the visible part of an object), they also define other properties of the object (such as the type of information contained in the object) and sometimes they also define certain routines that can be used to store, retrieve and manipulate data within the object.\nImportantly, all objects within R must have unique names to which they can be referred. Names given to any object in R can comprise virtually any sequence of letters and numbers providing that the following rules are adhered to:\n\nNames must begin with a letter (names beginning with numbers or operators are not permitted)\nNames cannot contain the following characters; space , - + * / # % & [ ] { } ( ) ~\n\nWhilst the above rules are necessary, the following naming conventions are also recommended:\n\nonly use lowercase letters and numbers\nuse underscores (_) to separate words (e.g. snake case)\ntry to use names that are both concise and meaningful.\n\nnames should reflect the content of the object. One of the powerful features of R is that there is virtually no limit to the number of objects (variables, datasets, results, models, etc) that can be in use at a time. However, without careful name management, objects can rapidly become misplaced or ambiguous. Therefore, the name of an object should reflect what it is, and what has happened to it. For example, the name log_fish_wts might be given to an object that contains log transformed fish weights. Moreover, many prefer to prefix the object name with a lowercase letter that denotes the type of data containing in the object. For example, d_mean_head_length might indicate that the object contains the mean head lengths stored as a double floating point (real numbers).\nalthough there are no restrictions on the length of names, shorter names are quicker to type and provide less scope for typographical errors and are therefore recommended (of course within the restrictions of the point above).\n\nwhere possible, avoid using names of common predefined functions and variables as this can provide a source of confusion for both you and R. For example, to represent the mean of a head length variable, use something like mean_head_length rather than mean (which is the name of a predefined function within R that calculates the mean of a set of numbers).",
    "crumbs": [
      "R basics",
      "Introduction to R"
    ]
  },
  {
    "objectID": "01_introduction_to_r.html#r-sessions-and-workspaces",
    "href": "01_introduction_to_r.html#r-sessions-and-workspaces",
    "title": "Introduction to R",
    "section": "2.2 R Sessions and Workspaces",
    "text": "2.2 R Sessions and Workspaces\nA number of objects have been created in the current session (a session encapsulates all the activity since the current instance of the R application was started). To review the names of all of the objects in the users current workspace (storage of user created objects);\n\nls()\n\n[1] \"ans1\" \"var1\" \"var2\"\n\n\nYou can also refine the scope of the ls() function to search for object names that match a pattern:\n\nls(pat = \"var\")\n\n[1] \"var1\" \"var2\"\n\nls(pat = \"a*1\")\n\n[1] \"ans1\" \"var1\"\n\n\nThe longer the session is running, the more objects will be created resulting in a very cluttered workspace. Unneeded objects can be removed using the rm() function. The rm() function only performs a side effect (deletes objects), if the function succeeds, it does not return any output. If it does return anything, it will be a warning or error.\n\nrm(var1, var2)   #remove the VAR1 and VAR2 objects\nrm(list = ls())  #remove all user defined objects\n\n\n\n\n\n\n\nNote\n\n\n\nIn the above examples, comments were appended to each line of code. Comments begin with a hash (#) character. Anything that follows a hash character will be ignored (until the end of the line).\nComments provide a convenient way to annotate your code so as to provide more explanation and clarity as to the intention and purpose of the associated code.\n\n\n\n2.2.1 Current working directory\nThe R working directory (location from which files/data are read and written) is by default, either the location of the R executable (or execution path in Linux) or the users home directory. The current working directory can be reviewed and changed (for the session) using the getwd() function and setwd() functions respectively. Note that R uses the Unix/Linux style directory subdivision markers. That is, R uses the forward slash / in path names rather than the regular \\ of Windows.\nWhen using setwd(), you can provide either an absolute path (the full path) or a relative path (relative to the current location). Obviously, you will get a different result to me when you issue the following:\n\ngetwd()                    #review the current working directory\n\n[1] \"/home/runner/work/workshops/workshops/tut\"\n\nsetwd(\"../\")               #change to the parent directory of the current working directory\nlist.files(path = getwd()) #list all files (and directories) in the current working directory\n\n[1] \"architects_daughter.zip\" \"data\"                   \n[3] \"docs\"                    \"inconsolata.zip\"        \n[5] \"Makefile\"                \"noto_sans.zip\"          \n[7] \"R\"                       \"tut\"                    \n\n\n\n\n2.2.2 Workspaces\nThroughout an R session, all objects (including loaded packages, see Section 6) that have been added are stored within the R global environment, called the workspace. Occasionally, it is desirable to save the workspace and thus all those objects (vectors, functions, etc) that were in use during a session so that they are available during subsequent sessions. This can be done using the save.image() function. Note, this will save the workspace to a file called .RData in the current working directory (usually the R startup directory), unless a file (filename and path) is supplied as an argument to the save.image() function. A previously saved workspace can be loaded by providing a full path and filename as an argument to the load() function.\nWhilst saving a workspace image can sometimes be convenient, it can also contribute greatly to organisational problems associated with large numbers of obsolete or undocumented objects. Instead, it is usually better to specifically store each of the objects you know you are going to want to have access to across sessions separately.\n\n\n2.2.3 Quitting elegantly\nTo quit R, issue the following command; Note in Windows and MacOSX, the application can also be terminated using the standard Exiting protocols.\n\nq()\n\nYou will then be asked whether or not you wish to save the current workspace. If you do, enter ‘Y’ otherwise enter ‘N’. Unless you have a very good reason to save the workspace, I would suggest that you do not. A workspace generated in a typical session will have numerous poorly named objects (objects created to temporarily store information whilst testing). Next time R starts, it could (likely will) restore this workspace thereby starting with a cluttered workspace, and becoming a potential source of confusion if you inadvertently refer to an object stored during a previous session. Moreover, if the workspace includes additional extension packages, these packages may also be loaded which will prevent them from being updated (often necessary when installing additional packages that depend on other packages).",
    "crumbs": [
      "R basics",
      "Introduction to R"
    ]
  },
  {
    "objectID": "01_introduction_to_r.html#functions",
    "href": "01_introduction_to_r.html#functions",
    "title": "Introduction to R",
    "section": "2.3 Functions",
    "text": "2.3 Functions\nAs wrappers for collections of commands used together to perform a task, functions provide a convenient way of interacting with all of these commands in sequence. Most functions require one or more inputs (parameters), and while a particular function can have multiple parameters, not all are necessarily required (some could have default values). Parameters are parsed to a function as arguments comprising the name of the parameter, an equals operator and the value of the parameter. Hence, arguments are specified as name/value pairs.\nConsider the seq() function, which generates a sequence of values (a vector) according to the values of the arguments. We can see that the default version of this function has the following definition:\n\nstr(seq.default)\n\nfunction (from = 1, to = 1, by = ((to - from)/(length.out - 1)), length.out = NULL, \n    along.with = NULL, ...)  \n\n\n\nif the seq() function is called without any arguments (e.g. seq()), it will return a single number 1. Using the default arguments for the function, it returns a vector starting at 1 (from   = 1), going up to 1 (to = 1) and thus having a length of 1.\nwe can alter this behavior by specifically providing values for the named arguments. The following generates a sequence of numbers from 2 to 10 incrementing by 1 (default):\n\nseq(from = 2, to = 10)\n\n[1]  2  3  4  5  6  7  8  9 10\n\n\nthe following generates a sequence of numbers from 2 to 10 incrementing by 2:\n\nseq(from = 2, to = 10, by = 2)\n\n[1]  2  4  6  8 10\n\n\nalternatively, instead of manipulating the increment space of the sequence, we could specify the desired length of the sequence:\n\nseq(from = 2, to = 10, length.out = 3)\n\n[1]  2  6 10\n\n\nnamed arguments need not include the full name of the parameter, so long as it is unambiguous which parameter is being referred to. For example, length.out could be shortened to just l since there are no other parameters of this function that start with ‘l’:\n\nseq(from = 2, to = 10, l = 4)\n\n[1]  2.000000  4.666667  7.333333 10.000000\n\n\nparameters can also be specified as unnamed arguments provided they are in the order specified in the function definition. For example to generate a sequence of numbers from 2 to 10 incrementing by 2:\n\nseq(2, 10, 2)\n\n[1]  2  4  6  8 10\n\n\nNote, although permittable, it is more difficult to unambiguously read/interpret the code and could easily be a source of bugs.\nnamed and unnamed arguments can be mixed, just remember the above rules about parameter order and unambiguous names:\n\nseq(2, 10, l = 4)\n\n[1]  2.000000  4.666667  7.333333 10.000000",
    "crumbs": [
      "R basics",
      "Introduction to R"
    ]
  },
  {
    "objectID": "01_introduction_to_r.html#function-overloading-polymorphism",
    "href": "01_introduction_to_r.html#function-overloading-polymorphism",
    "title": "Introduction to R",
    "section": "2.4 Function overloading (polymorphism)",
    "text": "2.4 Function overloading (polymorphism)\nMany routines can be applied to different sorts of data. That is, they are somewhat generic. For example, we could calculate the mean (arithmetic center) of a set of numbers or we could calculate the mean of a set of dates or times. Whilst the calculations in both cases are analogous to one another, they nevertheless differ sufficiently so as to warrant separate functions.\nWe could name the functions that calculate the mean of a set of numbers and the mean of a set of dates as mean_numbers and mean_dates respectively. Unfortunately, as this is a relatively common situation, the number of functions to learn rapidly expands. And from the perspective of writing a function that itself contains such a generic function, we would have to write multiple instances of the function in order to handle all the types of data we might want to accommodate.\nTo simplify the process of applying these generic functions, R provides yet another layer that is responsible for determining which of a series of overloaded functions is likely to be applicable according to the nature of the parameters and data parsed as arguments to the function. To see this in action, type mean followed by hitting the TAB key. The TAB key is used for auto-completion and therefore this procedure lists all the objects that begin with the letters ‘mean’.\n\nmean           mean.Date      mean.default   mean.difftime  mean.POSIXct   mean.POSIXlt\n\nIn addition to an object called mean, there are additional objects that are suffixed as a ‘.’ followed by a data type. In this case, the objects mean.default, mean.Date, mean.POSIXct, mean.POSIXlt and mean.difftime are functions that respectively calculate the mean of a set of numbers, dates, times, times, time and differences. The mean function determines which of the other functions is appropriate for the data parsed and then redirects to that appropriate function. Typically, this means that it is only necessary to remember the one generic function (in this case, mean()) as the specific functions are abstracted away.\n\n# mean of a series of numbers\nmean(c(1, 2, 3, 4))\n\n[1] 2.5\n\n# create a sequence of dates spaced 7 days apart between 29th Feb 2000 and 30th Apr 2000\nsample_dates &lt;- seq(from = as.Date(\"2000-02-29\"), to = as.Date(\"2000-04-30\"), by = \"7 days\")\n# print (view) these dates\nsample_dates\n\n[1] \"2000-02-29\" \"2000-03-07\" \"2000-03-14\" \"2000-03-21\" \"2000-03-28\"\n[6] \"2000-04-04\" \"2000-04-11\" \"2000-04-18\" \"2000-04-25\"\n\n# calculate the mean of these dates\nmean(sample_dates)\n\n[1] \"2000-03-28\"\n\n\nIn the above examples, we called the same function (mean) on both occasions. In the first instance, it was equivalent to calling the mean.default() function and in the second instance the mean.Date() function. Note that the seq() function is similarly overloaded.\nThe above example also illustrates another important behaviour of function arguments. Function calls can be nested within the arguments of other functions and function arguments are evaluated before the function runs. In this way, multiple steps to be truncated together (although for the sake of the codes’ readability and debugging, it is often better to break a problem up into smaller steps).\nIf a function argument itself contains a function (as was the case above with the from = and to = arguments, both of which called the as.Date() function which converts a character string into a date object), the value of the evaluated argument is parsed to the outside function. That is, evaluations are made from the inside to out. The above example, could have been further truncated to;\n\n# calculate the mean of a sequence of dates spaced 7 days apart between 29th Feb 2000 and 30th Apr 2000\nmean(seq(from = as.Date(\"2000-02-29\"), to = as.Date(\"2000-04-30\"), by = \"7 days\"))\n\n[1] \"2000-03-28\"\n\n\n\n2.4.1 The pipe character\nAs we can see from the example above, nested functions can be pretty awkward to read. As of version 4.1, R has had a pipe operator. The concept of piping dates back to the early UNIX days when separate programs were chained (‘piped’) together such that the output of one program became the input of the next and so on. This enabled each program to remain relatively simple, yet by piping sequences of programs together, rather complex results could be achieved.\nSimilarly the R pipe operator (|&gt;) enables nested functions to alternatively be expressed as a chain of functions:\n\n# calculate the mean of a sequence of dates spaced 7 days apart between 29th Feb 2000 and 30th Apr 2000\nseq(from = as.Date(\"2000-02-29\"), to = as.Date(\"2000-04-30\"), by = \"7 days\") |&gt; mean()\n\n[1] \"2000-03-28\"\n\n\nTo maximise code readability, it is good form to keep lines of code short (less than 80 characters). One way to do this is to place a line break after pipe characters. Moreover, a line break after each function argument allows us to have more topical and granular comments.\n\nseq(                              #take sequence of dates\n  from = as.Date(\"2000-02-29\"),   #from the 29th Feb 2000\n  to = as.Date(\"2000-04-30\"),     #to the 30th April 2000\n  by = \"7 days\") |&gt;               #incrementing by 7 days\n  mean()                          #and calculate the mean\n\n[1] \"2000-03-28\"",
    "crumbs": [
      "R basics",
      "Introduction to R"
    ]
  },
  {
    "objectID": "01_introduction_to_r.html#external-functions",
    "href": "01_introduction_to_r.html#external-functions",
    "title": "Introduction to R",
    "section": "2.5 External functions",
    "text": "2.5 External functions\nAs R is a scripting language (rather than a compiled language), it has the potential to be very slow (since syntax checking, machine instruction interpretation, etc must all take place at runtime rather than at compile time). Consequently, many of the functions are actually containers (wrappers) for external code (link libraries) precompiled in either C or Fortran. In this way, the environment can benefit from the flexibility of a scripting language whilst still maintaining most of the speed of a compiled language. Tutorial ? will introduce how to install and load external libraries.",
    "crumbs": [
      "R basics",
      "Introduction to R"
    ]
  },
  {
    "objectID": "01_introduction_to_r.html#vectors",
    "href": "01_introduction_to_r.html#vectors",
    "title": "Introduction to R",
    "section": "4.1 Vectors",
    "text": "4.1 Vectors\nVectors are a collection of one or more entries (values) of the same type (class) and are the basic storage unit in R. Vectors are one-dimensional arrays (have a single dimension - length) and can be thought of as a single column of data. Each entry in a vector has a unique index (like a row number) to enable reference to particular entries in the vector.\n\n4.1.1 Consecutive integers\nTo get a vector of consecutive integers, we can specify an expression of the form &lt;first integer&gt;:&lt;second integer&gt; where &lt;first integer&gt; and &lt;second integer&gt; represent the start and end of the sequence of integers respectively:\n\n5:10\n\n[1]  5  6  7  8  9 10\n\n5:-5\n\n [1]  5  4  3  2  1  0 -1 -2 -3 -4 -5\n\n\n\n\n4.1.2 The c() function\nThe c() function concatenates values together into a vector. To create a vector with the numbers 1, 4, 7, 21:\n\nc(1, 4, 7, 21)\n\n[1]  1  4  7 21\n\n\nAs an example, we could store the temperature recorded at 10 sites:\n\ntemperature &lt;- c(36.1, 30.6, 31, 36.3, 39.9, 6.5, 11.2, 12.8, 9.7, 15.9)\ntemperature\n\n [1] 36.1 30.6 31.0 36.3 39.9  6.5 11.2 12.8  9.7 15.9\n\n\nTo create a vector with the words ‘Fish’, ‘Rock’, ‘Tree’, ‘Git’:\n\nc('Fish', 'Rock', 'Tree', \"Git\")\n\n[1] \"Fish\" \"Rock\" \"Tree\" \"Git\" \n\n\n\n\n4.1.3 Regular or patterned sequences (rep())\nWe have already seen the use of the seq() function to create sequences of entries.\nSequences of repeated entries are supported with the rep() function:\n\nrep(4,5)\n\n[1] 4 4 4 4 4\n\nrep('Fish',5)\n\n[1] \"Fish\" \"Fish\" \"Fish\" \"Fish\" \"Fish\"\n\n\n\n\n4.1.4 The paste() function\nTo create a sequence of quadrat labels we could use the c() function as illustrated above, e.g.\n\nquadrats &lt;- c(\"Q1\",\"Q2\",\"Q3\",\"Q4\",\"Q5\",\"Q6\",\"Q7\",\"Q8\",\"Q9\",\"Q10\")\nquadrats\n\n [1] \"Q1\"  \"Q2\"  \"Q3\"  \"Q4\"  \"Q5\"  \"Q6\"  \"Q7\"  \"Q8\"  \"Q9\"  \"Q10\"\n\n\nA more elegant way of doing this is to use the paste() function:\n\nquadrats &lt;- paste(\"Q\", 1:10, sep = \"\")\nquadrats\n\n [1] \"Q1\"  \"Q2\"  \"Q3\"  \"Q4\"  \"Q5\"  \"Q6\"  \"Q7\"  \"Q8\"  \"Q9\"  \"Q10\"\n\n\nThis can be useful for naming vector elements. For example, we could use the names() function to name the elements of the temperature variable according to the quadrat labels.\n\nnames(temperature) &lt;- quadrats\ntemperature\n\n  Q1   Q2   Q3   Q4   Q5   Q6   Q7   Q8   Q9  Q10 \n36.1 30.6 31.0 36.3 39.9  6.5 11.2 12.8  9.7 15.9 \n\n\nThe paste() function can also be used in conjunction with other functions to generate lists of labels. For example, we could combine a vector in which the letters A, B, C, D and E (generated with the LETTERS constant) are each repeated twice consecutively (using the rep() function) with a vector that contains a 1 and a 2 to produce a character vector that labels sites in which the quadrats may have occurred.\n\nsite &lt;- paste(rep(LETTERS[1:5], each = 2), 1:2, sep = \"\")\nsite\n\n [1] \"A1\" \"A2\" \"B1\" \"B2\" \"C1\" \"C2\" \"D1\" \"D2\" \"E1\" \"E2\"\n\n\nOr, with the use of pipes:\n\nsite &lt;- rep(LETTERS[1:5], each = 2) |&gt;\n  paste(1:2, sep = \"\")\nsite\n\n [1] \"A1\" \"A2\" \"B1\" \"B2\" \"C1\" \"C2\" \"D1\" \"D2\" \"E1\" \"E2\"\n\n\nRather than specify that the components are not separated by any character (which is what we are doing above by indicating that the separator character should be ““), there is a version of paste() that does this automatically. It is paste0().\n\nsite &lt;- rep(LETTERS[1:5], each = 2) |&gt;\n  paste0(1:2)\nsite\n\n [1] \"A1\" \"A2\" \"B1\" \"B2\" \"C1\" \"C2\" \"D1\" \"D2\" \"E1\" \"E2\"\n\n\n\n\n\n\n\n\nMajor vector classes\n\n\n\n\n\n\n\n\n\nVector class\n\n\nExamples\n\n\n\n\n\n\ninteger(whole numbers)\n\n\n\n2:4\n\n[1] 2 3 4\n\nc(1, 3, 9)\n\n[1] 1 3 9\n\n\n\n\n\n\nnumeric(real numbers)\n\n\n\nc(8.4, 2.1)\n\n[1] 8.4 2.1\n\n\n\n\n\n\ncharacter(letters)\n\n\n\nc('A', 'ABC', 'def')\n\n[1] \"A\"   \"ABC\" \"def\"\n\n\n\n\n\n\nlogical(TRUE or FALSE)\n\n\n\n2:4 == 3\n\n[1] FALSE  TRUE FALSE\n\n\n\n\n\n\ndate(dates)\n\n\n\nc(as.Date(\"2000-02-29\"), as.Date(\"29/02/2000\",\"%d/%m/%Y\"))\n\n[1] \"2000-02-29\" \"2000-02-29\"\n\n\n\n\n\n\nPOSIXlt(date/time)\n\n\n\nstrptime('2011-03-27 01:30:00', format='%Y-%m-%d %H:%M:%S')\n\n[1] \"2011-03-27 01:30:00 UTC\"\n\n\n\n\n\n\n\n\n\n\n\n4.1.5 Factors\nFactors are more than a vector of characters. Factors have additional properties that are utilized during statistical analyses and graphical procedures. To illustrate the difference, we will create a vector to represent a categorical variable indicating the level of shading applied to 10 quadrats. Firstly, we will create a character vector:\n\nshade &lt;- rep(c(\"no\", \"full\"), each = 5)\nshade\n\n [1] \"no\"   \"no\"   \"no\"   \"no\"   \"no\"   \"full\" \"full\" \"full\" \"full\" \"full\"\n\n\nNow we convert this into a factor:\n\nshade &lt;- factor(shade)\nshade\n\n [1] no   no   no   no   no   full full full full full\nLevels: full no\n\n\nNotice the additional property (Levels) at the end of the output. Notice also that unless specified otherwise, the levels are ordered alphabetically. Whilst this does not impact on how the data appear in a vector, it does effect some statistical analyses, their interpretations as well as some tabular and graphical displays. If the alphabetical ordering does not reflect the natural order of the data, it is best to reorder the levels whilst defining the factor:\n\nshade &lt;- factor(shade, levels = c(\"no\", \"full\"))\nshade\n\n [1] no   no   no   no   no   full full full full full\nLevels: no full\n\n\nA more convenient way to create a balanced (equal number of replicates) factor is to use the gl() function. To create the shading factor from above:\n\nshade &lt;- gl(n = 2, k = 5, length = 10, labels = c(\"no\", \"full\"))\nshade\n\n [1] no   no   no   no   no   full full full full full\nLevels: no full\n\n\n\n\n4.1.6 Matrices\nMatrices have two dimensions (length and width). The entries (which must be all of the same length and type - class) are in rows and columns.\nWe could arrange the vector of shading into two columns:\n\nmatrix(temperature, nrow = 5)\n\n     [,1] [,2]\n[1,] 36.1  6.5\n[2,] 30.6 11.2\n[3,] 31.0 12.8\n[4,] 36.3  9.7\n[5,] 39.9 15.9\n\n\nSimilarly, We could arrange the vector of shading into two columns:\n\nmatrix(shade, nrow = 5)\n\n     [,1] [,2]  \n[1,] \"no\" \"full\"\n[2,] \"no\" \"full\"\n[3,] \"no\" \"full\"\n[4,] \"no\" \"full\"\n[5,] \"no\" \"full\"\n\n\nAs another example, we could store the X,Y coordinates for five quadrats within a grid. We start by generating separate vectors to represent the X and Y coordinates and then we bind them together using the cbind() function (which combines objects by columns):\n\nx &lt;- c(16.92, 24.03, 7.61, 15.49, 11.77)\ny&lt;- c(8.37, 12.93, 16.65, 12.2, 13.12)\nxy &lt;- cbind(x, y)\nxy\n\n         x     y\n[1,] 16.92  8.37\n[2,] 24.03 12.93\n[3,]  7.61 16.65\n[4,] 15.49 12.20\n[5,] 11.77 13.12\n\n\nWe could alternatively combine by rows using the rbind() function\n\nrbind(x, y)\n\n   [,1]  [,2]  [,3]  [,4]  [,5]\nx 16.92 24.03  7.61 15.49 11.77\ny  8.37 12.93 16.65 12.20 13.12\n\n\nWe could even alter the row names using an inbuilt vector of uppercase letters:\n\nrownames(xy) &lt;- LETTERS[1:5]\nxy\n\n      x     y\nA 16.92  8.37\nB 24.03 12.93\nC  7.61 16.65\nD 15.49 12.20\nE 11.77 13.12\n\n\nImportantly, all entries in a matrix must be of the same type. That is, they must all be numeric, or all be characters etc. If we attempt to mix a combination of data types in a matrix, then the data will all be converted into a type that can accommodate all the data. For example, if we attempt to bind together the numeric temperature data and the character site data into a matrix, then the result will be a matrix of characters (since while it is possible to covert numbers to strings, in this case the reverse is not possible).\n\ncbind(temperature, site)\n\n    temperature site\nQ1  \"36.1\"      \"A1\"\nQ2  \"30.6\"      \"A2\"\nQ3  \"31\"        \"B1\"\nQ4  \"36.3\"      \"B2\"\nQ5  \"39.9\"      \"C1\"\nQ6  \"6.5\"       \"C2\"\nQ7  \"11.2\"      \"D1\"\nQ8  \"12.8\"      \"D2\"\nQ9  \"9.7\"       \"E1\"\nQ10 \"15.9\"      \"E2\"\n\n\nOn the other hand, if we attempt to bind together the numeric temperature data and the factor shade data into a matrix, then the result will be a matrix of numbers (recall that factors are internally stored as integers, yet they have a levels property that acts rather like a lookup key).\n\ncbind(temperature, shade)\n\n    temperature shade\nQ1         36.1     1\nQ2         30.6     1\nQ3         31.0     1\nQ4         36.3     1\nQ5         39.9     1\nQ6          6.5     2\nQ7         11.2     2\nQ8         12.8     2\nQ9          9.7     2\nQ10        15.9     2\n\n\n\n\n4.1.7 Lists\nLists provide a way to group together multiple objects of different type and length. For example, whilst the contents of any single vector or matrix must all be of the one type and length (e.g. all numeric or all character), a list can contain any combination of vectors, matrices, scalars and of any type. Furthermore, the objects contained in a list do not need to be of the same lengths (c.f data frames). The output of most analyses are returned as lists.\nAs an example, we could group together the previously created isolated vectors and matrices into a single object that encapsulates the entire experiment:\n\nexperiment &lt;- list(\n  site = site,\n  quadrats = quadrats,\n  coordinates = xy,\n  shade = shade,\n  temperature = temperature\n)\nexperiment\n\n$site\n [1] \"A1\" \"A2\" \"B1\" \"B2\" \"C1\" \"C2\" \"D1\" \"D2\" \"E1\" \"E2\"\n\n$quadrats\n [1] \"Q1\"  \"Q2\"  \"Q3\"  \"Q4\"  \"Q5\"  \"Q6\"  \"Q7\"  \"Q8\"  \"Q9\"  \"Q10\"\n\n$coordinates\n      x     y\nA 16.92  8.37\nB 24.03 12.93\nC  7.61 16.65\nD 15.49 12.20\nE 11.77 13.12\n\n$shade\n [1] no   no   no   no   no   full full full full full\nLevels: no full\n\n$temperature\n  Q1   Q2   Q3   Q4   Q5   Q6   Q7   Q8   Q9  Q10 \n36.1 30.6 31.0 36.3 39.9  6.5 11.2 12.8  9.7 15.9 \n\n\nLists can be thought of as a set of objects bound into a single container. In the example above, the list object experiment contains a copy of the site, quadrats, coordinates, shade and temperature objects.\nImportantly, once a list has been created the objects within the list are not linked in any way to the original objects from which the list is formed. Consequently, any changes made to (for example) the temperature object will not be reflected in the content of the temperature object within the experiment list.\nTo access an object within a list, the $ operator is used as such:\n\nexperiment$temperature\n\n  Q1   Q2   Q3   Q4   Q5   Q6   Q7   Q8   Q9  Q10 \n36.1 30.6 31.0 36.3 39.9  6.5 11.2 12.8  9.7 15.9 \n\n\n\n\n4.1.8 Dataframes - data sets\nRarely are single biological variables collected in isolation. Rather, data are usually collected in sets of variables reflecting investigations of patterns between and/or among the different variables. Consequently, data sets are best organized into matrices of variables (vectors) all of the same lengths yet not necessarily of the same type. Hence, neither lists nor matrices represent natural storages for data sets. This is the role of data frames which are used to store a set of vectors of the same length (yet potentially different types) in a rectangular matrix.\nData frames are generated by combining multiple vectors together such that each vector becomes a separate column in the data frame. For a data frame to faithfully represent a data set, the sequence in which observations appear in the vectors must be the same for each vector, and each vector should have the same number of observations. For example, the first, second, third…etc entries in each vector must represent respectively, the observations collected from the first, second, third…etc sampling units.\nSince the focus of these tutorials is on the exploration, analysis and summary of data sets, and data sets are accommodated in R by data frames, the generation, importation, exportation, manipulation and management of data frames receives extensive coverage in many other subsequent tutorials.\nAs an simple example of a data frame, we could again group together the previously created isolated vectors into a single object that encapsulates a data set:\n\ndata &lt;- data.frame(\n  site = site,\n  quadrats = quadrats,\n  shade = shade,\n  temperature = temperature\n)\ndata\n\n    site quadrats shade temperature\nQ1    A1       Q1    no        36.1\nQ2    A2       Q2    no        30.6\nQ3    B1       Q3    no        31.0\nQ4    B2       Q4    no        36.3\nQ5    C1       Q5    no        39.9\nQ6    C2       Q6  full         6.5\nQ7    D1       Q7  full        11.2\nQ8    D2       Q8  full        12.8\nQ9    E1       Q9  full         9.7\nQ10   E2      Q10  full        15.9",
    "crumbs": [
      "R basics",
      "Introduction to R"
    ]
  },
  {
    "objectID": "01_introduction_to_r.html#object-information",
    "href": "01_introduction_to_r.html#object-information",
    "title": "Introduction to R",
    "section": "5.1 Object information",
    "text": "5.1 Object information\nAs indicated earlier, everything in R is an object. All objects have a type or class that encapsulates the sort of information stored in the object as well as determining how other functions interact with the object. The class of an object can be reviewed with the class() function:\n\nclass(temperature)\n\n[1] \"numeric\"\n\nclass(data)\n\n[1] \"data.frame\"\n\nclass(mean)\n\n[1] \"function\"\n\n\nThere is also a family of functions prefixed with is. that evaluate whether or not an object is of a particular class (or type) or not. The following table lists the common object query functions. All object query functions return a logical vector. Enter methods(is) for a more comprehensive list.\n\n\n\n\nFunction class\n\n\nReturns TRUE\n\n\nExamples\n\n\n\n\n\n\nis.numeric(x)\n\n\nif all elements of x are numeric or integers\n\n\n\nis.numeric(c(1, -3.5, temperature))\n\n[1] TRUE\n\n\n\n\n\n\nis.null(x)\n\n\nif x is null (the object has no length)\n\n\n\nis.null(NULL)\n\n[1] TRUE\n\n\n\n\n\n\nis.logical(x)\n\n\nif all elements of x are logical\n\n\n\nis.logical(c(TRUE, FALSE, 1))\n\n[1] FALSE\n\n\n\n\n\n\nis.character(x)\n\n\nif all elements of x are character strings\n\n\n\nis.character(c(\"A\", \"Plant\", quadrats))\n\n[1] TRUE\n\n\n\n\n\n\n\nis.vector(x)\n\n\nif the object x is a vector (has only a single dimension). Returns FALSE if object has attributes other than ‘names’.\n\n\nis.vector(temperature)\n\n[1] TRUE\n\n\n\n\n\n\nis.factor(x)\n\n\nif the object x is a factor\n\n\n\nis.factor(shade)\n\n[1] TRUE\n\n\n\n\n\n\nis.matrix(x)\n\n\nif the object x is a matrix (two dimensions, yet not adata.frame)\n\n\n\nis.matrix(xy)\n\n[1] TRUE\n\n\n\n\n\n\nis.list(x)\n\n\nif the object x is a list\n\n\n\nis.list(experiment)\n\n[1] TRUE\n\n\n\n\n\n\nis.data.frame(x)\n\n\nif the object x is a data.frame\n\n\n\nis.data.frame(data)\n\n[1] TRUE\n\n\n\n\n\n\nis.na(x)\n\n\nfor each missing (NA) element in x\n\n\n\nis.na(c(NA, 2))\n\n[1]  TRUE FALSE\n\n\n\n\n\n\n!\n\n\n(‘not’) operator as a prefix converts the above functions into ‘is.not’\n\n\n\n!is.factor(data)\n\n[1] TRUE\n\n\n\n\n\n\n\n5.1.1 Attributes\nMany R objects also have a set of attributes, the number and type of which are specific to each class of object. For example, a matrix object has a specific number of dimensions as well as row and column names. The attributes of an object can be viewed using the attributes() function:\n\nattributes(xy)\n\n$dim\n[1] 5 2\n\n$dimnames\n$dimnames[[1]]\n[1] \"A\" \"B\" \"C\" \"D\" \"E\"\n\n$dimnames[[2]]\n[1] \"x\" \"y\"\n\n\nSimilarly, the attr() function can be used to view and set individual attributes of an object, by specifying the name of the object and the name of the attribute (as a character string) as arguments. For example:\n\nattr(xy, \"dim\")\n\n[1] 5 2\n\nattr(xy, \"description\") &lt;- \"coordinates of quadrats\"\nxy\n\n      x     y\nA 16.92  8.37\nB 24.03 12.93\nC  7.61 16.65\nD 15.49 12.20\nE 11.77 13.12\nattr(,\"description\")\n[1] \"coordinates of quadrats\"\n\n\nNote that in the above example, the attribute ‘description’ is not a in-built attribute of a matrix. When a new attribute is set, this attribute is displayed along with the object. This provides a useful way of attaching a description (or other metadata) to an object, thereby reducing the risks of the object becoming unfamiliar.",
    "crumbs": [
      "R basics",
      "Introduction to R"
    ]
  },
  {
    "objectID": "01_introduction_to_r.html#object-conversion",
    "href": "01_introduction_to_r.html#object-conversion",
    "title": "Introduction to R",
    "section": "5.2 Object conversion",
    "text": "5.2 Object conversion\nObjects can be converted or coerced into other objects using a family of functions with a as. prefix. Note that there are some obvious restrictions on these conversions as most objects cannot be completely accommodated by all other object types, and therefore some information (such as certain attributes) may be lost or modified during the conversion. Objects and elements that cannot be successfully coerced are returned as NA. The following table lists the common object coercion functions. Use methods(as) for a more comprehensive list.\n\n\n\n\nFunction\n\n\nConverts object to\n\n\n\n\n\n\nas.numeric(x)\n\n\na numeric vector (‘integer’ or ‘real’). Factors converted to integers.\n\n\n\n\nas.null(x)\n\n\na NULL\n\n\n\n\nas.logical(x)\n\n\na logical vector. A values of &gt;1 converted to TRUE otherwise FALSE.\n\n\n\n\nas.character(x)\n\n\na character (string) vector.\n\n\n\n\nas.vector(x)\n\n\na vector. All attributes (including names) are removed.\n\n\n\n\nas.factor(x)\n\n\na factor. This is an abbreviated (with respect to its argument set) version of the factor() function.\n\n\n\n\nas.matrix(x)\n\n\na matrix. Any non-numeric elements result in all matrix elements being converted to characters.\n\n\n\n\nas.list(x)\n\n\na list\n\n\n\n\nas.data.frame(x)\n\n\na data.frame. Matrix columns and list items are converted into separate vectors of the dataframe and character vectors are converted into factors. All previous attributes are removed.\n\n\n\n\nas.date(x)\n\n\na date",
    "crumbs": [
      "R basics",
      "Introduction to R"
    ]
  },
  {
    "objectID": "01_introduction_to_r.html#indexing",
    "href": "01_introduction_to_r.html#indexing",
    "title": "Introduction to R",
    "section": "5.3 Indexing",
    "text": "5.3 Indexing\nIndexing is the means by which data are filtered (subsetted) to include and exclude certain entries.\n\n5.3.1 Vector indexing\nSubsets of vectors are produced by appending an index vector (inclosed in square brackets []) to a vector name. There are four common forms of vector indexing used to extract a subset of vectors:\n\nVector of positive integers - a set of integers that indicate which elements of the vector should be included:\n\ntemperature[2]\n\n  Q2 \n30.6 \n\ntemperature[2:5]\n\n  Q2   Q3   Q4   Q5 \n30.6 31.0 36.3 39.9 \n\ntemperature[c(1, 5, 6, 9)]\n\n  Q1   Q5   Q6   Q9 \n36.1 39.9  6.5  9.7 \n\n\nVector of negative integers - a set of integers that indicate which elements of the vector should be excluded:\n\ntemperature[-2]\n\n  Q1   Q3   Q4   Q5   Q6   Q7   Q8   Q9  Q10 \n36.1 31.0 36.3 39.9  6.5 11.2 12.8  9.7 15.9 \n\ntemperature[c(1, 5, 6, 9) * -1]\n\n  Q2   Q3   Q4   Q7   Q8  Q10 \n30.6 31.0 36.3 11.2 12.8 15.9 \n\n\nVector of character strings (referencing names) - for vectors whose elements have been named, a vector of names can be used to select elements to include:\n\ntemperature[\"Q1\"]\n\n  Q1 \n36.1 \n\ntemperature[c(\"Q1\", \"Q4\")]\n\n  Q1   Q4 \n36.1 36.3 \n\n\nVector of logical values - a vector of logical values (TRUE or FALSE) the same length as the vector being subsetted. Entries corresponding to a logical TRUE are included, FALSE are excluded:\n\ntemperature[temperature &lt; 15]\n\n  Q6   Q7   Q8   Q9 \n 6.5 11.2 12.8  9.7 \n\ntemperature[shade == \"no\"]\n\n  Q1   Q2   Q3   Q4   Q5 \n36.1 30.6 31.0 36.3 39.9 \n\ntemperature[temperature &lt; 34 & shade == \"no\"]\n\n  Q2   Q3 \n30.6 31.0 \n\ntemperature[temperature &lt; 10 | shade == \"no\"]\n\n  Q1   Q2   Q3   Q4   Q5   Q6   Q9 \n36.1 30.6 31.0 36.3 39.9  6.5  9.7 \n\n\n\n\n\n5.3.2 Matrix indexing\nSimilar to vectors, matrices can be indexed using positive integers, negative integers, character strings and logical vectors. However, whereas vectors have a single dimension (length), matrices have two dimensions (length and width). Hence, indexing needs to reflect this. It is necessary to specify both the row and column number. Matrix indexing takes of the form of [row.indices, col.indices] where row.indices and col.indices respectively represent sequences of row and column indices. If a row or column index sequence is omitted, it is interpreted as the entire row or column respectively.\n\nxy[3, 2]\n\n[1] 16.65\n\nxy[3, ]\n\n    x     y \n 7.61 16.65 \n\nxy[, -2]\n\n    A     B     C     D     E \n16.92 24.03  7.61 15.49 11.77 \n\nxy[\"A\", 1:2]\n\n    x     y \n16.92  8.37 \n\nxy[, \"x\"]\n\n    A     B     C     D     E \n16.92 24.03  7.61 15.49 11.77 \n\nxy[xy[, \"x\"] &gt; 12, ]\n\n      x     y\nA 16.92  8.37\nB 24.03 12.93\nD 15.49 12.20\n\n\nIf you think that last example looks awkward you would not be alone. In a later tutorial, I will introduce an alternative way of manipulating data for data frames.\n\n\n5.3.3 List indexing\nLists consist of collections of objects that need not be of the same size or type. The objects within a list are indexed by appending an index vector (enclosed in single or double square brackets, [] or [[]]), to the list name. Single square brackets provide access to multiple list items (returned as a list), whereas double square brackets provide access to individual list items (returned according to the type of object represented by the list item). A single object within a list can also be referred to by appending a string character ($) followed by the name of the object to the list names (e.g. list$object). The elements of objects within a list are indexed according to the object type. Vector indices to objects within other objects (lists) are placed within their own square brackets outside the list square brackets: Recall the experiment list we generated earlier.\n\nexperiment\n\n$site\n [1] \"A1\" \"A2\" \"B1\" \"B2\" \"C1\" \"C2\" \"D1\" \"D2\" \"E1\" \"E2\"\n\n$quadrats\n [1] \"Q1\"  \"Q2\"  \"Q3\"  \"Q4\"  \"Q5\"  \"Q6\"  \"Q7\"  \"Q8\"  \"Q9\"  \"Q10\"\n\n$coordinates\n      x     y\nA 16.92  8.37\nB 24.03 12.93\nC  7.61 16.65\nD 15.49 12.20\nE 11.77 13.12\n\n$shade\n [1] no   no   no   no   no   full full full full full\nLevels: no full\n\n$temperature\n  Q1   Q2   Q3   Q4   Q5   Q6   Q7   Q8   Q9  Q10 \n36.1 30.6 31.0 36.3 39.9  6.5 11.2 12.8  9.7 15.9 \n\n\nThe following examples illustrate the common ways to subset (index) lists.\n\nA vector of positive numbers (single brackets) - that indicate which list items should be included:\n\nexperiment[c(1,3)]\n\n$site\n [1] \"A1\" \"A2\" \"B1\" \"B2\" \"C1\" \"C2\" \"D1\" \"D2\" \"E1\" \"E2\"\n\n$coordinates\n      x     y\nA 16.92  8.37\nB 24.03 12.93\nC  7.61 16.65\nD 15.49 12.20\nE 11.77 13.12\n\n\nA single positive number (double brackets) - that indicates which list item should be included:\n\nexperiment[[1]]\n\n [1] \"A1\" \"A2\" \"B1\" \"B2\" \"C1\" \"C2\" \"D1\" \"D2\" \"E1\" \"E2\"\n\n\nA single character string (double brackets) - that indicates which list item should be included:\n\nexperiment[['temperature']]\n\n  Q1   Q2   Q3   Q4   Q5   Q6   Q7   Q8   Q9  Q10 \n36.1 30.6 31.0 36.3 39.9  6.5 11.2 12.8  9.7 15.9 \n\n\nExtract the first element of each list item - returned as a matrix:\n\nsapply(experiment, \"[\" ,1)\n\n          site       quadrats    coordinates          shade temperature.Q1 \n          \"A1\"           \"Q1\"        \"16.92\"            \"1\"         \"36.1\" \n\n##notice that only one element of the coordinate pair is included\n##OR when the list items are not vectors\ndo.call(cbind, experiment)[1, ]\n\nWarning in (function (..., deparse.level = 1) : number of rows of result is not\na multiple of vector length (arg 1)\n\n\n       site    quadrats           x           y       shade temperature \n       \"A1\"        \"Q1\"     \"16.92\"      \"8.37\"         \"1\"      \"36.1\"",
    "crumbs": [
      "R basics",
      "Introduction to R"
    ]
  },
  {
    "objectID": "01_introduction_to_r.html#pattern-matching-and-replacement",
    "href": "01_introduction_to_r.html#pattern-matching-and-replacement",
    "title": "Introduction to R",
    "section": "5.4 Pattern matching and replacement",
    "text": "5.4 Pattern matching and replacement\nAn important part of filtering is the ability to detect patterns on which to base selections or exclusions. Numerical and categorical filtering rules are generally fairly straight forward, however complex filtering rules can also be devised from character vectors. Furthermore, the ability to search and replace character strings within a character vector can also be very useful.\n\n5.4.1 grep - index of match\nThe grep() function searches within a vector for matches to a pattern and returns the index of all matching entries.\n\n## get the indexes of elements of the site vector that contain an 'A' \ngrep(pattern = \"A\", experiment$site)\n\n[1] 1 2\n\n## use the results of the grep as indexes to select only those 'site'\n## values that contain an 'A'\nexperiment$site[grep(pattern = \"a\", experiment$site)]\n\ncharacter(0)\n\n\nThe pattern can comprise any valid regular expression and is therefore very flexible:\n\n## get the indexes of values of the 'site' vector within the `data`\n## dataframe that contain either an 'A', 'B' or 'C' followed by a '1'\ngrep(\"[a-c]1\", data$site)\n\ninteger(0)\n\n## select only those rows of the `data` dataframe that correspond to a\n## 'site' value of either an 'A', 'B' or 'C' followed by a '1'\ndata[grep(\"[a-c]1\", data$site), ]\n\n[1] site        quadrats    shade       temperature\n&lt;0 rows&gt; (or 0-length row.names)\n\n\n\n\n5.4.2 regexpr - position and length of match\nRather than return the indexes of matching entries, the regexpr() function returns the position of the match within each string as well as the length of the pattern within each string (-1 values correspond to entries in which the pattern is not found).\n\naust &lt;- c(\"adelaide\", \"brisbane\", \"canberra\", \"darwin\", \"hobart\", \"melbourne\", \"perth\", \"sydney\")\naust\n\n[1] \"adelaide\"  \"brisbane\"  \"canberra\"  \"darwin\"    \"hobart\"    \"melbourne\"\n[7] \"perth\"     \"sydney\"   \n\n## get the position and length of string of characters containing an\n## 'a' and an 'e' separated by any number of characters\nregexpr(pattern=\"a.*e\", aust)\n\n[1]  1  6  2 -1 -1 -1 -1 -1\nattr(,\"match.length\")\n[1]  8  3  4 -1 -1 -1 -1 -1\nattr(,\"index.type\")\n[1] \"chars\"\nattr(,\"useBytes\")\n[1] TRUE\n\n\n\n\n5.4.3 gsub - pattern replacement\nThe gsub() function replaces all instances of an identified pattern within a character vector with an alternative set of characters. The similar sub() function replaces only the first instance.\n\ndata$shade\n\n [1] no   no   no   no   no   full full full full full\nLevels: no full\n\ngsub(\"no\", \"Not shaded\", data$shade)\n\n [1] \"Not shaded\" \"Not shaded\" \"Not shaded\" \"Not shaded\" \"Not shaded\"\n [6] \"full\"       \"full\"       \"full\"       \"full\"       \"full\"      \n\n\nIt is also possible to extend the functionality to accomodate perl-compatible regular expressions.\n\n## convert all the capital values entries into uppercase identify (and\n## store) all words (`\\\\w`) convert stored pattern (`\\\\1`) to uppercase\n## (`\\\\U`)\ngsub(\"(\\\\w)\", \"\\\\U\\\\1\", aust, perl = TRUE)\n\n[1] \"ADELAIDE\"  \"BRISBANE\"  \"CANBERRA\"  \"DARWIN\"    \"HOBART\"    \"MELBOURNE\"\n[7] \"PERTH\"     \"SYDNEY\"   \n\n\n\n\n5.4.4 substr - extracting substrings\nThe substr() function is used to extract parts of string (set of characters) entries within character vectors and thus is useful for making truncated labels (particularly for graphical summaries). For example, if we had a character vector containing the names of the Australian capital cities and required abbreviations (first 3 characters) for graph labels:\n\n## recall the AUST character vector that lists the Australian capital\n## cities\naust\n\n[1] \"adelaide\"  \"brisbane\"  \"canberra\"  \"darwin\"    \"hobart\"    \"melbourne\"\n[7] \"perth\"     \"sydney\"   \n\nsubstr(aust, start = 1, stop = 3)\n\n[1] \"ade\" \"bri\" \"can\" \"dar\" \"hob\" \"mel\" \"per\" \"syd\"\n\n\nAlternatively, we could use the abbreviate() function.\n\nabbreviate(aust, minlength = 3)\n\n adelaide  brisbane  canberra    darwin    hobart melbourne     perth    sydney \n    \"adl\"     \"brs\"     \"cnb\"     \"drw\"     \"hbr\"     \"mlb\"     \"prt\"     \"syd\" \n\n\n\n\n5.4.5 Value matching\nIn addition to the above matching procedures, it is possible to compare vectors via the usual set of binary operators (x&lt;y, x&gt;y, x≤y, x≥y, x==y and x!=y).\n\nshade == 'no'\n\n [1]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE\n\ntemperature &gt; 32\n\n   Q1    Q2    Q3    Q4    Q5    Q6    Q7    Q8    Q9   Q10 \n TRUE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE \n\n\nNote, that the comparisons are made in an item-wise manner. That is, item one of the right hand vector is compared to item one of the left hand vector, and item two of each vector are compared to one another and so on. If the two vectors are not of equal length, the shorter vector is recycled (that is, it returns to the start of that vector and keeps going).\n\n## Compare 'Q1' to items 1,3,5,7,9 of quadrats and compare 'Q3' to\n## items 2,4,6,8,10.\nquadrats == c('Q1','Q3')\n\n [1]  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBe very cautious when using the binary operators x==y or x!=y to compare numeric vectors as they do not allow for rounding errors or finite representation of fractions and will almost always return FALSE even for values that appear identical. As an alternative, consider using a combination of all.equal() and identical():\n\n\n\n(0.6 - 0.4) == (0.4 - 0.2)\n\n[1] FALSE\n\nall.equal((0.6 - 0.4), (0.4 - 0.2))\n\n[1] TRUE\n\nidentical(all.equal((0.6 - 0.4), (0.4 - 0.2)), TRUE)\n\n[1] TRUE\n\n\nEach of the search and replace functions listed above uses only a single search item (albeit with pattern matching that can accommodate multiple patterns). The match() function searches for the first instance of items in the lookup vector (vector of values to be matched against) within the vector to be matched (first vector) returning the index of the first instance. Similarly, the special binary operator %in% indicates whether or not (TRUE or FALSE) an item of the matching vector is contained anywhere within the first vector. This latter mechanism makes a very useful filter.\n\n## match the items within the `shade` vector against a lookup character\n## vector containing only the string of \"no\" returning the index\n## within the lookup vector\nmatch(shade,\"no\")\n\n [1]  1  1  1  1  1 NA NA NA NA NA\n\n## match the items within the shade vector against a lookup character\n## vector containing only the string of \"no\" returning the index\n## within the lookup vector\nmatch(shade,\"no\")\n\n [1]  1  1  1  1  1 NA NA NA NA NA\n\n## same match as above, yet returning a logical vector corresponding\n## to whether each item in the first vector is matched or not\nshade %in% 'no'\n\n [1]  TRUE  TRUE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE\n\n## match quadrats of 'Q1', 'Q4' and 'Q10'\nmatch(quadrats, c(\"Q1\",\"Q4\",\"Q10\"))\n\n [1]  1 NA NA  2 NA NA NA NA NA  3\n\nquadrats %in% c(\"Q1\",\"Q4\",\"Q10\")\n\n [1]  TRUE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE  TRUE\n\n## use the resulting logical vector as a filter\ndata[quadrats %in% c(\"Q1\",\"Q4\",\"Q10\"),]\n\n    site quadrats shade temperature\nQ1    A1       Q1    no        36.1\nQ4    B2       Q4    no        36.3\nQ10   E2      Q10  full        15.9",
    "crumbs": [
      "R basics",
      "Introduction to R"
    ]
  },
  {
    "objectID": "01_introduction_to_r.html#sorting",
    "href": "01_introduction_to_r.html#sorting",
    "title": "Introduction to R",
    "section": "5.5 Sorting",
    "text": "5.5 Sorting\nThe sort() function is used to sort vector entries in increasing (or decreasing) order.\n\nsort(temperature)\n\n  Q6   Q9   Q7   Q8  Q10   Q2   Q3   Q1   Q4   Q5 \n 6.5  9.7 11.2 12.8 15.9 30.6 31.0 36.1 36.3 39.9 \n\nsort(temperature, decreasing = TRUE)\n\n  Q5   Q4   Q1   Q3   Q2  Q10   Q8   Q7   Q9   Q6 \n39.9 36.3 36.1 31.0 30.6 15.9 12.8 11.2  9.7  6.5 \n\n\nThe order() function is used to get the position of each entry in a vector if it were sorted in increasing (or decreasing) order.\n\norder(temperature)\n\n [1]  6  9  7  8 10  2  3  1  4  5\n\norder(temperature, decreasing = TRUE)\n\n [1]  5  4  1  3  2 10  8  7  9  6\n\n\nHence the smallest entry in the temperature vector was at position (index) 6 and so on.\nThe rank() function is used to get the ranking of each entry in a vector if it were sorted in increasing (or decreasing) order.\n\nrank(temperature)\n\n Q1  Q2  Q3  Q4  Q5  Q6  Q7  Q8  Q9 Q10 \n  8   6   7   9  10   1   3   4   2   5 \n\n\nIndicating that the first entry in the temperature vector was ranked eighth in increasing order. Ranks from decreasing order can be produced by then reversing the returned vector using the rev() function.\n\nrev(rank(temperature))\n\nQ10  Q9  Q8  Q7  Q6  Q5  Q4  Q3  Q2  Q1 \n  5   2   4   3   1  10   9   7   6   8 \n\n## or via pipe\nrank(temperature) |&gt; rev()\n\nQ10  Q9  Q8  Q7  Q6  Q5  Q4  Q3  Q2  Q1 \n  5   2   4   3   1  10   9   7   6   8",
    "crumbs": [
      "R basics",
      "Introduction to R"
    ]
  },
  {
    "objectID": "01_introduction_to_r.html#formatting-data",
    "href": "01_introduction_to_r.html#formatting-data",
    "title": "Introduction to R",
    "section": "5.6 Formatting data",
    "text": "5.6 Formatting data\n\n5.6.1 Rounding of numerical data\nThe ceiling() function rounds vector entries up to the nearest integer\n\nceiling(temperature)\n\n Q1  Q2  Q3  Q4  Q5  Q6  Q7  Q8  Q9 Q10 \n 37  31  31  37  40   7  12  13  10  16 \n\n\nThe floor() function rounds vector entries down to the nearest integer\n\nfloor(temperature)\n\n Q1  Q2  Q3  Q4  Q5  Q6  Q7  Q8  Q9 Q10 \n 36  30  31  36  39   6  11  12   9  15 \n\n\nThe trunc() function rounds vector entries to the nearest integer towards ‘0’ (zero)\n\nseq(from = -2, to = 2, by = 0.5)\n\n[1] -2.0 -1.5 -1.0 -0.5  0.0  0.5  1.0  1.5  2.0\n\ntrunc(seq(from = -2, to = 2, by = 0.5))\n\n[1] -2 -1 -1  0  0  0  1  1  2\n\n\nThe round() function rounds vector entries to the nearest numeric with the specified number of decimal places. Digits of 5 are rounded off to the nearest even digit.\n\nround(temperature)\n\n Q1  Q2  Q3  Q4  Q5  Q6  Q7  Q8  Q9 Q10 \n 36  31  31  36  40   6  11  13  10  16 \n\nround(seq(from = -2, to = 2, by = 0.5))\n\n[1] -2 -2 -1  0  0  0  1  2  2\n\nround(temperature/2.2, digits = 2)\n\n   Q1    Q2    Q3    Q4    Q5    Q6    Q7    Q8    Q9   Q10 \n16.41 13.91 14.09 16.50 18.14  2.95  5.09  5.82  4.41  7.23 \n\nround(temperature, digits = -1)\n\n Q1  Q2  Q3  Q4  Q5  Q6  Q7  Q8  Q9 Q10 \n 40  30  30  40  40  10  10  10  10  20 \n\n\n\n\n5.6.2 Notation and labelling of numeric or character data\nOccasionally (mainly for graphical displays), it is necessary to be able to adjust the other aspects of the formatting of vector entries. For example, you may wish to have numbers expressed in scientific notation (2.93e-04 rather than 0.000293) or insert commas every 3 digits left of the decimal point or even add prefixes or suffixes to numbers or words. These procedures are supported via a number of functions. The uses of each function are contrasted in the following table followed by common usage examples below.\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\npaste()\nConcatenate vectors after converting into characters\n\n\nformat()\nAdjust decimal places, justification, padding and width of string and whether to use scientific notation\n\n\nformatC()\nA version of format() that is compliant with ‘C’ style formatting.\n\n\nsprintf()\nA wrapper for the ‘C’ style formatting function of the same name = provides even greater flexibility (and complexity).\n\n\n\n\npaste()\nCombine multiple elements together along with other character strings.\n\npaste(\"Quadrat\", 1:3, sep = \":\")\n\n[1] \"Quadrat:1\" \"Quadrat:2\" \"Quadrat:3\"\n\n##create a joint label for site and quadrat combinations\npaste(site, quadrats, sep = \":\")\n\n [1] \"A1:Q1\"  \"A2:Q2\"  \"B1:Q3\"  \"B2:Q4\"  \"C1:Q5\"  \"C2:Q6\"  \"D1:Q7\"  \"D2:Q8\" \n [9] \"E1:Q9\"  \"E2:Q10\"\n\n## create a formula relating temperature to quadrat, site and shade\npaste(names(data)[4], paste(names(data)[-4], collapse = \"+\"), sep = \"~\")\n\n[1] \"temperature~site+quadrats+shade\"\n\n## or more neatly\npaste(names(data)[4],\n  paste(names(data)[-4], collapse = \"+\"),\n  sep = \"~\"\n)\n\n[1] \"temperature~site+quadrats+shade\"\n\n\n\n\nformat()\nOverloaded generic function for formatting objects (particularly numeric vectors). The most prominent features include:\n\nAutomatically adding leading or trailing spaces to create equal width labels (via trim =, width = and justify = )\nApplication of scientific notation (via scientific =)\nRounding of numbers (via digits = and nsmall =)\nApplies to each column in a dataframe separately\n\n\n## create equal width strings by adding padding to the start (left\n## side) of numbers\nformat(temperature)\n\n    Q1     Q2     Q3     Q4     Q5     Q6     Q7     Q8     Q9    Q10 \n\"36.1\" \"30.6\" \"31.0\" \"36.3\" \"39.9\" \" 6.5\" \"11.2\" \"12.8\" \" 9.7\" \"15.9\" \n\n## create labels with a minimum of 2 digits to the right hand side of\n## the decimal place\nformat(temperature, nsmall = 2)\n\n     Q1      Q2      Q3      Q4      Q5      Q6      Q7      Q8      Q9     Q10 \n\"36.10\" \"30.60\" \"31.00\" \"36.30\" \"39.90\" \" 6.50\" \"11.20\" \"12.80\" \" 9.70\" \"15.90\" \n\n## create labels that are rounded numbers\nformat(temperature, digits = 1)\n\n  Q1   Q2   Q3   Q4   Q5   Q6   Q7   Q8   Q9  Q10 \n\"36\" \"31\" \"31\" \"36\" \"40\" \" 6\" \"11\" \"13\" \"10\" \"16\" \n\n## create labels that are scientific representations of the numbers\nformat(temperature, scientific = TRUE)\n\n        Q1         Q2         Q3         Q4         Q5         Q6         Q7 \n\"3.61e+01\" \"3.06e+01\" \"3.10e+01\" \"3.63e+01\" \"3.99e+01\" \"6.50e+00\" \"1.12e+01\" \n        Q8         Q9        Q10 \n\"1.28e+01\" \"9.70e+00\" \"1.59e+01\" \n\n## apply formatting rules to a dataframe (notice the left\n## justification of Shade and the number of decimal places of\n## temperature)\nformat(data, justify = \"left\", nsmall = 2)\n\n    site quadrats shade temperature\nQ1    A1      Q1   no         36.10\nQ2    A2      Q2   no         30.60\nQ3    B1      Q3   no         31.00\nQ4    B2      Q4   no         36.30\nQ5    C1      Q5   no         39.90\nQ6    C2      Q6   full        6.50\nQ7    D1      Q7   full       11.20\nQ8    D2      Q8   full       12.80\nQ9    E1      Q9   full        9.70\nQ10   E2      Q10  full       15.90\n\n\n\n\nformatC()\nSimilar to the format() function, yet also allows ‘C’ style formatting specifications:\n\n‘d’ for integers\n‘f’ for reals in the standard xxx.xxx format\n‘e’, ‘E’ for reals in the scientific (n.ddde+nn) format\n‘g’, ‘G’ for reals in the scientific (n.ddde+nn) format when it saves space to do so\n‘s’ for strings\n\n\nseq(pi, pi * 10000, length = 5)\n\n[1]     3.141593  7856.337828 15709.534064 23562.730300 31415.926536\n\n## format to integers  \nformatC(seq(pi, pi * 10000, length = 5), format = \"d\")\n\n[1] \"3\"     \"7856\"  \"15709\" \"23562\" \"31415\"\n\n## scientific notation\nformatC(seq(pi, pi * 10000, length = 5), format = \"e\", digits = 2)\n\n[1] \"3.14e+00\" \"7.86e+03\" \"1.57e+04\" \"2.36e+04\" \"3.14e+04\"\n\n## scientific notation only if it saves space\nformatC(seq(pi, pi * 10000, length = 5), format = \"g\", digits = 2)\n\n[1] \"3.1\"     \"7.9e+03\" \"1.6e+04\" \"2.4e+04\" \"3.1e+04\"\n\n## floating point format with 1000's indicators\nformatC(seq(pi, pi * 10000, length = 5), format = \"f\", big.mark = \",\", digits = 2)\n\n[1] \"3.14\"      \"7,856.34\"  \"15,709.53\" \"23,562.73\" \"31,415.93\"\n\n\n\n\nsprintf()\nSimilar to the format() function, yet also allows ‘C’ style formatting specifications:\n\n‘d’ for integers\n‘f’ for reals in the standard xxx.xxx format\n‘e’, ‘E’ for reals in the scientific (n.ddde+nn) format\n‘g’, ‘G’ for reals in the scientific (n.ddde+nn) format when it saves space to do so\n‘s’ for strings\n\n\nPI &lt;- seq(pi, pi * 10000, length = 5)\nPI\n\n[1]     3.141593  7856.337828 15709.534064 23562.730300 31415.926536\n\n## format to integers\nsprintf(\"%.0f\", PI)\n\n[1] \"3\"     \"7856\"  \"15710\" \"23563\" \"31416\"\n\n## format to two decimal places and 6 characters to the left of the\n## decimal point (right justified)\nsprintf(\"%6.2f\", PI)\n\n[1] \"  3.14\"   \"7856.34\"  \"15709.53\" \"23562.73\" \"31415.93\"\n\n## scientific notation\nsprintf(\"%e\", PI)\n\n[1] \"3.141593e+00\" \"7.856338e+03\" \"1.570953e+04\" \"2.356273e+04\" \"3.141593e+04\"\n\n## scientific notation only when it saves space\nsprintf(\"%6.2g\", PI)\n\n[1] \"   3.1\"  \"7.9e+03\" \"1.6e+04\" \"2.4e+04\" \"3.1e+04\"\n\n## concatenating strings\nsprintf(\"%s-%s\", site, quadrats)\n\n [1] \"A1-Q1\"  \"A2-Q2\"  \"B1-Q3\"  \"B2-Q4\"  \"C1-Q5\"  \"C2-Q6\"  \"D1-Q7\"  \"D2-Q8\" \n [9] \"E1-Q9\"  \"E2-Q10\"\n\nsprintf(\"%s=%.2g\", 'val', PI)\n\n[1] \"val=3.1\"     \"val=7.9e+03\" \"val=1.6e+04\" \"val=2.4e+04\" \"val=3.1e+04\"\n\nsprintf(\"%s=%6.2g\", 'val', PI)\n\n[1] \"val=   3.1\"  \"val=7.9e+03\" \"val=1.6e+04\" \"val=2.4e+04\" \"val=3.1e+04\"\n\nsprintf('%11s', sprintf(\"%s=%.2g\", 'val', PI))\n\n[1] \"    val=3.1\" \"val=7.9e+03\" \"val=1.6e+04\" \"val=2.4e+04\" \"val=3.1e+04\"",
    "crumbs": [
      "R basics",
      "Introduction to R"
    ]
  },
  {
    "objectID": "01_introduction_to_r.html#applying-functions-repetitively",
    "href": "01_introduction_to_r.html#applying-functions-repetitively",
    "title": "Introduction to R",
    "section": "5.7 Applying functions repetitively",
    "text": "5.7 Applying functions repetitively\nAs R is a programming language, it naturally has constructs for controlling flow via looping and conditional evaluation. R’s basic control-flow constructs is the topic of another tutorial. Despite the enormous flexibility gained via the usual control-flow constructs, recall that as R is a scripting language (rather than a compiled language), it is relatively slow. In particular, repetitive tasks (such as looping though a dataframe and applying the same function to different subsets of the data) are especially inefficient.\nThere are a number of functions in R that are designed to allow the repetitive application of a function thereby replacing the need to write loops.\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\nrep()\nDuplicates the result of a function multiple times\n\n\nreplicated()\nPerforms a function multiple times\n\n\napply()\nRepetitively apply a function over the margins of a matrix\n\n\ntapply()\nRepetitively apply a function to cells made up of unique combinations of factor levels\n\n\nlapply()\nRepetitively apply a function to the elements of a list of vector and return a list.\n\n\n\nThe replicate() function repeatedly performs the function specified in the second argument the number of times indicated by the first argument. The important distinction between the replicate() function and the rep() function described earlier, is that the former repeatedly performs the function whereas the later performs the function only once and then duplicates the result multiple times.\nSince most functions produce the same result each time they are performed, for many uses, both functions produce identical results. The one group of functions that do not produce identical results each time, are those involved in random number generation. Hence, the replicate() function is usually used in conjunction with random number generators (such as runif(), which will be described in greater detail in subsequent tutorial) to produce sets of random numbers. Consider first the difference between rep() and replicate():\n\nrep(runif(1), times = 5)\n\n[1] 0.7020219 0.7020219 0.7020219 0.7020219 0.7020219\n\nreplicate(n = 5, runif(1))\n\n[1] 0.546292045 0.007718241 0.302366231 0.323846373 0.308730325\n\n\nWhen the function being run within runif() itself produces a vector of length &gt; 1, the runif() function combines each of the vectors together as separate columns in a matrix:\n\nreplicate(n = 5, runif(5))\n\n          [,1]      [,2]        [,3]      [,4]      [,5]\n[1,] 0.9580511 0.2566856 0.106377281 0.2403867 0.2542036\n[2,] 0.8640270 0.7999805 0.693549562 0.1094617 0.1779478\n[3,] 0.6344061 0.0117873 0.003645214 0.5035335 0.9572608\n[4,] 0.3745481 0.8556894 0.478987866 0.5904530 0.7242430\n[5,] 0.7689539 0.1795727 0.750206979 0.0979138 0.1638585\n\n\n\n5.7.1 Apply functions along matrix margins\nThe apply() function applies a function to the margins (1=row margins and 2=column margins) of a matrix. For example, we might have a matrix that represents the abundance of three species of moth from three habitat types:\n\nmoth &lt;- cbind(SpA = c(25, 6, 3), SpB = c(12, 12, 3), SpC = c(7, 2, 19))\nrownames(moth) &lt;- paste(\"Habitat\", 1:3, sep = \"\")\nmoth\n\n         SpA SpB SpC\nHabitat1  25  12   7\nHabitat2   6  12   2\nHabitat3   3   3  19\n\n\nThe apply() function could be used to calculate the column means (mean abundance of each species across habitat types):\n\napply(moth, MARGIN = 2, FUN = mean)\n\n      SpA       SpB       SpC \n11.333333  9.000000  9.333333 \n\n\n\n\n5.7.2 Pivot tables\nThe tapply() function applies a function to a vector separately for each level of a factorial variable. For example, if we wanted to calculate the mean temperature for each level of the shade variable:\n\ntapply(temperature, INDEX = shade, FUN = mean)\n\n   no  full \n34.78 11.22 \n\n## calculate the mean temperature per shade and quadrat number combination\n## quadrat number is just the last digit of the quadrats vector\n## extracted via substr(site, 2, 2)\ntapply(temperature, list(shade, quadnum = substr(site, 2, 2)), mean)\n\n      quadnum\n              1        2\n  no   35.66667 33.45000\n  full 10.45000 11.73333\n\n\n\n\n5.7.3 Apply a function over a list\nThe lapply() and sapply() functions apply a function separately to each of the objects in a list and return a list and vector/matrix respectively. For example, to find out the length of each of the objects within the experiment list:\n\nlapply(experiment, length)\n\n$site\n[1] 10\n\n$quadrats\n[1] 10\n\n$coordinates\n[1] 10\n\n$shade\n[1] 10\n\n$temperature\n[1] 10\n\nsapply(experiment, length)\n\n       site    quadrats coordinates       shade temperature \n         10          10          10          10          10",
    "crumbs": [
      "R basics",
      "Introduction to R"
    ]
  },
  {
    "objectID": "01_introduction_to_r.html#listing-installed-packages",
    "href": "01_introduction_to_r.html#listing-installed-packages",
    "title": "Introduction to R",
    "section": "6.1 Listing installed packages",
    "text": "6.1 Listing installed packages\nThe installed.packages() function tabulates a list of all the currently installed packages available on your system along with the package path (where is resides on your system) and version number. Additional fields can be requested (including “Priority”, “Depends”, “Imports”, “LinkingTo”, “Suggests”, “Enhances”, “OS_type”, “License” and “Built”).\n\ninstalled.packages()\ninstalled.packages(fields=c(\"Package\", \"LibPath\", \"Version\", \"Depends\",\"Built\"))\n\n\n\n\n\n\n\nNote\n\n\n\nIn the above, I have intentionally supressed the output so as not to flood the output (I have a very large number of packages installed on my machine).\n\n\nYet more information can be obtained for any single package with the packageDescription() and library functions - the latter provides all the information of the former and then includes a descriptive index of all the functions and datasets defined within the package.\n\npackageDescription('MASS')\n\nPackage: MASS\nPriority: recommended\nVersion: 7.3-60\nDate: 2023-05-02\nRevision: $Rev: 3621 $\nDepends: R (&gt;= 4.0), grDevices, graphics, stats, utils\nImports: methods\nSuggests: lattice, nlme, nnet, survival\nAuthors@R: c(person(\"Brian\", \"Ripley\", role = c(\"aut\", \"cre\", \"cph\"),\n        email = \"ripley@stats.ox.ac.uk\"), person(\"Bill\", \"Venables\",\n        role = \"ctb\"), person(c(\"Douglas\", \"M.\"), \"Bates\", role =\n        \"ctb\"), person(\"Kurt\", \"Hornik\", role = \"trl\", comment =\n        \"partial port ca 1998\"), person(\"Albrecht\", \"Gebhardt\", role =\n        \"trl\", comment = \"partial port ca 1998\"), person(\"David\",\n        \"Firth\", role = \"ctb\"))\nDescription: Functions and datasets to support Venables and Ripley,\n        \"Modern Applied Statistics with S\" (4th edition, 2002).\nTitle: Support Functions and Datasets for Venables and Ripley's MASS\nLazyData: yes\nByteCompile: yes\nLicense: GPL-2 | GPL-3\nURL: http://www.stats.ox.ac.uk/pub/MASS4/\nContact: &lt;MASS@stats.ox.ac.uk&gt;\nNeedsCompilation: yes\nPackaged: 2023-05-02 16:42:41 UTC; ripley\nAuthor: Brian Ripley [aut, cre, cph], Bill Venables [ctb], Douglas M.\n        Bates [ctb], Kurt Hornik [trl] (partial port ca 1998), Albrecht\n        Gebhardt [trl] (partial port ca 1998), David Firth [ctb]\nMaintainer: Brian Ripley &lt;ripley@stats.ox.ac.uk&gt;\nRepository: CRAN\nDate/Publication: 2023-05-04 07:32:21 UTC\nBuilt: R 4.3.2; x86_64-pc-linux-gnu; 2024-03-04 23:43:59 UTC; unix\n\n-- File: /opt/R/4.3.2/lib/R/library/MASS/Meta/package.rds \n\nlibrary(help='MASS')",
    "crumbs": [
      "R basics",
      "Introduction to R"
    ]
  },
  {
    "objectID": "01_introduction_to_r.html#installing-packages",
    "href": "01_introduction_to_r.html#installing-packages",
    "title": "Introduction to R",
    "section": "6.2 Installing packages",
    "text": "6.2 Installing packages\nThe R community contains some of the brightest and most generous mathematician, statisticians and practitioners who continue to actively develop and maintain concepts and routines. Most of these routines end up being packaged as a collection of functions and then hosted on one or more publicly available sites so that others can benefit from their efforts.\nThe locations of collections of packages are called repositories or ‘repos’ for short. There four main repositories are CRAN, Bioconductor, R-Forge and github. By default, R is only ‘tuned in’ to CRAN. That is any package queries or actions pertain just to the CRAN repositories.\nTo get a tabulated list of all the packages available on CRAN (warning there are over 5000 packages, so this will be a large table - I will suppress the output):\n\navailable.packages()\n\n\n6.2.1 Comprehensive R Archive Network - CRAN\nCRAN is a repository of R packages mirrored across 90 sites throughout the world. Packages are installed from CRAN using the install.packages() function. The first (and only mandatory) argument to the install.packages() function is the name of the package(s) to install (pkgs =). If no other arguments are provided, the install.packages() function will search CRAN for the specified package(s) and install it along with any of its dependencies that are not yet installed on your system.\nNote, unless you have started the session with administrator (root) privileges, the packages will be installed within a path of your home folder. Whilst this is not necessarily a bad thing, it does mean that the package is not globally available to all users on your system (not that it is common to have multiple users of a single system these days). Moreover, it means that R packages reside in multiple locations across your system. The packages that came with your R install will be in one location (or a couple or related locations) and the packages that you have installed will be in another location.\nTo see the locations currently used on your system, you can issue the following statement.\n\n.libPaths()\n\n[1] \"/home/runner/work/_temp/Library\" \"/opt/R/4.3.2/lib/R/site-library\"\n[3] \"/opt/R/4.3.2/lib/R/library\"     \n\n\nTo install a specific package (and its dependencies). The package that I have chosen to demonstrate this with (remotes) is a package that enables R packages to be installed from git repositories (such as github, and will be featured in a later subsection).\n\ninstall.packages(\"remotes\")\n\nYou will be prompted to select a mirror site. In the absence of any other criterion, just select the mirror that is closed geographically to you. The terminal will then provide feedback about the progress and status of the install process. By indicating a specific repository, you can avoid being prompted for a mirror. For example, I chose to use a CRAN mirror at Melbourne University (Australia), and therefore the following statement gives me direct access\n\ninstall.packages(\"remotes\", repos = \"http://cran.csiro.au\")\n\nFinally, you could provide a vector of repository names if you were unsure which repository was likely to contain the package you were after. This can also be useful if your preferred mirror regularly experiences downtime - the alternative mirror (second in the vector) is used only when the first fails.\n\n\n6.2.2 Bioconductor\nBioconductor is an open source and open development project devoted to genomic data analysis tools, most of which are available as R packages. Whilst initially the packages focused primarily on the manipulation and analysis of DNA microarrays, as the scope of the projects has expanded, so too has the functional scope of the packages there hosted.\n\nsource(\"http://bioconductor.org/biocLite.R\")\nbiocLite(\"limma\")\n\nOr to install multiple packages from Bioconductor\n\nsource(\"http://bioconductor.org/biocLite.R\")\nbiocLite(c(\"GenomicFeatures\", \"AnnotationDbi\"))\n\n\n\n6.2.3 R-Forge\nUnlike both CRAN and Bioconductor (which are essentially package repositories), R-Forge is an entire R package development platform. Package development is supported through a range of services including:\n\nversion control (SVN) - allowing multiple collaborators to maintain current and historical versions of files by facilitating simultaneous editing, conflict resolution and rolling back\ndaily package checking and building - so packages are always up to date\nbug tracking and feature request tools\nmailing lists and message boards\nfull backup and archival system\n\nAnd all of this within a mature content management system like web environment. Installing packages from R-Forge is the same as it is for CRAN, just that the path of the root repository needs to be specified with the repos= argument.\n\ninstall.packages(\"lme4.0\", repos = \"http://R-Forge.R-project.org\")\n\n\n\n6.2.4 Github (via remotes)\nGithub builds upon the philosophy of the development platform promoted by the Source Forge family (including R-Forge) by adding the ability to fork a project. Forking is when the direction of a project is split so that multiple new opportunities can be explored without jeopardizing the stability and integrity of the parent source. If the change in direction proves valuable, the project (package) can either become a new package or else feedback into the development of the original package.\nHadley Wickham and Co have yet again come up with a set of outrageously useful tools (remotes package). This package is a set of functions that simplify (albeit slightly dictatorially) the processes of installing packages from remote and local repositories (Github, Gitlab, Bitbucket etc)\nIn order to make use of this package to install packages from github, the remotes package must itself be installed (we did this earlier). It is recommended that this install take place from CRAN (as outline above). Thereafter, the remotes package can be included in the search path and the install_github function used to retrieve and install a nominated package or packages from Github.\n\nremotes::install_github(\"ggplot2\")\n\nAs described above, Github is a development platform and therefore it is also a source of ‘bleeding edge’ development versions of packages. Whilst the development versions are less likely to be as stable or even as statistically rigorous as the final release versions, they do offer the very latest ideas and routines. They provide the very latest snapshot of where the developers are currently at.\nMost of the time users only want the stable release versions of a package. However there are times when having the ability to try out new developments as they happen can be very rewarding. The install_dev() function allows for the installation of the development version of a package.\nThe more complex devtools package (also by Hadley Wickham et al) provides a set of functions that simplify (albeit slightly dictatorially) the processes of package authoring, building, releasing and installing. Within the devtools package, the dev_mode() function provides a switch that can be used to toggle your system in and out of development mode. When in development mode, installed packages are quarantined within a separate path (R-dev) to prevent them overriding or conflicting with the stable versions that are critical for your regular analyses.\n\n## switch to development mode\ndevtools::dev_mode(on = TRUE)\n##install the development version of ggplot2\ndevtools::install_github(\"ggplot2\")\n## use the development version of ggplot2 \nlibrary(ggplot2)\n## switch development mode off\ndevtools::dev_mode(on = FALSE)\n## stable version of ggplot2 is now engaged\n\n\n\n6.2.5 Manual download and install\nPackages are made available on the various repositories in compressed form and differ between Windows, MacOSX and Linux versions. Those web repositories all have functionality for navigating or searching through the repositories for specific packages. The packages (compressed files) can be directly downloaded from these sites.\nAdditionally, some packages are not available on the various repositories and firewalls and proxies can sometimes prevent R from accessing the repositories directly. In these cases, packages must be manually downloaded and installed.\nThere are a number of ways to install a package that resides locally. Note, do not uncompress the packages.\n\nFrom the command line (outside of R).\n\n\nR CMD INSTALL packagename \n\nwhere packagename is replaced by the path and name of the compressed package.\n\nUsing the install.packages() function by specifying repos = NULL.\n\n\ninstall.packages('packagename', repos=NULL)\n\nwhere packagename is replaced by the path (if not in the current working directory) and name of the compressed package.\n\nVia the Windows RGui, select the Install package(s) from local zip files… option of the Packages menu and select the compressed package.",
    "crumbs": [
      "R basics",
      "Introduction to R"
    ]
  },
  {
    "objectID": "01_introduction_to_r.html#updating-packages",
    "href": "01_introduction_to_r.html#updating-packages",
    "title": "Introduction to R",
    "section": "6.3 Updating packages",
    "text": "6.3 Updating packages\nAn integral component of package management is being able to maintain an up to date system. Many packages are regularly updated so as to adopt new ideas and functionality. Indeed, it is the speed of functional evolution that sets R apart from most other statistical environments.\nAlong with the install.packages() function, there are three other functions to help manage and maintain the packages on your system.\n\nold.packages() compares the versions of packages you have installed with the versions of those packages available in the current repositories. It tabulates the names, install paths and versions of old packages on your system.\n\nold.packages()\n\nAlternative repositories (than CRAN) can be indicated via the repos   = argument.\n\nold.packages(repos = \"http://R-Forge.R-project.org\")\n## or even multiple repos\nold.packages(repos = c(\"http://cran.csiro.au\", \"http://R-Forge.R-project.org\"))\n\nnew.packages() provides a tabulated list of all the packages on the repository that are either not in your local install, or else are of a newer version. Note, with over 4000 packages available on CRAN, unless the repos= parameter is pointing to somewhere very specific (and with a narrow subset of packages) this function is rarely of much use.\n\nnew.packages()\n\nupdate.packages() downloads and installs packages for which newer versions of those packages identified as ‘old’ by the old.packages() function. Just like old.packages(), alternative or multiple repositories can be specified.\n\nupdate.packages()\n## or from alternative multiple repos\nupdate.packages(repos = c(\"http://cran.csiro.au\", \"http://R-Forge.R-project.org\"))",
    "crumbs": [
      "R basics",
      "Introduction to R"
    ]
  },
  {
    "objectID": "01_introduction_to_r.html#package-management-pak",
    "href": "01_introduction_to_r.html#package-management-pak",
    "title": "Introduction to R",
    "section": "6.4 Package management (pak)",
    "text": "6.4 Package management (pak)\nPackage management can be a relatively complex task. These days packages are sourced from a variety and mixture of locations (CRAN, Github etc). Furthermore, most packages have a complex network of dependencies (that is, they depend on other packages). The fine folk over at Rstudio have developed a package called pak that aims to provide a unified and simplified interface to package management.\nThis next-generation package installer offers several key advantages for the technical R user:\n\nParallel downloads: pak leverages multi-core processing to download multiple packages simultaneously, significantly reducing installation time.\nIntelligent dependency resolution: pak automatically resolves package dependencies, installing the necessary versions in the correct order, ensuring a seamless experience.\nExpanded package sources: pak supports installation from diverse repositories like Bioconductor and even GitHub URLs, providing access to a broader range of cutting-edge tools.\nFine-grained control: pak gives you the power to specify them explicitly, offering greater control over your R environment.\nExtensible architecture: pak exposes an API for building custom extensions and integrating seamlessly with your data science workflows.\n\nBefore we can take advantage of pak package management, it must first be installed from CRAN using the traditional package installation method.\n\ninstall.packages(\"pak\")\n\n\n6.4.1 Dependencies\nFor any given package, we can see the dependencies. To illustrate, I will focus on the Matrix package.\n\npak::pkg_deps(\"Matrix\")\n\nℹ Loading metadata database\n\n\n✔ Loading metadata database ... done\n\n\n\n\n\n# A data frame: 2 × 36\n  ref     type  direct directpkg status package version license needscompilation\n  &lt;chr&gt;   &lt;chr&gt; &lt;lgl&gt;  &lt;lgl&gt;     &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;   &lt;lgl&gt;           \n1 lattice stan… FALSE  FALSE     OK     lattice 0.22-6  GPL (&gt;… TRUE            \n2 Matrix  stan… TRUE   TRUE      OK     Matrix  1.6-5   GPL (&gt;… TRUE            \n# ℹ 27 more variables: priority &lt;chr&gt;, md5sum &lt;chr&gt;, sha256 &lt;chr&gt;,\n#   filesize &lt;int&gt;, built &lt;chr&gt;, platform &lt;chr&gt;, rversion &lt;chr&gt;,\n#   repotype &lt;chr&gt;, repodir &lt;chr&gt;, target &lt;chr&gt;, deps &lt;list&gt;, mirror &lt;chr&gt;,\n#   sources &lt;list&gt;, remote &lt;list&gt;, error &lt;list&gt;, metadata &lt;list&gt;,\n#   dep_types &lt;list&gt;, params &lt;list&gt;, sysreqs &lt;chr&gt;, cache_status &lt;chr&gt;,\n#   sysreqs_packages &lt;list&gt;, sysreqs_pre_install &lt;chr&gt;,\n#   sysreqs_post_install &lt;chr&gt;, sysreqs_install &lt;chr&gt;, lib_status &lt;chr&gt;, …\n\n\nAfter some database checking, the above function returns a tibble (like a data frame, yet with some special properties that include truncated output) containing a row for each dependency. In this example, the tibble has just two rows (one for the Matrix package, and the other for its only dependency, the Lattice package). To save space, the many columns have been truncated, yet listed below the tibble.\nAlternatively, we could view the dependencies as a tree.\n\npak::pkg_deps_tree(\"Matrix\")\n\nMatrix 1.6-1.1 -&gt; 1.6-5 [upd][bld][cmp][dl] (2.88 MB)\n└─lattice 0.21-9 -&gt; 0.22-6 [upd][bld][cmp][dl] (598.58 kB)\n\nKey:  [upd] update | [dl] download | [bld] build | [cmp] compile\n\n\nWe can see from the above that the Matrix package depends on the Lattice package.\n\n\n6.4.2 Installing packages\nTo install a package:\n\nfrom CRAN or Bioconductor: just provide the package name as an argument\n\npak::pkg_install(\"tidyverse\")\n\nfrom Github: provide the package name in the form of user/repo. You can also nominate a specific branch (user/repo@branch) or tag (user/repo@tag).\n\npak::pkg_install(\"tidyverse/dplyr\")\n\n\nSimilarly, pak::pkg_install() can be used for package updating. If the package has not yet been installed, the package will be installed, yet if the package has already been installed, then it will instead be updated (unless it is already the most up to date version).\nIf the upgrade = TRUE argument is supplied, then all the dependencies will also be updated.\n\n\n6.4.3 Removing packages\nPackage can be removed using the pak::pkg_remove() function.",
    "crumbs": [
      "R basics",
      "Introduction to R"
    ]
  },
  {
    "objectID": "01_introduction_to_r.html#namespaces",
    "href": "01_introduction_to_r.html#namespaces",
    "title": "Introduction to R",
    "section": "6.5 Namespaces",
    "text": "6.5 Namespaces\nEarly on in this tutorial, I presented a set of rules and recommendations for object naming. One recommendation that I stressed was to avoid using names for objects that are the names of common functions (like mean) so as to (hopefully) avoid conflicting with any of the functions built in to R.\nHaving made these recommendations, I will now say that R is not overly fragile and is sufficiently cleaver to enable it to resolve many naming conflicts. Object names are context specific (see also object overloading above).\nWhen the name of an object is supplied that could be used to refer to multiple objects (for example, if you had created an object called mean there would be two objects named mean - your object and the inbuilt function), R first attempts to determine which object you are likely to have been referring to.\nObjects are defined and apply within certain contexts or namespaces. Namespaces defined the context (environment) in which an object is available. Objects created within functions, remain local to those functions. Hence if an object is created within a function, it is not available outside that function.\nThe namespace provides a context in which R should look for an object (such as a function). Functions defined within packages are available for use, when the library is loaded. This is essentially adding the libraries namespace to the list of contexts to that R should search within when you confront it with an expression.\nAlternatively, we can prefix the function name with the package name (its namespace) thereby explicitly indicating the context in which the function is defined and thus, the function will be found.\nFor example, lets say we wanted to create sparse diagonal matrix (a matrix with values in the diagonals and blanks in the off diagonals. There is a function called Diagonal in the Matrix package. We could expose this function (and all others in the package via the library function or we could just prefix the function name with the package name.\n\n## call the Diagonal function (however it wont be found)\nDiagonal(3)\n\nError in Diagonal(3): could not find function \"Diagonal\"\n\n## call the diagonal function from the Matrix package\nMatrix::Diagonal(3)\n\n3 x 3 diagonal matrix of class \"ddiMatrix\"\n     [,1] [,2] [,3]\n[1,]    1    .    .\n[2,]    .    1    .\n[3,]    .    .    1\n\n\nSimilarly, prefixing the namespace to the function name allows us to explicitly nominate exactly which function we want to use in the event that there are two functions of the same name in different packages.",
    "crumbs": [
      "R basics",
      "Introduction to R"
    ]
  },
  {
    "objectID": "02_editors.html",
    "href": "02_editors.html",
    "title": "Code Editors",
    "section": "",
    "text": "1 Introduction\nIn the previous tutorial, we installed R and began exploring the language. If this was your very first time using R and perhaps your first exposure to any programming language, it is likely that you worked through the tutorial using either the R Gui (if on windows) or the terminal application in MacOSX or Linux.\nWhilst these tools to provide direct interaction with the R engine, they do little to support your efforts to develop code, documentation and sophisticated analysis outputs. This is where code editors come in.\nCode editors are a specialized software tools designed for creating and modifying source code of computer programs. They provide essential features such as syntax highlighting, code completion, and error checking, enhancing the efficiency and accuracy of programming tasks. Code editors are essential for developers, offering a streamlined environment for writing, editing, and organizing code, facilitating the software development process.\nChoosing an appropriate code editor is crucial for efficient R development. Each editor offers unique features and interfaces, catering to different preferences and workflows. This guide will walk you through some of the popular choices, including RStudio, Visual Studio Code, Emacs, Neovim, and Sublime Text, helping you make an informed decision.\nOf these, particular emphasis will be placed on RStudio. This is primarily because it is specifically designed to be an Intergrated Development Environment (IDE) for R. It is developed by active members of the R community for the R community. Furthermore, because it is a dedicated R IDE, it works straight out of the box with little to no configuration necessary. By contrast, the other editors are general code editors and thus must be specifically configured to provide R based functionality.\n\n\n2 Editors\n\nRStudioVisual Studio CodeEmacsNeoVimSublime Text\n\n\n\n2.0.1 Overview\nRStudio stands out as a widely used and dedicated Integrated Development Environment (IDE) designed for R development. Its user-friendly interface and comprehensive features make it a popular choice among R users.\n\n\n2.0.2 Installation\n\nInstalling RStudio on Windows:Installing RStudio on macOS:Installing RStudio on Linux:\n\n\n\nDownload R:\n\nRStudio requires R to be installed. If you have not already done so, download and install R from the official CRAN website.\n\nDownload RStudio:\n\nVisit the RStudio Download page and select the “RStudio Desktop” version compatible with your Windows operating system.\n\nInstall RStudio:\n\nRun the downloaded RStudio installer and follow the installation wizard.\nAccept the default settings unless you have specific preferences.\n\nLaunch RStudio:\n\nAfter installation, launch RStudio from the Start menu or desktop shortcut.\n\n\n\n\n\nDownload R:\n\nIf you have not already done so, download and install R on macOS from the official CRAN website.\n\nDownload RStudio:\n\nNavigate to the RStudio Download page and choose the “RStudio Desktop” version for macOS.\n\nInstall RStudio:\n\nRun the downloaded RStudio package, and macOS will guide you through the installation process.\n\nLaunch RStudio:\n\nOpen RStudio from the Applications folder or use Spotlight to search for it.\n\n\n\n\n\nDownload R:\n\nIf you have not already done so, install R on your Linux distribution using the package manager. For example, on Ubuntu, run:\n\n\n\nsudo apt-get install r-base\n\n\nDownload RStudio:\n\nVisit the RStudio Download page and choose the appropriate RStudio Desktop version for your Linux distribution.\n\nInstall RStudio:\n\nRun the downloaded RStudio package, and follow any additional instructions based on your Linux distribution.\n\nLaunch RStudio:\n\nOpen a terminal and type rstudio to launch RStudio.\n\n\n\n\n\n\n\n2.0.3 Key Features\nRStudio offers an integrated scripting and console environment, extensive support for RMarkdown, and streamlined package management capabilities.\nI strongly encourage you to look over the RStudio user guide - particularly the Getting Started section.\n\n\n\n\n2.0.4 Overview\nVisual Studio Code (VSCode) is a versatile and extensible code editor known for its speed and efficiency. While not exclusively designed for R, it offers excellent support for the language through extensions.\n\n\n2.0.5 Installation\n\nDownload Visual Studio Code:\n\nVisit the Visual Studio Code Download page and choose the version suitable for your operating system (Windows, macOS, or Linux).\nFollow the installation instructions for your specific operating system.\n\nInstall Visual Studio Code:\n\nRun the downloaded installer and follow the installation wizard.\nAccept the default settings unless you have specific preferences.\n\nLaunch Visual Studio Code:\n\nAfter installation, launch VSCode from the Start menu or applications folder.\n\n\n\n\n2.0.6 Setting Up R Support in Visual Studio Code:\n\nInstall R Extension:\n\nOpen VSCode and go to the Extensions view by clicking on the square icon on the sidebar or using the shortcut Ctrl+Shift+X.\nSearch for “R” in the Extensions view search box.\nInstall the “R Language” extension provided by Yuki Ueda.\n\nConfigure R Path (Optional):\n\nOpen the VSCode settings by pressing Ctrl+, or navigating to File &gt; Preferences &gt; Settings.\nClick on the “Open Settings (JSON)” icon in the upper-right corner of the Settings tab.\nAdd the following JSON configuration to set the path to your R executable:\n\n\n\n\"r.rpath.windows\": \"C:\\\\Program Files\\\\R\\\\R-4.x.x\\\\bin\\\\x64\\\\R.exe\",  // Replace with your R path\n\n\nSelect R Interpreter:\n\nCreate or open an R script in VSCode.\nLook for the “Select an R interpreter” notification at the bottom-right corner.\nClick on “Select R Interpreter” and choose the R version you installed.\n\nInstall Required R Packages:\n\nOpen the integrated terminal in VSCode using Ctrl+` .\nInstall the necessary R packages (e.g., languageserver and formatR) by running the following commands:\n\n\n\ninstall.packages(\"languageserver\")\ninstall.packages(\"formatR\")\n\n\nReload Window:\n\nAfter configuring R support, it’s recommended to reload the VSCode window to apply the changes.\n\nVerify R Support:\n\nOpen R Script:\n\nCreate or open an R script (.R file) in VSCode.\n\nCheck R Features:\n\nVerify that R features such as syntax highlighting, code completion, and linting are functioning correctly.\n\nRun R Script:\n\nRun parts of your R script or the entire script to ensure that the R interpreter is correctly executing code.\n\n\n\n\n\n2.0.7 Key Features\nVSCode is lightweight, supports the R Language Server, and can be enhanced with various extensions to meet specific development needs.\n\n\n\n\n2.0.8 Overview\nEmacs is a highly customizable and extensible text editor renowned for its versatility. It may have a steeper learning curve, but its power lies in its ability to adapt to individual preferences.\n\n\n2.0.9 Installation\n\nDownload and Install Emacs:\n\nVisit the GNU Emacs Download page and select the appropriate version for your operating system (Windows, macOS, or Linux).\nFollow the installation instructions provided on the download page.\n\nLaunch Emacs:\n\nAfter installation, launch Emacs. On Windows, you can find it in the Start menu. On macOS and Linux, open a terminal and type emacs.\nConfiguring Emacs for R Support:\n\nInstall ESS (Emacs Speaks Statistics):\n\nESS is a package for Emacs that provides support for various statistical languages, including R.\nOpen Emacs and add the following to your Emacs configuration file (usually ~/.emacs or ~/.emacs.d/init.el):\n\n\n\n;; Add MELPA repository for package installation\n(require 'package)\n(add-to-list 'package-archives '(\"melpa\" . \"https://melpa.org/packages/\") t)\n(package-initialize)\n\n;; Install ESS package\n(unless (package-installed-p 'ess)\n  (package-refresh-contents)\n  (package-install 'ess))\n\n\nConfigure ESS:\n\nCustomize your Emacs configuration to set up ESS for R. Add the following lines to your configuration file:\n\n\n\n;; Configure ESS for R\n(require 'ess-site)\n\n\nSelect R Interpreter:\n\nOpen an R script in Emacs. ESS should automatically detect your R installation.\nIf needed, customize the R interpreter by adding the following line to your configuration file:\n\n\n\n(setq inferior-R-program-name \"/path/to/R\")\n\n\nUseful Keybindings (Optional):\n\nAdd keybindings for common ESS commands. For example:\n\n\n\n(global-set-key (kbd \"C-c C-k\") 'ess-eval-buffer)\n(global-set-key (kbd \"C-c C-r\") 'ess-eval-region)\n\n\nESS Documentation (Optional):\n\nAccess ESS documentation by typing C-h i to open the Info viewer, then select “ESS” from the menu.\n\nReload Configuration:\n\nAfter making changes to your Emacs configuration, restart Emacs or use M-x load-file to reload the configuration.\n\nVerify R Support:\n\nOpen R Script:\n\nCreate or open an R script (.R file) in Emacs.\n\nCheck ESS Features:\n\nVerify that ESS features such as syntax highlighting, code evaluation, and interaction with R are functioning correctly.\n\nRun R Script:\n\nEvaluate parts of your R script or the entire script to ensure that the R interpreter is correctly executing code.\n\n\n\n\n\n2.0.10 Key Features\nEmacs supports extensive extensibility through packages, boasts ESS (Emacs Speaks Statistics) for R integration, and offers Org Mode for literate programming.\n\n\n\n\n2.0.11 Overview\nNeovim is a modern and extensible text editor that builds on the foundation of Vim. It combines the efficiency of Vim with additional features for a more contemporary editing experience.\n\n\n2.0.12 Installation\n\nDownload and Install Neovim:\n\nVisit the Neovim GitHub Releases page and download the installer appropriate for your operating system (Windows, macOS, or Linux).\nFollow the installation instructions provided on the GitHub page.\n\nLaunch Neovim:\n\nAfter installation, launch Neovim. On Windows, you can find it in the Start menu or use the executable. On macOS and Linux, open a terminal and type nvim.\n\nConfiguring Neovim for R Support:\n\nInstall a Plugin Manager (Optional):\n\nWhile optional, using a plugin manager makes it easier to manage Neovim plugins. Popular choices include vim-plug and dein.vim.\nFollow the installation instructions provided by the chosen plugin manager.\n\n\nInstall Nvim-R Plugin:\n\nNvim-R is a plugin that enhances Neovim for R development.\nAdd the following lines to your Neovim configuration file (usually ~/.config/nvim/init.vim or ~/.vimrc):\n\n\n\n\" For vim-plug\nPlug 'jalvesaq/Nvim-R'\n\n\" For dein.vim\ncall dein#add('jalvesaq/Nvim-R')\n\n\nConfigure Nvim-R:\n\nCustomize your Neovim configuration to set up Nvim-R for R. Add the following lines to your configuration file:\n\n\n\n\" Set the path to your R executable (replace with your actual path)\nlet g:vimrplugin_Rexecutable = '/path/to/R'\n\n\" Enable filetype plugin and indentation\nfiletype plugin indent on\n\n\" Set R as the default file type for .R files\nau BufNewFile,BufRead *.R set filetype=r\n\n\nInstall Plugins:\n\nOpen Neovim and run the command to install the configured plugins:\n\nFor vim-plug: :PlugInstall\nFor dein.vim: :call dein#install()\n\n\nReload Configuration:\n\nAfter adding the configuration, restart Neovim or use :source % to reload the configuration.\n\nVerify R Support:\n\nOpen R Script:\n\nCreate or open an R script (.R file) in Neovim.\n\nCheck Nvim-R Features:\n\nVerify that Nvim-R features, such as syntax highlighting, code evaluation, and interaction with R, are functioning correctly.\n\nRun R Script:\n\nEvaluate parts of your R script or the entire script to ensure that the R interpreter is correctly executing code.\n\n\n\n\n\n2.0.13 Key Features\nNeovim maintains Vim compatibility, supports plugins for extended functionality, and emphasizes efficient text editing.\n\n\n\n\n2.0.14 Overview\nSublime Text is a lightweight yet feature-rich text editor appreciated for its speed and simplicity. While not R-specific, it offers a customizable environment suitable for various programming languages.\n\n\n2.0.15 Installation\n\nDownload and Install Sublime Text:\n\nVisit the Sublime Text Download page and download the installer for your operating system (Windows, macOS, or Linux).\nFollow the installation instructions provided on the website.\n\nLaunch Sublime Text:\n\nAfter installation, launch Sublime Text. You can find it in the Start menu on Windows, in the Applications folder on macOS, or by using the terminal on Linux.\n\nConfiguring Sublime Text for R Support:\n\nInstall Package Control:\n\nPackage Control is a package manager for Sublime Text. Follow the installation instructions on the Package Control website.\n\nInstall Terminus Package:\n\nOpen Sublime Text and press Ctrl+Shift+P (Windows/Linux) or Cmd+Shift+P (macOS) to open the command palette.\nType “Install Package” and select “Package Control: Install Package.”\nSearch for “Terminus” and install the package.\n\nInstall R:\n\nMake sure you have R installed on your system. You can download it from the official R website.\n\nConfigure Terminus for R:\n\nOpen Sublime Text and create or open an R script (.R file).\nPress Ctrl+ (Windows/Linux) or Cmd+ (macOS) to open the Terminus console.\nIn the Terminus console, type the following command to start an R session:\n\n\n\n\nR\n\n- Terminus will open a new terminal at the bottom of Sublime Text,\n  providing an interactive R session.\n- Create Build System (Optional):\n  - You can create a custom build system for R scripts to simplify\n    execution.\n    - Open a new file in Sublime Text and paste the following JSON\n      configuration:\n\n{\n    \"cmd\": [\"R\", \"--slave\", \"--vanilla\", \"-f\", \"$file\"],\n    \"file_regex\": \"^(?:(...*?):([0-9]+):([0-9]+)|(...*?))$\",\n    \"selector\": \"source.R\"\n}\n\n    - Save the file with the extension .sublime-build in the\n      \"User\" directory of your Sublime Text \"Packages\" folder. You\n      can find this folder by selecting \"Preferences\" &gt; \"Browse\n      Packages...\" in Sublime Text.\n  - Run R Script:\n    - Open an R script in Sublime Text.\n    - Use the Terminus console to interact with the R session and\n      execute commands.\n\n\n2.0.16 Key Features\nSublime Text boasts multiple cursors, supports extensions through Package Control, and provides ample customization options.",
    "crumbs": [
      "R basics",
      "Code editors"
    ]
  },
  {
    "objectID": "03_data_frames.html",
    "href": "03_data_frames.html",
    "title": "Data frames",
    "section": "",
    "text": "Step 1\nBefore beginning this tutorial, we should make sure we have all the tools in place. We will therefore start by installing the tidyverse ecosystem of packages. Among the many packages included under this umbrella are the packages readr, readxl and tibble - each of which will be used in this tutorial.\nIn addition, the foreign package supports importing data from other statistical software (such as Sas, Stata, Systat, System and Minitab).\nLet start by installing the tidyverse ecosystem of packages along with foreign.\n\npak::pkg_install(\"tidyverse\")\npak::pkg_install(\"foreign\")\n\nNow we will load these packages so that they are available for the rest of the session.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(foreign)\n\n\n\n\n\n\n\nNote\n\n\n\nNotice in the above output, when we load the tidyverse package, some validation steps are performed to indicate which actual packages were loaded. Importantly, notice also that a couple of conflicts are identified. The first of these dplyr::filter() masks stats::filter() indicates that once the dplyr package was loaded the previous definition of a function called filter (from the stats package) was overwritten (masked) by a definition contained wihin the dplyr package.\nThis is not an error. Rather, it is a warning to advise that if you were expecting to call the filter function and were expecting to get the behaviour defined within the stats package, then you should preface the call with the stats namespace. For example, call stats::filter() rather than just filter().\nNo such issues arose when loading the foreign package.\n\n\nStep 2\nThe second necessary preparation is to prepare the file system for a tidy working environment. Rather than place all R scripts, data and outputs into a single (increasingly cluttered folder), it is always better to organise your project into a set number of folders. For this tutorial, I would recommend setting up the following structure.\n../\n|-- data\n|-- scripts\nNow within your chosen editor, I suggest you create an R script within the scripts folder and set this path as the working directory.\nStep 3\nThe final preparation step is to download some data files to use during this tutorial. These files should be placed in the data folder. Each of the files are abbreviated versions of the same Mac Nally (1996) data set, yet each is in a different format (some are text files, others are in formats of other software). Each format is listed below, along with a link to manually access the data and an R code snippet that will download the file and place it in the ../data folder.\n\nmacnally.csv: a comma separated format\n\ndownload.file('https://github.com/ReefCloud/workshops/tree/main/data/macnally.csv', '../data/macnally.csv')\n\nmacnally.txt: a tab separated format\n\ndownload.file('https://github.com/ReefCloud/workshops/tree/main/data/macnally.txt', '../data/macnally.txt')\n\nmacnally.xlsx: an excel workbook format\n\ndownload.file('https://github.com/ReefCloud/workshops/tree/main/data/macnally.xlsx', '../data/macnally.xlsx')",
    "crumbs": [
      "R basics",
      "Data frames"
    ]
  },
  {
    "objectID": "03_data_frames.html#data.frame",
    "href": "03_data_frames.html#data.frame",
    "title": "Data frames",
    "section": "2.1 data.frame",
    "text": "2.1 data.frame\nData frames are generated by amalgamating vectors of the same length together. To illustrate the translation of a data set (collection of variables) into an R data frame (collection of vectors), a portion of a real data set by Mac Nally (1996) in which the bird communities were investigated from 37 sites across five habitats in southeastern Australia will be used. Although the original data set includes the measured maximum density of 102 bird species from the 37 sites, for simplicity’s sake only two bird species (GST: gray shrike thrush, EYR: eastern yellow robin) and the first eight of the sites will be included. The truncated data set, comprises a single factorial (or categorical) variable, two continuous variables, and a set of site (row) names, and is as follows:\n\n\n\nSite\nHABITAT\nGST\nEYR\n\n\n\n\nReedy Lake\nMixed\n3.4\n0.0\n\n\nPearcedale\nGipps.Manna\n3.4\n9.2\n\n\nWarneet\nGipps.Manna\n8.4\n3.8\n\n\nCranbourne\nGipps.Manna\n3.0\n5.0\n\n\nLysterfield\nMixed\n5.6\n5.6\n\n\nRed Hill\nMixed\n8.1\n4.1\n\n\nDevilbend\nMixed\n8.3\n7.1\n\n\nOlinda\nMixed\n4.6\n5.3\n\n\n\nFirstly, we will generate the three variables (excluding the site labels as they are not variables) separately:\n\nhabitat &lt;- factor(c('Mixed', 'Gipps.Manna', 'Gipps.Manna', 'Gipps.Manna', 'Mixed',\n  'Mixed', 'Mixed', 'Mixed'))\ngst &lt;- c(3.4, 3.4, 8.4, 3.0, 5.6, 8.1, 8.3, 4.6)\neyr &lt;- c(0.0, 9.2, 3.8, 5.0, 5.6, 4.1, 7.1, 5.3)\n\nNext, use the list the names of the vectors as arguments in the data.frame() function to amalgamate the three separate variables into a single data frame (data set) which we will call macnally (after the author).\n\nmacnally &lt;- data.frame(habitat, gst, eyr)\nmacnally\n\n      habitat gst eyr\n1       Mixed 3.4 0.0\n2 Gipps.Manna 3.4 9.2\n3 Gipps.Manna 8.4 3.8\n4 Gipps.Manna 3.0 5.0\n5       Mixed 5.6 5.6\n6       Mixed 8.1 4.1\n7       Mixed 8.3 7.1\n8       Mixed 4.6 5.3\n\n\nNotice that each vector (variable) becomes a column in the data frame and that each row represents a single sampling unit (in this case, each row represents a different site). By default, the rows are named using numbers corresponding to the number of rows in the data frame. However, these can be altered to reflect the names of the sampling units by assigning a list of alternative names to the row.names() (data frame row names) property of the data frame.\n\nrow.names(macnally) &lt;- c('Reedy Lake', 'Pearcedale', 'Warneet', 'Cranbourne',\n  'Lysterfield', 'Red Hill', 'Devilbend', 'Olinda')\nmacnally\n\n                habitat gst eyr\nReedy Lake        Mixed 3.4 0.0\nPearcedale  Gipps.Manna 3.4 9.2\nWarneet     Gipps.Manna 8.4 3.8\nCranbourne  Gipps.Manna 3.0 5.0\nLysterfield       Mixed 5.6 5.6\nRed Hill          Mixed 8.1 4.1\nDevilbend         Mixed 8.3 7.1\nOlinda            Mixed 4.6 5.3",
    "crumbs": [
      "R basics",
      "Data frames"
    ]
  },
  {
    "objectID": "03_data_frames.html#expand.grid",
    "href": "03_data_frames.html#expand.grid",
    "title": "Data frames",
    "section": "2.2 expand.grid",
    "text": "2.2 expand.grid\nWhen the data set contains multiple fully crossed categorical variables (factors), the expand.grid() function provides a convenient way to create the factor vectors.\n\nexpand.grid(rep = 1:4, \n  B = paste(\"b\", 1:2, sep = \"\"), \n  A = paste(\"a\", 1:3, sep = \"\")\n)\n\n   rep  B  A\n1    1 b1 a1\n2    2 b1 a1\n3    3 b1 a1\n4    4 b1 a1\n5    1 b2 a1\n6    2 b2 a1\n7    3 b2 a1\n8    4 b2 a1\n9    1 b1 a2\n10   2 b1 a2\n11   3 b1 a2\n12   4 b1 a2\n13   1 b2 a2\n14   2 b2 a2\n15   3 b2 a2\n16   4 b2 a2\n17   1 b1 a3\n18   2 b1 a3\n19   3 b1 a3\n20   4 b1 a3\n21   1 b2 a3\n22   2 b2 a3\n23   3 b2 a3\n24   4 b2 a3",
    "crumbs": [
      "R basics",
      "Data frames"
    ]
  },
  {
    "objectID": "03_data_frames.html#as_tibble",
    "href": "03_data_frames.html#as_tibble",
    "title": "Data frames",
    "section": "2.3 as_tibble",
    "text": "2.3 as_tibble\nTibbles are a modern re-imagining of data frames in R that focus on clarity, consistency, and user-friendliness. While both data frames and tibbles both hold data in rows and columns, tibbles introduce several key differences:\n\nPreserved Data Types: Unlike data frames which coerce strings to factors, tibbles maintain the original data types, facilitating accurate analysis and avoiding surprises.\nExplicit Naming: Column names are always strings, preventing unintentional creation of numeric or logical variables.\nImproved Printing: Tibbles display a concise overview, presenting only the first 10 rows and all fitting columns to screen, making exploration more efficient.\nStreamlined Subsetting: Accessing specific columns is simpler and safer, minimizing potential errors related to partial matching.\n\nThe as_tibble function converts a data frame into a tibble.\n\nmacnally.tbl &lt;- as_tibble(macnally)\nmacnally.tbl\n\n# A tibble: 8 × 3\n  habitat       gst   eyr\n  &lt;fct&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 Mixed         3.4   0  \n2 Gipps.Manna   3.4   9.2\n3 Gipps.Manna   8.4   3.8\n4 Gipps.Manna   3     5  \n5 Mixed         5.6   5.6\n6 Mixed         8.1   4.1\n7 Mixed         8.3   7.1\n8 Mixed         4.6   5.3\n\n\nSince the example data set is so small, there is no appreciable difference in how it is presented as either a data frame or a tibble. It is mainly when the data sets get larger that the distinctions become more apparent.",
    "crumbs": [
      "R basics",
      "Data frames"
    ]
  },
  {
    "objectID": "03_data_frames.html#tribble",
    "href": "03_data_frames.html#tribble",
    "title": "Data frames",
    "section": "2.4 tribble",
    "text": "2.4 tribble\nThe tribble() function allows us to construct tibbles directly.\n\nmacnally.tbl &lt;- tribble(\n  ~habitat, ~gst, ~eyr,\n  \"Mixed\", 3.4, 0.0,\n  \"Gipps.Manna\", 3.4, 9.2,\n  \"Gipps.Manna\", 8.4, 3.8,\n  \"Gipps.Manna\", 3.0, 5.0,\n  \"Mixed\", 5.6, 5.6,\n  \"Mixed\", 8.1, 4.1,\n  \"Mixed\", 8.3, 7.1,\n  \"Mixed\", 4.6, 5.3,\n  )\nmacnally.tbl\n\n# A tibble: 8 × 3\n  habitat       gst   eyr\n  &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 Mixed         3.4   0  \n2 Gipps.Manna   3.4   9.2\n3 Gipps.Manna   8.4   3.8\n4 Gipps.Manna   3     5  \n5 Mixed         5.6   5.6\n6 Mixed         8.1   4.1\n7 Mixed         8.3   7.1\n8 Mixed         4.6   5.3\n\n\nNote that the construction of tibbles like this more closely resembles the eventual structure of the data. Compare this to the way data frames are constructed (by combining individual vectors).",
    "crumbs": [
      "R basics",
      "Data frames"
    ]
  },
  {
    "objectID": "03_data_frames.html#importing-from-text-file",
    "href": "03_data_frames.html#importing-from-text-file",
    "title": "Data frames",
    "section": "3.1 Importing from text file",
    "text": "3.1 Importing from text file\nThe easiest form of importation is from a pure text file. Since most software that accepts file input can read plain text files, text files can be created in all spreadsheet, database and statistical software packages and are also the default outputs of most data collection devices.\nIn a text file, data are separated (or delimited) by a specific character, which in turn defines what sort of text file it is. The text file should broadly represent the format of the data frame.\n\nvariables should be in columns and sampling units in rows. the first\nrow should contain the variable names and if there are row names, these should be in the first column\n\nThe following examples illustrate the format of the abbreviated Mac Nally (1996) data set created as both comma delimited (left) and tab delimited (right) files as well as the corresponding read.table() commands used to import the files.\n\n\n\n\n\n\nNote\n\n\n\nThe following examples assume that the above data will be in the current working directory. If the current working directory (which can be checked with the getwd() function) does not contain these files, then either:\n\ninclude the full path name (or path relative to the current working directory) as the filename argument\nchange the current working directory of your session prior to continuing (use the setwd() function)\ncopy and paste the files into the current working directory.\n\n\n\n\n\nComma delimited text file .csv\n\nLOCATION,HABITAT,GST,EYR\nReedy Lake,Mixed,3.4,0.0\nPearcedale,Gipps.Manna,3.4,9.2\nWarneet,Gipps.Manna,8.4,3.8\nCranbourne,Gipps.Manna,3.0,5.0\n....\n\n\nmacnally_new &lt;- read_csv(\"../data/macnally.csv\", \n  trim_ws = TRUE)\n\nRows: 37 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): LOCATION, HABITAT\ndbl (2): GST, EYR\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nmacnally_new\n\n# A tibble: 37 × 4\n   LOCATION      HABITAT              GST   EYR\n   &lt;chr&gt;         &lt;chr&gt;              &lt;dbl&gt; &lt;dbl&gt;\n 1 Reedy Lake    Mixed                3.4   0  \n 2 Pearcedale    Gipps.Manna          3.4   9.2\n 3 Warneet       Gipps.Manna          8.4   3.8\n 4 Cranbourne    Gipps.Manna          3     5  \n 5 Lysterfield   Mixed                5.6   5.6\n 6 Red Hill      Mixed                8.1   4.1\n 7 Devilbend     Mixed                8.3   7.1\n 8 Olinda        Mixed                4.6   5.3\n 9 Fern Tree Gum Montane Forest       3.2   5.2\n10 Sherwin       Foothills Woodland   4.6   1.2\n# ℹ 27 more rows\n\n\n\n\n\nTab delimited text file .txt\n\nLOCATION    HABITAT     GST EYR\nReedy Lake  Mixed       3.4 0.0\nPearcedale  Gipps.Manna 3.4 9.2\nWarneet     Gipps.Manna 8.4 3.8\nCranbourne  Gipps.Manna 3.0 5.0\n....\n\n\nmacnally_new &lt;- read_tsv(\"../data/macnally.txt\", \n  trim_ws = TRUE)\n\nRows: 37 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr (2): LOCATION, HABITAT\ndbl (2): GST, EYR\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nmacnally_new \n\n# A tibble: 37 × 4\n   LOCATION      HABITAT              GST   EYR\n   &lt;chr&gt;         &lt;chr&gt;              &lt;dbl&gt; &lt;dbl&gt;\n 1 Reedy Lake    Mixed                3.4   0  \n 2 Pearcedale    Gipps.Manna          3.4   9.2\n 3 Warneet       Gipps.Manna          8.4   3.8\n 4 Cranbourne    Gipps.Manna          3     5  \n 5 Lysterfield   Mixed                5.6   5.6\n 6 Red Hill      Mixed                8.1   4.1\n 7 Devilbend     Mixed                8.3   7.1\n 8 Olinda        Mixed                4.6   5.3\n 9 Fern Tree Gum Montane Forest       3.2   5.2\n10 Sherwin       Foothills Woodland   4.6   1.2\n# ℹ 27 more rows\n\n\n\n\nIn the above, the trim_ws = TRUE argument indicates that leading and trailing spaces should be removed from all the data. This is important as often spreadsheets (I’m looking at you Excel), add spaces before or after words (in particular). These are invisible, yet can cause huge headaches when running analyses or graphing..\nThe read_csv and read_tzv functions provide feedback about what they have imported. Specifically, they list the number of rows and columns, what the delimeting character is and the data type assigned to each field (variable/column).\nThe data are imported as a tibble.\nThere are numerous ways to specify the filename.\n\nusing full paths\n\nmacnally_new &lt;- read_csv(\"/home/Project/data/macnally.csv\", trim_ws = TRUE)\n\n\n\n\n\n\n\nNote\n\n\n\nIn the above example, the full path used was appropriate for the machine that the code was run on. However, it is unlikely to reflect a valid path on your machine. You may want to adjust it accordingly.\n\n\nusing relative paths\n\nmacnally_new &lt;- read_csv(\"../data/macnally.csv\", trim_ws = TRUE)\n\n\n\n\n\n\n\nNote\n\n\n\nRecall that ../data/ means navigate out of the current directory and into the data directory.\n\n\nusing ULRs\n\nmacnally_new &lt;- read_csv(url(\"https://github.com/ReefCloud/workshops/tree/main/data/macnally.csv\"), trim_ws = TRUE)\n\nIn the above example, the data are accessed directly from a remote location.",
    "crumbs": [
      "R basics",
      "Data frames"
    ]
  },
  {
    "objectID": "03_data_frames.html#importing-from-the-clipboard",
    "href": "03_data_frames.html#importing-from-the-clipboard",
    "title": "Data frames",
    "section": "3.2 Importing from the clipboard",
    "text": "3.2 Importing from the clipboard\nThe read_tsv() function can also be used to import data (into a tibble) that has been placed on the clipboard by other software, thereby providing a very quick and convenient way of obtaining data from spreadsheets. Simply replace the filename argument with the clipboard() function. For example, to import data placed on the clipboard from Microsoft Excel, select the relevant cells, click copy and then in R, use the following syntax;\n\nmacnally_new &lt;- read_tsv(clipboard(), trim_ws = TRUE)\n\n\n\n\n\n\n\nWarning\n\n\n\nAlthough importing data from the clipboard can be convenient for quickly exploring something, it should mostly be discouraged from a reproducibility perspective:\n\nwhen such code is included in a script, the script will just import whatever is present on the clipboard at the time - which may or may not be what you expect it to be\nthere is no way to record the providence of the data because it is not pointing to a specific file or source.",
    "crumbs": [
      "R basics",
      "Data frames"
    ]
  },
  {
    "objectID": "03_data_frames.html#importing-from-excel",
    "href": "03_data_frames.html#importing-from-excel",
    "title": "Data frames",
    "section": "3.3 Importing from Excel",
    "text": "3.3 Importing from Excel\nMicrosoft Excel is more than just a spreadsheet, it can contain macros, formulae, multiple worksheets and formatting. There are numerous ways to import xlsx files into R, yet depending on the complexity of the original files, the translations can be incomplete and inconsistent.\nOne of the easiest and safest ways to import data from Excel is either to save the worksheet as a text file (comma or tab delimited) and import the data as a text file (see above), or to copy the data to the clipboard in Excel and import the clipboard data into R.\nNevertheless, it is also possible to directly import a sheet from an excel workbook. Tidyverse includes a package called readxl, however as it is not one of the ‘core’ packages, it is not automatically loaded as part of the ecosystem when the tidyverse package is loaded. Hence to use the readxl package, it must be explicitly loaded.\n\nlibrary(readxl)\nmacnally_new &lt;- read_xlsx(\"../data/macnally.xlsx\", sheet = \"macnally\", trim_ws = TRUE)",
    "crumbs": [
      "R basics",
      "Data frames"
    ]
  },
  {
    "objectID": "03_data_frames.html#summary",
    "href": "03_data_frames.html#summary",
    "title": "Data frames",
    "section": "4.1 summary()",
    "text": "4.1 summary()\nThe summary() function is an overloaded function whose behaviour depends on the object passed to the function. When summary() is called with a data.frame, a summary is provided in which:\n\nnumeric vectors (variables) are summarized by the standard 5 number statistics and if there are any missing values, the number of missing values is also provided\ncategorical (factors) vectors are tallied up - that is, the number of instances of each level are counted.\nboolean states are also tallied\ncharacter vectors are only described by their length\ndate (and POSIX) vectors are summarized by 5 number summaries\n\n\nsummary(macnally)\n\n        habitat       gst            eyr           bool        \n Gipps.Manna:3   Min.   :3.00   Min.   :0.000   Mode :logical  \n Mixed      :5   1st Qu.:3.40   1st Qu.:4.025   FALSE:4        \n                 Median :5.10   Median :5.150   TRUE :4        \n                 Mean   :5.60   Mean   :5.013                  \n                 3rd Qu.:8.15   3rd Qu.:5.975                  \n                 Max.   :8.40   Max.   :9.200                  \n     char                date           \n Length:8           Min.   :2000-02-29  \n Class :character   1st Qu.:2000-03-18  \n Mode  :character   Median :2000-04-05  \n                    Mean   :2000-04-05  \n                    3rd Qu.:2000-04-23  \n                    Max.   :2000-05-12",
    "crumbs": [
      "R basics",
      "Data frames"
    ]
  },
  {
    "objectID": "03_data_frames.html#str",
    "href": "03_data_frames.html#str",
    "title": "Data frames",
    "section": "4.2 str()",
    "text": "4.2 str()\nSimilar to summary(), the str() function is an overloaded. The str() function generally produces a compact view of the structure of an object. When str() is called with a data.frame, this compact view comprises a nested list of abbreviated structures.\n\nstr(macnally)\n\n'data.frame':   8 obs. of  6 variables:\n $ habitat: Factor w/ 2 levels \"Gipps.Manna\",..: 2 1 1 1 2 2 2 2\n $ gst    : num  3.4 3.4 8.4 3 5.6 8.1 8.3 4.6\n $ eyr    : num  0 9.2 3.8 5 5.6 4.1 7.1 5.3\n $ bool   : logi  TRUE FALSE TRUE FALSE TRUE FALSE ...\n $ char   : chr  \"Large\" \"Small\" \"Large\" \"Small\" ...\n $ date   : Date, format: \"2000-02-29\" \"2000-03-10\" ...",
    "crumbs": [
      "R basics",
      "Data frames"
    ]
  },
  {
    "objectID": "03_data_frames.html#glimpse",
    "href": "03_data_frames.html#glimpse",
    "title": "Data frames",
    "section": "4.3 glimpse()",
    "text": "4.3 glimpse()\nThe glimpse() function in the tibble package is similar to str() except that it attempts to maximize the amount of data displayed according to the dimensions of the output.\n\nglimpse(macnally)\n\nRows: 8\nColumns: 6\n$ habitat &lt;fct&gt; Mixed, Gipps.Manna, Gipps.Manna, Gipps.Manna, Mixed, Mixed, Mi…\n$ gst     &lt;dbl&gt; 3.4, 3.4, 8.4, 3.0, 5.6, 8.1, 8.3, 4.6\n$ eyr     &lt;dbl&gt; 0.0, 9.2, 3.8, 5.0, 5.6, 4.1, 7.1, 5.3\n$ bool    &lt;lgl&gt; TRUE, FALSE, TRUE, FALSE, TRUE, FALSE, TRUE, FALSE\n$ char    &lt;chr&gt; \"Large\", \"Small\", \"Large\", \"Small\", \"Large\", \"Small\", \"Large\",…\n$ date    &lt;date&gt; 2000-02-29, 2000-03-10, 2000-03-20, 2000-03-31, 2000-04-10, 20…",
    "crumbs": [
      "R basics",
      "Data frames"
    ]
  },
  {
    "objectID": "03_data_frames.html#others",
    "href": "03_data_frames.html#others",
    "title": "Data frames",
    "section": "4.4 Others",
    "text": "4.4 Others\nThere are also numerous graphical methods including view() and fix(), however, I have focused on the script friendly routines. As the graphical routines require user input, they are inappropriate to include in scripts.\nWithin Rstudio, a data frame can be viewed like a spreadsheet. To view the data this way, click on the name of the data frame within the Environment pane. Furthermore, when in R Notebook mode, a simple functioning spreadsheet will be embedded within the notebook.",
    "crumbs": [
      "R basics",
      "Data frames"
    ]
  },
  {
    "objectID": "03_data_frames.html#saverdsreadrds",
    "href": "03_data_frames.html#saverdsreadrds",
    "title": "Data frames",
    "section": "6.1 saveRDS/readRDS",
    "text": "6.1 saveRDS/readRDS\nFor example:\n\n## save just the macnally data frame to the data folder\nsaveRDS(macnally, file = \"../data/macnally.rds\")\n\nThis will save a single object in a compressed format.\nThe saved object(s) can be loaded during subsequent sessions by providing the name of the saved workspace image file as an argument to the load() function. For example:\n\nmacnally &lt;- readRDS(\"../data/macnally_stats.rds\")",
    "crumbs": [
      "R basics",
      "Data frames"
    ]
  },
  {
    "objectID": "03_data_frames.html#saveload",
    "href": "03_data_frames.html#saveload",
    "title": "Data frames",
    "section": "6.2 save/load",
    "text": "6.2 save/load\nWhen you want to save multiple objects, the save() function is convenient. This stores multiple objects in a binary (non-compressed) format.\n\n## save just the macnally data frame to the data folder\nsave(macnally, file = \"../data/macnally.RData\")\n## calculate the mean gst\nmean_gst &lt;- mean(macnally$gst)\n## display the mean gst\nmean_gst\n## save the macnally data frame as well as the mean gst object\nsave(macnally, mean_gst, file = \"../data/macnally_stats.RData\")\n\nThe saved object(s) can be loaded during subsequent sessions by providing the name of the saved workspace image file as an argument to the load() function. For example:\n\nload(\"../data/macnally_stats.RData\")\n\nNote, the load() reads the object(s) into the current environment with each object being assigned the names they were originally assigned when they were saved.",
    "crumbs": [
      "R basics",
      "Data frames"
    ]
  },
  {
    "objectID": "03_data_frames.html#dump",
    "href": "03_data_frames.html#dump",
    "title": "Data frames",
    "section": "6.3 dump",
    "text": "6.3 dump\nSimilarly, a straight un-encoded text version of an object (including dataframes and tibbles) can be saved or added to a text file (such as an R script) using the dump() function.\n\ndump(\"macnally\", file = \"../data/macnally\")\n\nIf the file character string is left empty, the text representation of the object will be written to the console. This output can then be viewed or copied and pasted into a script file, thereby providing a convenient way to bundle together data sets along with graphical and analysis commands that act on the data sets. It can even be used to paste data into an email.\n\ndump(\"macnally\", file = \"\")\n\nThereafter, the dataset is automatically included when the script is sourced and cannot accidentally become separated from the script.",
    "crumbs": [
      "R basics",
      "Data frames"
    ]
  },
  {
    "objectID": "04_data_wrangling.html",
    "href": "04_data_wrangling.html",
    "title": "Data wrangling (tidyverse and friends)",
    "section": "",
    "text": "I apologize in advance, this tutorial requires quite a bit of explaining and context before it can get into the code…. Good data manipulation is an art form that requires the use of many specific tools (functions) and expert data manipulation comes from the integration of these tools together. Therefore it is necessary to have an overview of the tool set before investigating any single tool.",
    "crumbs": [
      "R basics",
      "Data wrangling"
    ]
  },
  {
    "objectID": "04_data_wrangling.html#the-grammar-of-data-manipulation",
    "href": "04_data_wrangling.html#the-grammar-of-data-manipulation",
    "title": "Data wrangling (tidyverse and friends)",
    "section": "2.1 The grammar of data manipulation",
    "text": "2.1 The grammar of data manipulation\nHadley and his collaborators argue that there is a grammar of data manipulation and that manipulations comprises a set of verbs. Furthermore, the philosophy adheres to the UNIX ideal that rather than attempt to build large complex tools that perform a large set of operations, it is better (with regards to usability and robustness) to build a tool set in which each tool performs a single, specific task and that the individual tools can be bolted together to achieve the complex operations.\nThe core of the tidyverse data manipulation ecosystem tools (verbs) can be organised into five categories within two types on the basis of what aspects of the data they operate on:\n\noperate on a single data set\n\noperate on the rows\n\ndplyr::arrange - changing the order of the rows (sorting)\ndplyr::filter - subset of rows based on column values\ndplyr::slice - subset of rows based on position\n\noperate on the columns\n\ndplyr::select - subset of columns\ndplyr::rename - change the names of columns\ndplyr::pull - extract a single column as a vector\ndplyr::distinct - unique combinations of column values\ndplyr::mutate - adding columns and modifying column values\ntidyr::unite - combine multiple columns together\ntidyr::separate - separating a single column into multiple columns\n\noperate on groups of columns\n\ndplyr::summarise - aggregating (collapsing) to a single row\ndplyr::count - count the number of unique combinations single row\ndplyr::group_by - define groups of rows\n\nreshapes (pivots) the data\n\ntidyr::pivot_longer - lengthen data from wide format\ntidyr::pivot_wider - widen data from long format\n\n\noperate of two data sets\n\n_join - merge (join) two data sets together based on common field(s)\n\n\nIn base R, variables (columns) are referred to by either their name (as a prefix to the environment - the data.frame etc), name (as a string) or their index (position). For example to access a variable called “Var1” within a dataset called “data”, we could use either data$Var1, data[,\"Var1\"] or data[,1] (if the focal variable was in the first column).\nThe tidyverse ecosystem adopts an alternative concept called tidy evaluation to make referring to columns more flexible.\n\ndata-masking - refer to variables as if they were directly attached to the current environment (e.g. Var1 instead of data$Var1. This form of variable referral is used within:\n\narrange()\nfilter()\ncount()\nmutate()\nsummarise()\ngroup_by()\n\ntidy selection - refer to variables by their position, name or type (e.g. starts_with(\"var\")). This form of variable referral is used within:\n\nselect()\nrename()\npull()\nacross() - which brings tidy selection semantics to data-masking (and thus functions like mutate())\n\nThe following table highlights the various tidy-selection options. The examples all illustrate a hypothetical data set (data) with hypothetical columns (Var1, Var2, etc) and cannot be evaluated directly. They all mostly illustrate the concept using the select() function.\n\n\n\n\n\ntidy-selection\n\n\nDescription\n\n\nExamples\n\n\n\n\n\n\nBare names\n\n\nSelect columns based on their names.Columns can be excluded by prepending the name with a negative sign (-).\n\n\n\nselect(data, Var1)\n\n\nselect(data, Var1, Var2)\n\n\nselect(data, c(Var1, Var2))\n\n\nselect(data, -Var1)\n\n\n\n\n\nRanges of columns\n\n\nSelect columns based on their a range of names or column numbers. The selections will be inclusive. Prepending with a negative sign (-) acts as an exclusion.\n\n\n\nselect(data, Var1:Var3)\n\n\nselect(data, 2:4)\n\n\n\n\n\nBoolean helper functions -contains() -starts_with() -ends_with() -matches() -num_range() -everything() -where() -all_of() -any_of()\n\n\nSelect columns based on evaluating functions on the column names - contains() - names containing the string - starts_with() - names starting with the string - ends_with() - names starting with the string - matches() - names matched with a regex - num_range() - names that start with string followed by numbers - everything() - all variables - useful in combination with other selections - where() - variable inclusion predicated by a function - all_of() - all variables included in a character vector - any_of() - any variables included in a character vector Each of the above can be reversed by prepending with either a exclamation sign (!) or negative sign (-). Also note that by default, these are not case sensitive.\n\n\n\nselect(data, contains(\"Var\"))\n\n\nselect(data, !contains(\"Var\"))\n\n\nselect(data, starts_with(\"Var\"))\n\n\nselect(data, ends_with(\"Var\"))\n\n\nselect(data, matches(\"^.var[1-5]$\"))\n\n\nselect(data, num_range(\"Var\", 1:2))\n\n\nselect(data, Var3, everything())\n\n\nselect(data, where(is.numeric))\n\n\nvars &lt;- c(\"Var1\", \"Var2\")\nselect(data, all_of(vars))\n\n\nvars &lt;- c(\"Var1\", \"Other\")\nselect(data, any_of(vars))",
    "crumbs": [
      "R basics",
      "Data wrangling"
    ]
  },
  {
    "objectID": "04_data_wrangling.html#piping",
    "href": "04_data_wrangling.html#piping",
    "title": "Data wrangling (tidyverse and friends)",
    "section": "2.2 Piping",
    "text": "2.2 Piping\nTypically, data manipulation/preparations comprise multiple steps and stages in which a series of alterations, additions etc are performed sequentially such that the data are progressively molded into a form appropriate for analysis etc. That is, to achieve the desired result, we must bolt multiple tools (verbs) together.\nTraditionally, this would have involved a separate expression for each step often resulting in the generation of numerous intermediate data sets. Furthermore, in an attempt to reduce the number of intermediate steps, functions are often nested within other functions such that alterations are made inline within another function.\nFor example, the following pseudo code examples illustrates the traditional and nested approaches to achieving numerous data manipulation steps:\n\n\nTraditional\n\ndata1 &lt;- select(data, ...)\ndata2 &lt;- group_by(data1, ...)\ndata3 &lt;- summarise(data2, ...)\n\n\nNested functions\n\ndata &lt;- summarise(group_by(select(data, ...)))\n\n\n\nCollectively, these practices can yield code that is very difficult to read and interpret.\nA long honoured unix coding principle is that each application should focus on performing one action and performing that function well. In order to perform a sequence of actions therefore involves piping (via the unix pipe character |) the output of one application to the input of another application and so on in a chain. The grammar of data wrangling also adopts the principle of each tool specializing on one action and tools should be piped together to achieve a sequence of actions.\nThe piping (glue) operator in R (as of version 4.1) is |&gt;. An object on the left hand side of the |&gt; operator is passed as the first argument of the function on the right hand side.\n\n\n\n\n\n\nNative vs the magrittr (%&gt;%) pipe\n\n\n\n\n\nPrior to the native adoption of the pipe character in R, piping was supported via the magrittr package. Within this package, the pipe operator is %&gt;%. Although both the native and magrittr pipe operators are somewhat analogous, they are not homologous.\n\nSyntax:\n\nNative pipe: Simple |&gt; symbol placed between the expression and function call.\nMagrittr pipe: Double percentage signs %&gt;% offering visual distinction from surrounding code.\n\nPlaceholder:\n\nNative pipe: No built-in placeholder for piped-in values. Functions need to handle them explicitly.\nMagrittr pipe: Dot placeholder (.), allowing concise access to piped-in values within the function call.\n\nFunctionality:\n\nNative pipe: More basic functionality, primarily focused on function chaining.\nMagrittr pipe: Offers additional features like:\n\nExposition pipe (%~$%): Displays the values passed through the pipe, facilitating debugging.\nInterjection pipe (~&gt;%): Modifies the piped-in value before passing it to the next function.\nLazy evaluation: Delays intermediate calculations until necessary, potentially improving performance.\n\n\nScope:\n\nNative pipe: Recently introduced, integrated directly into R Base package.\nMagrittr pipe: A separate package, offering more extensibility and customization options.\n\nAdoption:\n\nNative pipe: Gaining popularity due to its simplicity and integration with base R.\nMagrittr pipe: Widely used, having established a large community and ecosystem of compatible packages.\n\nChoosing between them depends on:\n\nPersonal preference: Some prefer the visual clarity of %&gt;%, while others favor the simplicity of |&gt;.\nProject requirements: Consider the need for features like the dot placeholder or additional pipe types.\nTeam collaboration: Choose a consistent style for shared code or adhere to project standards.\n\n\nUltimately, both pipes are powerful tools for data manipulation in R. Understanding their differences helps you choose the most appropriate option for your specific needs and coding style.\n\n\n\nIn pseudo code, the piping approach to the above manipulation would be:\n\ndata &lt;- data |&gt;\n    select(...) |&gt;\n    group_by(...) |&gt;\n    summarise(...)\n\nIf the code is set out as above (with each verb on a separate line), it gives an opportunity to provide a short comment to the right side of each line to provide useful documentation.\n\n\nA more specific example\n\nAs a motivating (if not a bit extreme) example, lets say we wanted to calculate the logSumExp function:\n\\[\nlog(\\sum^{n}_{i=1} e^{x_i})\n\\]\n\n## Generate some data\nset.seed(123)\nx &lt;- rgamma(10,5,1)\n## Calculate the logSumExp\nlog(sum(exp(x)))\n\n[1] 9.316408\n\n## OR\nx1 &lt;- exp(x)\nx2 &lt;-sum(x1)\nlog(x2)\n\n[1] 9.316408\n\n\nThe piping approach could be:\n\nx |&gt; exp() |&gt; sum() |&gt; log()\n\n[1] 9.316408\n\n\nTo reiterate, the following three are equivalent:\n\nexp(x)\n\n [1]   29.653639 1601.918872    5.101634  118.918637 7140.686681  252.361318\n [7]    9.175114    4.863565 1756.825350  199.466617\n\nx |&gt; exp()\n\n [1]   29.653639 1601.918872    5.101634  118.918637 7140.686681  252.361318\n [7]    9.175114    4.863565 1756.825350  199.466617\n\n\nas are the following:\n\nlog(x, base=10)\n\n [1] 0.5301465 0.8679950 0.2120706 0.6792861 0.9480981 0.7427928 0.3456667\n [8] 0.1991438 0.8733941 0.7239190\n\nx |&gt; log(base=10)\n\n [1] 0.5301465 0.8679950 0.2120706 0.6792861 0.9480981 0.7427928 0.3456667\n [8] 0.1991438 0.8733941 0.7239190\n\n\n\nMost of the following examples will demonstrate isolated data manipulation actions (such as filtering, summarising or joining) as this focuses on the specific uses of these functions without the distractions and complications of other actions. For isolated uses, piping has little (if any) advantages. Nevertheless, in recognition that data manipulations rarely comprise a single action (rather they are a series of linked actions), for all remaining examples demonstrated in the tidyverse (dplyr/tidyr) context, piping will be used.",
    "crumbs": [
      "R basics",
      "Data wrangling"
    ]
  },
  {
    "objectID": "04_data_wrangling.html#tibbles",
    "href": "04_data_wrangling.html#tibbles",
    "title": "Data wrangling (tidyverse and friends)",
    "section": "2.3 Tibbles",
    "text": "2.3 Tibbles\ndata.frame’s are collections of variables of the identical length (yet not necessarily the same type) that the fundamental data structures used by most modelling routines in R. Nevertheless, there are numerous properties of data.frames that make them less than ideal. tibbles have been more recently engineered to address these shortcomings:\n\nif a data.frame is very large, the print method can result output that is too large to be useful. By contrast, tibbles truncate the output to a maximum of 10 rows and as many columns as will fit in the output. The tibble print method also displays the class (type) of each column (variable).\n\n\nshow comparison\n\n\n\n\ndat.1 |&gt; as.data.frame() \n\n   Treatment Plot Dose Time  Resp1  Resp2\n1    Control   P1    H    1   8.12   3.06\n2    Control   P1    H    2  20.55  25.94\n3    Control   P1    H    3  27.49  29.85\n4    Control   P1    H    4  44.79  25.39\n5    Control   P1    M    1  20.99  20.31\n6    Control   P1    M    2  37.54  17.62\n7    Control   P1    M    3  61.46  98.44\n8    Control   P1    M    4  82.21 160.01\n9    Control   P1    L    1  31.73  21.22\n10   Control   P1    L    2  59.08  37.51\n11   Control   P1    L    3  94.54 119.22\n12   Control   P1    L    4 121.17 116.45\n13   Control   P2    H    1   8.14  23.93\n14   Control   P2    H    2  13.36  28.02\n15   Control   P2    H    3  33.37  37.17\n16   Control   P2    H    4  39.87  38.25\n17   Control   P2    M    1  19.95  19.73\n18   Control   P2    M    2  42.83  40.52\n19   Control   P2    M    3  62.46   4.81\n20   Control   P2    M    4  81.78 136.66\n21   Control   P2    L    1  32.76  30.70\n22   Control   P2    L    2  62.35 123.78\n23   Control   P2    L    3  90.22 113.87\n24   Control   P2    L    4 114.03  76.52\n25 Exclusion   P3    H    1  21.86  23.58\n26 Exclusion   P3    H    2  39.83  28.03\n27 Exclusion   P3    H    3  59.53  21.32\n28 Exclusion   P3    H    4  75.59  90.76\n29 Exclusion   P3    M    1  38.57  30.63\n30 Exclusion   P3    M    2  81.25  83.61\n31 Exclusion   P3    M    3 124.08 124.09\n32 Exclusion   P3    M    4 159.69 112.65\n33 Exclusion   P3    L    1  61.16  39.53\n34 Exclusion   P3    L    2 119.84 110.27\n35 Exclusion   P3    L    3 175.87 286.33\n36 Exclusion   P3    L    4 238.76  54.23\n37 Exclusion   P4    H    1  18.82  28.60\n38 Exclusion   P4    H    2  39.82  39.07\n39 Exclusion   P4    H    3  63.30  93.43\n40 Exclusion   P4    H    4  82.29  60.15\n41 Exclusion   P4    M    1  39.51  45.90\n42 Exclusion   P4    M    2  79.24  88.04\n43 Exclusion   P4    M    3 122.09  84.19\n44 Exclusion   P4    M    4 161.67 256.34\n45 Exclusion   P4    L    1  57.93  85.24\n46 Exclusion   P4    L    2 117.88 167.90\n47 Exclusion   P4    L    3 181.09 314.49\n48 Exclusion   P4    L    4 242.31 304.70\n\n\n\n\ndat.1 |&gt; as_tibble()\n\n# A tibble: 48 × 6\n   Treatment Plot  Dose   Time Resp1  Resp2\n   &lt;fct&gt;     &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 Control   P1    H         1  8.12   3.06\n 2 Control   P1    H         2 20.6   25.9 \n 3 Control   P1    H         3 27.5   29.8 \n 4 Control   P1    H         4 44.8   25.4 \n 5 Control   P1    M         1 21.0   20.3 \n 6 Control   P1    M         2 37.5   17.6 \n 7 Control   P1    M         3 61.5   98.4 \n 8 Control   P1    M         4 82.2  160.  \n 9 Control   P1    L         1 31.7   21.2 \n10 Control   P1    L         2 59.1   37.5 \n# ℹ 38 more rows\n\n\n\n\n\ndata.frames have very strict column naming rules and when these are not satisfied, the names will be altered in order to adhere to the rules. tibbles permit a wider range of names.\n\n\nshow comparison\n\n\n\n\ndata.frame('1.3' = 1:6, 'Wt (kg)' = 1:6)\n\n  X1.3 Wt..kg.\n1    1       1\n2    2       2\n3    3       3\n4    4       4\n5    5       5\n6    6       6\n\n\n\n\ntibble('1.3' = 1:6, 'Wt (kg)' = 1:6)\n\n# A tibble: 6 × 2\n  `1.3` `Wt (kg)`\n  &lt;int&gt;     &lt;int&gt;\n1     1         1\n2     2         2\n3     3         3\n4     4         4\n5     5         5\n6     6         6\n\n\n\n\n\ncharacter vectors are often coerced into factors (categorical variables). This is not the case with tibbles.\nwhen vectors are added that are of a different length to the others in the data.frame, the values of the shorter vector(s) are recycled until all vectors are the same length. This behaviour can be dangerous and is not permitted in tibbles (except for vectors of length of one).\n\n\nshow comparison\n\n\n\n\ndata.frame(Var1 = 1:6, Var2 = 1:2, Var3 = 1)\n\n  Var1 Var2 Var3\n1    1    1    1\n2    2    2    1\n3    3    1    1\n4    4    2    1\n5    5    1    1\n6    6    2    1\n\n\n\n\ntibble(Var1 = 1:6, Var2 = 1:2, Var3 = 1)\n\nError in `tibble()`:\n! Tibble columns must have compatible sizes.\n• Size 6: Existing data.\n• Size 2: Column `Var2`.\nℹ Only values of size one are recycled.\n\ntibble(Var1 = 1:6, Var2 = 1)\n\n# A tibble: 6 × 2\n   Var1  Var2\n  &lt;int&gt; &lt;dbl&gt;\n1     1     1\n2     2     1\n3     3     1\n4     4     1\n5     5     1\n6     6     1\n\n\n\n\n\nwhen sub-setting via column indices, data.frames return a vector (when only a single index is provided) or a data.frame (if multiple indices are provided). This inconsistency is problematic in the context of a processing pipeline. tibble’s will always return a tibble from column indices.\n\n\nshow comparison\n\n\n\n\nas.data.frame(dat.1)[,2]\n\n [1] P1 P1 P1 P1 P1 P1 P1 P1 P1 P1 P1 P1 P2 P2 P2 P2 P2 P2 P2 P2 P2 P2 P2 P2 P3\n[26] P3 P3 P3 P3 P3 P3 P3 P3 P3 P3 P3 P4 P4 P4 P4 P4 P4 P4 P4 P4 P4 P4 P4\nLevels: P1 P2 P3 P4\n\n\n\n\nas_tibble(dat.1)[,2]\n\n# A tibble: 48 × 1\n   Plot \n   &lt;fct&gt;\n 1 P1   \n 2 P1   \n 3 P1   \n 4 P1   \n 5 P1   \n 6 P1   \n 7 P1   \n 8 P1   \n 9 P1   \n10 P1   \n# ℹ 38 more rows\n\n\n\n\n\ndata.frames permit partial matching via $ indexing. This can be problematic in the context of a processing pipeline. tibbles expressly forbid this.\n\n\nshow comparison\n\n\n\n\nas.data.frame(dat.1)$Plo\n\n [1] P1 P1 P1 P1 P1 P1 P1 P1 P1 P1 P1 P1 P2 P2 P2 P2 P2 P2 P2 P2 P2 P2 P2 P2 P3\n[26] P3 P3 P3 P3 P3 P3 P3 P3 P3 P3 P3 P4 P4 P4 P4 P4 P4 P4 P4 P4 P4 P4 P4\nLevels: P1 P2 P3 P4\n\n\n\n\nas_tibble(dat.1)$Plo\n\nWarning: Unknown or uninitialised column: `Plo`.\n\n\nNULL\n\n\n\n\n\ndata.frames have very clumsy support for list columns (a list column is a column whose cells contain lists). List columns are better supported in tibbles. The ability to support list columns is an integral for the functional programming routines of the purrr package.\n\n\nshow comparison\n\nIn this fictitious example, I would like to store three different data.frames (or any three objects for that matter) in the three cells of a variable (e.g. Var2). This is not possible in a data.frame - it will coerce the Var2 column into multiple columns (one for each object) and thus will also recycle the length of each to ensure they are all the same length. Once the data have been coerced into a data.frame, the structure is lost (we can no longer access Var2).\nThese issues are addressed in the tibble.\n\n\n\nD &lt;- data.frame(Var1 = LETTERS[1:3],\n           Var2 = list(\n               data.frame(a = 1:6),\n               data.frame(a = 1:3),\n               data.frame(a = 1:2)\n           ))\nD\n\n  Var1 Var2.a Var2.a.1 Var2.a.2\n1    A      1        1        1\n2    B      2        2        2\n3    C      3        3        1\n4    A      4        1        2\n5    B      5        2        1\n6    C      6        3        2\n\nD$Var2\n\nNULL\n\n\n\n\nD &lt;- tibble(Var1 = LETTERS[1:3],\n       Var2 = list(\n           data.frame(a = 1:6),\n           data.frame(a = 1:3),\n           data.frame(a = 1:2)\n           ))\nD\n\n# A tibble: 3 × 2\n  Var1  Var2        \n  &lt;chr&gt; &lt;list&gt;      \n1 A     &lt;df [6 × 1]&gt;\n2 B     &lt;df [3 × 1]&gt;\n3 C     &lt;df [2 × 1]&gt;\n\nD$Var2\n\n[[1]]\n  a\n1 1\n2 2\n3 3\n4 4\n5 5\n6 6\n\n[[2]]\n  a\n1 1\n2 2\n3 3\n\n[[3]]\n  a\n1 1\n2 2",
    "crumbs": [
      "R basics",
      "Data wrangling"
    ]
  },
  {
    "objectID": "04_data_wrangling.html#summary-functions",
    "href": "04_data_wrangling.html#summary-functions",
    "title": "Data wrangling (tidyverse and friends)",
    "section": "3.1 Summary functions",
    "text": "3.1 Summary functions\n\n\n\n\nFunction\n\n\nDescription\n\n\nExamples\n\n\n\n\n\n\nmean(), median()\n\n\nArithmetic mean and median.\nNote, boolean vectors are stored as a set of 0 (FALSE) and 1 (TRUE) and thus mathematical functions operate on them as if they were numbers.\n\n\n\nmean(x)\n\n[1] 4.814615\n\nmedian(B)\n\n[1] 1\n\n\n\n\n\n\nsum(), prod()\n\n\nSum and product.\n\n\n\nsum(x)\n\n[1] 48.14615\n\nprod(B)\n\n[1] 0\n\n\n\n\n\n\nvar(), sd()\n\n\nVariance and standard deviation.\n\n\n\nvar(x)\n\n[1] 6.692355\n\nsd(B)\n\n[1] 0.5163978\n\n\n\n\n\n\nmad(), IQR()\n\n\nMedian Absolute Deviation and Inter-Quartile Range.\n\n\n\nmad(x)\n\n[1] 3.540549\n\nIQR(B)\n\n[1] 1\n\n\n\n\n\n\nmin(), max()\n\n\nMinimum and maximum.\n\n\n\nmin(x)\n\n[1] 1.581772\n\nmax(B)\n\n[1] 1\n\n\n\n\n\n\nquantile()\n\n\nQuantiles\n\n\n\nquantile(x)\n\n      0%      25%      50%      75%     100% \n1.581772 2.509767 5.037043 6.916934 8.873564 \n\nquantile(x, p = 0.25)\n\n     25% \n2.509767 \n\n\n\n\n\n\ndplyr::first(), dplyr::last(), dplyr::nth()\n\n\nFirst, laste and nth value.\n\n\n\nfirst(x)\n\n[1] 3.389585\n\nfirst(x, order_by = A)\n\n[1] 3.389585\n\nlast(B)\n\n[1] TRUE\n\nnth(A, n = 4)\n\n[1] \"b\"\n\n\n\n\n\n\ndplyr::n(), dplyr::n_distinct()\n\n\nNumber of values, number of distinct values.\nn() is a special case that can only be used within the context of a data.frame or tibble.\n\n\n\nn_distinct(A)\n\n[1] 2",
    "crumbs": [
      "R basics",
      "Data wrangling"
    ]
  },
  {
    "objectID": "04_data_wrangling.html#vectorised-functions",
    "href": "04_data_wrangling.html#vectorised-functions",
    "title": "Data wrangling (tidyverse and friends)",
    "section": "3.2 Vectorised functions",
    "text": "3.2 Vectorised functions\n\n\n\n\nFunction\n\n\nDescription\n\n\nExamples\n\n\n\n\n\n\n+,-,*,/,^,%/%, %%\n\n\ntypical arithmetic operators\n\n\n\nx + 2\n\n [1]  5.389585  9.378957  3.629561  6.778440\n [5] 10.873564  7.530862  4.216495  3.581772\n [9]  9.471264  7.295647\n\nx * B\n\n [1] 0.000000 7.378957 1.629561 4.778440\n [5] 8.873564 0.000000 2.216495 0.000000\n [9] 0.000000 5.295647\n\n\n\n\n\n\nlog(),log2(),log10(), exp()\n\n\nlogarithms and exponentials\n\n\n\nlog(x)\n\n [1] 1.2207074 1.9986324 0.4883105 1.5641140\n [5] 2.1830765 1.7103437 0.7959271 0.4585455\n [9] 2.0110642 1.6668851\n\nlog10(x)\n\n [1] 0.5301465 0.8679950 0.2120706 0.6792861\n [5] 0.9480981 0.7427928 0.3456667 0.1991438\n [9] 0.8733941 0.7239190\n\nexp(x)\n\n [1]   29.653639 1601.918872    5.101634\n [4]  118.918637 7140.686681  252.361318\n [7]    9.175114    4.863565 1756.825350\n[10]  199.466617\n\n\n\n\n\n\n&lt;,&lt;=,&gt;,&gt;=,!=,==\n\n\nlogical operators\n\n\n\nx &lt; 5\n\n [1]  TRUE FALSE  TRUE  TRUE FALSE FALSE\n [7]  TRUE  TRUE FALSE FALSE\n\nB == TRUE\n\n [1] FALSE  TRUE  TRUE  TRUE  TRUE FALSE\n [7]  TRUE FALSE FALSE  TRUE\n\n\n\n\n\n\nbetween()\n\n\nWhether a value is between two numbers\n\n\n\nbetween(x, 3, 5)\n\n [1]  TRUE FALSE FALSE  TRUE FALSE FALSE\n [7] FALSE FALSE FALSE FALSE\n\n\n\n\n\n\nnear()\n\n\nA safe way of assessing equality (==) in floating points\n\n\n\nx == 3.39\n\n [1] FALSE FALSE FALSE FALSE FALSE FALSE\n [7] FALSE FALSE FALSE FALSE\n\nnear(x, y = 3.39, tol =0.01)\n\n [1]  TRUE FALSE FALSE FALSE FALSE FALSE\n [7] FALSE FALSE FALSE FALSE\n\n\n\n\n\n\nlag(), lead()\n\n\nshift cases down/up by one\n\n\n\nlag(x)\n\n [1]       NA 3.389585 7.378957 1.629561\n [5] 4.778440 8.873564 5.530862 2.216495\n [9] 1.581772 7.471264\n\nlead(x)\n\n [1] 7.378957 1.629561 4.778440 8.873564\n [5] 5.530862 2.216495 1.581772 7.471264\n [9] 5.295647       NA\n\n\n\n\n\n\ncummax(), cummin(), dplyr::cummean()\n\n\nCumulative max, min and mean\n\n\n\ncummax(x)\n\n [1] 3.389585 7.378957 7.378957 7.378957\n [5] 8.873564 8.873564 8.873564 8.873564\n [9] 8.873564 8.873564\n\ncummin(B)\n\n [1] 0 0 0 0 0 0 0 0 0 0\n\ncummean(x)\n\n [1] 3.389585 5.384271 4.132701 4.294136\n [5] 5.210021 5.263495 4.828209 4.422404\n [9] 4.761167 4.814615\n\ncummean(x &gt; 2)\n\n [1] 1.0000000 1.0000000 0.6666667 0.7500000\n [5] 0.8000000 0.8333333 0.8571429 0.7500000\n [9] 0.7777778 0.8000000\n\ncummean(B)\n\n [1] 0.0000000 0.5000000 0.6666667 0.7500000\n [5] 0.8000000 0.6666667 0.7142857 0.6250000\n [9] 0.5555556 0.6000000\n\n\n\n\n\n\ncumsum(), cumprod()\n\n\nCumulative sum and product\n\n\n\ncumsum(x)\n\n [1]  3.389585 10.768542 12.398103 17.176543\n [5] 26.050107 31.580969 33.797464 35.379235\n [9] 42.850499 48.146146\n\ncumsum(x &gt; 3)\n\n [1] 1 2 2 3 4 5 5 5 6 7\n\ncumprod(B)\n\n [1] 0 0 0 0 0 0 0 0 0 0\n\n\n\n\n\n\ndplyr::cumall(), dplyr::cumany()\n\n\nCumulative all and any (mainly for use with filtering).\n\n\n\ncumall(x)\n\n [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE\n [9] TRUE TRUE\n\ncumall(x &gt; 2)\n\n [1]  TRUE  TRUE FALSE FALSE FALSE FALSE\n [7] FALSE FALSE FALSE FALSE\n\ncumany(x &gt; 4)\n\n [1] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n [7]  TRUE  TRUE  TRUE  TRUE\n\ncumany(B)\n\n [1] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n [7]  TRUE  TRUE  TRUE  TRUE\n\ncumall(B)\n\n [1] FALSE FALSE FALSE FALSE FALSE FALSE\n [7] FALSE FALSE FALSE FALSE\n\n\n\n\n\n\nrank(), order()\n\n\nRank and order of items\n\n\n\nrank(x)\n\n [1]  4  8  2  5 10  7  3  1  9  6\n\norder(x)\n\n [1]  8  3  7  1  4 10  6  2  9  5\n\nrank(B)\n\n [1] 2.5 7.5 7.5 7.5 7.5 2.5 7.5 2.5 2.5 7.5\n\n\n\n\n\n\ndplyr::min_rank(), dplyr::dense_rank(), dplyr::percent_rank()\n\n\nRank in which ties = min, without gaps and expressed as a percentage.\n\n\n\nmin_rank(x)\n\n [1]  4  8  2  5 10  7  3  1  9  6\n\ndense_rank(x)\n\n [1]  4  8  2  5 10  7  3  1  9  6\n\npercent_rank(x)\n\n [1] 0.3333333 0.7777778 0.1111111 0.4444444\n [5] 1.0000000 0.6666667 0.2222222 0.0000000\n [9] 0.8888889 0.5555556\n\nrank(B)\n\n [1] 2.5 7.5 7.5 7.5 7.5 2.5 7.5 2.5 2.5 7.5\n\n\n\n\n\n\ndplyr::row_number()\n\n\nRank in which ties = first.\n\n\n\nrow_number(x)\n\n [1]  4  8  2  5 10  7  3  1  9  6\n\n\n\n\n\n\ndplyr::cume_dist()\n\n\nCumulative empirical distribution (proportion less than current value).\n\n\n\ncume_dist(x)\n\n [1] 0.4 0.8 0.2 0.5 1.0 0.7 0.3 0.1 0.9 0.6\n\n\n\n\n\n\ndplyr::ntile()\n\n\nPartition into (n) bins.\n\n\n\nntile(x, n = 3)\n\n [1] 1 3 1 2 3 2 1 1 3 2\n\n\n\n\n\n\ndplyr::if_else()\n\n\nElementwise (case by case) if and else.\n\n\n\nif_else(x &gt; 3, true = \"H\", false = \"L\")\n\n [1] \"H\" \"H\" \"L\" \"H\" \"H\" \"H\" \"L\" \"L\" \"H\" \"H\"\n\n\n\n\n\n\ndplyr::case_when()\n\n\nElementwise multiple if and else.\n\n\n\ncase_when(x &lt;= 3 ~ \"L\",\n          x &gt; 3 & x &lt;= 6 ~ \"M\",\n          x &gt; 6 ~ \"H\")\n\n [1] \"M\" \"H\" \"L\" \"M\" \"H\" \"M\" \"L\" \"L\" \"H\" \"M\"",
    "crumbs": [
      "R basics",
      "Data wrangling"
    ]
  },
  {
    "objectID": "04_data_wrangling.html#subset-columns-select",
    "href": "04_data_wrangling.html#subset-columns-select",
    "title": "Data wrangling (tidyverse and friends)",
    "section": "5.1 Subset columns (select)",
    "text": "5.1 Subset columns (select)\nSelecting works by either including (or excluding) the column names that you indicate or via special selection ‘Helper’ functions that pass a vector of column indices to include in the subset data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirst 10 rows of the `dat.1` data.frame\n\n\nTreatment\nPlot\nDose\nTime\nResp1\nResp2\n\n\n\n\nControl\nP1\nH\n1\n8.12\n3.06\n\n\nControl\nP1\nH\n2\n20.55\n25.94\n\n\nControl\nP1\nH\n3\n27.49\n29.85\n\n\nControl\nP1\nH\n4\n44.79\n25.39\n\n\nControl\nP1\nM\n1\n20.99\n20.31\n\n\nControl\nP1\nM\n2\n37.54\n17.62\n\n\nControl\nP1\nM\n3\n61.46\n98.44\n\n\nControl\nP1\nM\n4\n82.21\n160.01\n\n\nControl\nP1\nL\n1\n31.73\n21.22\n\n\nControl\nP1\nL\n2\n59.08\n37.51\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe select() function users tidy-select semantics.\n\nInclusion / exclusion of from bare namesInclusion / exclusion based on positionInclusion / exclusion from name predicatesSelect and num_rangeReorder columns…Select and all_of / any_ofSelect and where\n\n\nSelect specific variables\n\ndat.1 |&gt; select(Treatment, Dose, Time, Resp1) |&gt;\n    head() \n\n  Treatment Dose Time Resp1\n1   Control    H    1  8.12\n2   Control    H    2 20.55\n3   Control    H    3 27.49\n4   Control    H    4 44.79\n5   Control    M    1 20.99\n6   Control    M    2 37.54\n\n\nExclude specific variables\n\ndat.1 |&gt; select(-Plot, -Resp2) |&gt;\n    head()\n\n  Treatment Dose Time Resp1\n1   Control    H    1  8.12\n2   Control    H    2 20.55\n3   Control    H    3 27.49\n4   Control    H    4 44.79\n5   Control    M    1 20.99\n6   Control    M    2 37.54\n\n\n\n\nInclude columns Treatment through to Time\n\ndat.1 |&gt; select(Treatment:Time) |&gt;\n    head() \n\n  Treatment Plot Dose Time\n1   Control   P1    H    1\n2   Control   P1    H    2\n3   Control   P1    H    3\n4   Control   P1    H    4\n5   Control   P1    M    1\n6   Control   P1    M    2\n\n\nExclude columns Treatment through to Time\n\ndat.1 |&gt; select(-(Treatment:Time)) |&gt;\n    head() \n\n  Resp1 Resp2\n1  8.12  3.06\n2 20.55 25.94\n3 27.49 29.85\n4 44.79 25.39\n5 20.99 20.31\n6 37.54 17.62\n\n\nExclude columns Treatment through to Time\n\ndat.1 |&gt; select(!(Treatment:Time)) |&gt;\n    head() \n\n  Resp1 Resp2\n1  8.12  3.06\n2 20.55 25.94\n3 27.49 29.85\n4 44.79 25.39\n5 20.99 20.31\n6 37.54 17.62\n\n\n\n\nNames containing and ‘r’ (case insensitive)\n\ndat.1 |&gt; select(contains(\"R\")) |&gt;\n    head() \n\n  Treatment Resp1 Resp2\n1   Control  8.12  3.06\n2   Control 20.55 25.94\n3   Control 27.49 29.85\n4   Control 44.79 25.39\n5   Control 20.99 20.31\n6   Control 37.54 17.62\n\n\nNames starting with ‘r’ (case insensitive)\n\ndat.1 |&gt; select(starts_with(\"R\")) |&gt;\n    head() \n\n  Resp1 Resp2\n1  8.12  3.06\n2 20.55 25.94\n3 27.49 29.85\n4 44.79 25.39\n5 20.99 20.31\n6 37.54 17.62\n\n\nNames ending in ‘e’ (case insensitive)\n\ndat.1 |&gt; select(ends_with(\"e\")) |&gt;\n    head() \n\n  Dose Time\n1    H    1\n2    H    2\n3    H    3\n4    H    4\n5    M    1\n6    M    2\n\n\nNames that are exactly four characters long\n\ndat.1 |&gt; select(matches(\"^.{4}$\")) |&gt;\n    head()\n\n  Plot Dose Time\n1   P1    H    1\n2   P1    H    2\n3   P1    H    3\n4   P1    H    4\n5   P1    M    1\n6   P1    M    2\n\n\n\n\n\ndat.1 |&gt; select(num_range(\"Resp\", 1:2)) |&gt;\n    head()\n\n  Resp1 Resp2\n1  8.12  3.06\n2 20.55 25.94\n3 27.49 29.85\n4 44.79 25.39\n5 20.99 20.31\n6 37.54 17.62\n\n\n\n\n\ndat.1 |&gt; select(num_range(\"Resp\", 1:2),\n                 everything()) |&gt;\n    head() \n\n  Resp1 Resp2 Treatment Plot Dose Time\n1  8.12  3.06   Control   P1    H    1\n2 20.55 25.94   Control   P1    H    2\n3 27.49 29.85   Control   P1    H    3\n4 44.79 25.39   Control   P1    H    4\n5 20.99 20.31   Control   P1    M    1\n6 37.54 17.62   Control   P1    M    2\n\n\n\n\nSelect from a vector of strings\n\nVars &lt;- c(\"Resp1\", \"Resp2\")\ndat.1 |&gt; select(all_of(Vars)) |&gt;\n    head()\n\n  Resp1 Resp2\n1  8.12  3.06\n2 20.55 25.94\n3 27.49 29.85\n4 44.79 25.39\n5 20.99 20.31\n6 37.54 17.62\n\n\nSelect from string vector of strings…\n\nVars &lt;- c(\"Resp1\", \"Resp2\", \"Resp3\")\ndat.1 |&gt; select(any_of(Vars)) |&gt;\n    head() \n\n  Resp1 Resp2\n1  8.12  3.06\n2 20.55 25.94\n3 27.49 29.85\n4 44.79 25.39\n5 20.99 20.31\n6 37.54 17.62\n\n\n\nVars &lt;- c(\"Resp1\", \"Resp2\", \"Resp3\")\ndat.1 |&gt; select(-any_of(Vars)) |&gt;\n    head() \n\n  Treatment Plot Dose Time\n1   Control   P1    H    1\n2   Control   P1    H    2\n3   Control   P1    H    3\n4   Control   P1    H    4\n5   Control   P1    M    1\n6   Control   P1    M    2\n\n\n\n\n\ndat.1 |&gt; select(where(is.numeric)) |&gt;\n    head() \n\n  Time Resp1 Resp2\n1    1  8.12  3.06\n2    2 20.55 25.94\n3    3 27.49 29.85\n4    4 44.79 25.39\n5    1 20.99 20.31\n6    2 37.54 17.62\n\n\n\n\n\nSince tibbles permit column names that have special characters in them, in order to refer to a column that has a name with special characters, it is necessary to enclose the name in backticks. For example, to select the variable, Pocillopora damicornis (which contains a space in the name - these are not permitted in data.frames, yet are permitted in tibbles) and print the first six rows:\n\ntikus |&gt;\n    select(`Pocillopora damicornis`) |&gt;\n    head()\n\n   Pocillopora damicornis\nV1                     79\nV2                     51\nV3                     42\nV4                     15\nV5                      9\nV6                     72",
    "crumbs": [
      "R basics",
      "Data wrangling"
    ]
  },
  {
    "objectID": "04_data_wrangling.html#renaming-columns-rename",
    "href": "04_data_wrangling.html#renaming-columns-rename",
    "title": "Data wrangling (tidyverse and friends)",
    "section": "5.2 Renaming columns (rename)",
    "text": "5.2 Renaming columns (rename)\nNote, it is possible to have column names renamed during a select action.\nBoth the rename() and select() functions user tidy-select semantics.\n\ndat.1 |&gt;\n    select(\"Weight\" = Resp1) |&gt;\n    head()\n\n  Weight\n1   8.12\n2  20.55\n3  27.49\n4  44.79\n5  20.99\n6  37.54\n\n\nIf we want to retain the other variables, we would also have to include them in the select either explicitly, or via a helper function.\n\ndat.1 |&gt;\n    select(everything(), \"Weight\" = Resp1) |&gt;\n    head()\n\n  Treatment Plot Dose Time Weight Resp2\n1   Control   P1    H    1   8.12  3.06\n2   Control   P1    H    2  20.55 25.94\n3   Control   P1    H    3  27.49 29.85\n4   Control   P1    H    4  44.79 25.39\n5   Control   P1    M    1  20.99 20.31\n6   Control   P1    M    2  37.54 17.62\n\n\nHowever, note that this might not retain the order of the columns. Hence, for simple renaming of columns, the rename function is more convenient.\n\ndat.1 |&gt;\n    rename(\"Weight\" = Resp1) |&gt;\n    head()\n\n  Treatment Plot Dose Time Weight Resp2\n1   Control   P1    H    1   8.12  3.06\n2   Control   P1    H    2  20.55 25.94\n3   Control   P1    H    3  27.49 29.85\n4   Control   P1    H    4  44.79 25.39\n5   Control   P1    M    1  20.99 20.31\n6   Control   P1    M    2  37.54 17.62\n\n\nIt is also possible to rename columns based on a lookup (list or data.frame). This is handy for cases when data columns have conveniently abbreviated names yet you are preparing data for tabular output - and thus need more descriptive column names.\n\nlookup &lt;- list(\"Response 1\" = \"Resp1\",\n               \"Response 2\" = \"Resp2\")\ndat.1 |&gt;\n    rename(!!!lookup) |&gt;\n    head()\n\n  Treatment Plot Dose Time Response 1 Response 2\n1   Control   P1    H    1       8.12       3.06\n2   Control   P1    H    2      20.55      25.94\n3   Control   P1    H    3      27.49      29.85\n4   Control   P1    H    4      44.79      25.39\n5   Control   P1    M    1      20.99      20.31\n6   Control   P1    M    2      37.54      17.62\n\n\nIn the above example, the big bang operator  !!! forces-splice a list operator. That is, the elements of the list are spliced into the statement as if they had been included directly.\nTo do the same from a data.frame lookup..\n\nlookup &lt;- tribble(\n    ~Abbr_name, ~Long_name,\n    \"Resp1\",     \"Response 1\",\n    \"Resp2\",     \"Response 2\")\n## Convert to list of pairs\nlookup &lt;- lookup |&gt;\n    select(Long_name, Abbr_name) |&gt; \n    deframe() |&gt;\n    list()\ndat.1 |&gt;\n    rename(!!!lookup) |&gt;\n    head()\n\n  Treatment Plot Dose Time Response 1 Response 2\n1   Control   P1    H    1       8.12       3.06\n2   Control   P1    H    2      20.55      25.94\n3   Control   P1    H    3      27.49      29.85\n4   Control   P1    H    4      44.79      25.39\n5   Control   P1    M    1      20.99      20.31\n6   Control   P1    M    2      37.54      17.62",
    "crumbs": [
      "R basics",
      "Data wrangling"
    ]
  },
  {
    "objectID": "04_data_wrangling.html#select-a-single-column-pull",
    "href": "04_data_wrangling.html#select-a-single-column-pull",
    "title": "Data wrangling (tidyverse and friends)",
    "section": "5.3 Select a single column (pull)",
    "text": "5.3 Select a single column (pull)\nAs indicated earlier, single column indices on tibbles return a single column tibble. To extract a single column as a vector, there is a pull function.\nThe pull() function users tidy-select semantics.\n\ndat.1 |&gt; pull(Resp1) \n\n [1]   8.12  20.55  27.49  44.79  20.99  37.54  61.46  82.21  31.73  59.08\n[11]  94.54 121.17   8.14  13.36  33.37  39.87  19.95  42.83  62.46  81.78\n[21]  32.76  62.35  90.22 114.03  21.86  39.83  59.53  75.59  38.57  81.25\n[31] 124.08 159.69  61.16 119.84 175.87 238.76  18.82  39.82  63.30  82.29\n[41]  39.51  79.24 122.09 161.67  57.93 117.88 181.09 242.31",
    "crumbs": [
      "R basics",
      "Data wrangling"
    ]
  },
  {
    "objectID": "04_data_wrangling.html#subset-of-rows-filter",
    "href": "04_data_wrangling.html#subset-of-rows-filter",
    "title": "Data wrangling (tidyverse and friends)",
    "section": "5.4 Subset of rows (filter)",
    "text": "5.4 Subset of rows (filter)\nFiltering selects rows for which a condition is evaluated to be TRUE. Hence, any logical expression or vectorized function that returns Boolean values (TRUE or FALSE) can be used for filtering.\nThe filter() function users data-masking semantics.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirst 10 rows of the `dat.1` data.frame\n\n\nTreatment\nPlot\nDose\nTime\nResp1\nResp2\n\n\n\n\nControl\nP1\nH\n1\n8.12\n3.06\n\n\nControl\nP1\nH\n2\n20.55\n25.94\n\n\nControl\nP1\nH\n3\n27.49\n29.85\n\n\nControl\nP1\nH\n4\n44.79\n25.39\n\n\nControl\nP1\nM\n1\n20.99\n20.31\n\n\nControl\nP1\nM\n2\n37.54\n17.62\n\n\nControl\nP1\nM\n3\n61.46\n98.44\n\n\nControl\nP1\nM\n4\n82.21\n160.01\n\n\nControl\nP1\nL\n1\n31.73\n21.22\n\n\nControl\nP1\nL\n2\n59.08\n37.51\n\n\n\n\n\n\n\n\n\n\n\n\nHelper function\nCombine multiple selections …\n\n\n\n\nif_any()\nWith an an OR\n\n\nif_all()\nWith an AND)\n\n\n\nNote, by default, the above searchers are NOT case sensitive\n\n\n\nLogical/Boolean function\nDescription\n\n\n\n\n==\nis equal to\n\n\n!=\nis not equal to\n\n\n&lt;\nis less than\n\n\n&gt;\nis greater than\n\n\n&lt;=\nis less than or equal to\n\n\n&gt;=\nis greater than or equal to\n\n\nis.na()\nis equal to NA\n\n\n!is.na()\nis not equal to NA\n\n\n%in%\nis in the following vector\n\n\n!\nnot\n\n\n& &&\nlogical AND\n\n\n| ||\nlogical OR\n\n\nxor()\nelementwise OR\n\n\nisTRUE()\nis true\n\n\nisFALSE()\nis false\n\n\n\n\n\n\nFilter by category levelFilter by multiple category levelsFilter by numericFilter and betweenFilter by numeric (cumulative functions)Filter by multiple filters (AND)Filter by multiple filters (OR)Filter by multiple selection filters (AND)Filter by multiple selection filters (OR)\n\n\n\ndat.1 |&gt; filter(Dose == \"H\")\n\n   Treatment Plot Dose Time Resp1 Resp2\n1    Control   P1    H    1  8.12  3.06\n2    Control   P1    H    2 20.55 25.94\n3    Control   P1    H    3 27.49 29.85\n4    Control   P1    H    4 44.79 25.39\n5    Control   P2    H    1  8.14 23.93\n6    Control   P2    H    2 13.36 28.02\n7    Control   P2    H    3 33.37 37.17\n8    Control   P2    H    4 39.87 38.25\n9  Exclusion   P3    H    1 21.86 23.58\n10 Exclusion   P3    H    2 39.83 28.03\n11 Exclusion   P3    H    3 59.53 21.32\n12 Exclusion   P3    H    4 75.59 90.76\n13 Exclusion   P4    H    1 18.82 28.60\n14 Exclusion   P4    H    2 39.82 39.07\n15 Exclusion   P4    H    3 63.30 93.43\n16 Exclusion   P4    H    4 82.29 60.15\n\n\n\n\n\ndat.1 |&gt; filter(Dose %in% c(\"H\", \"M\"))\n\n   Treatment Plot Dose Time  Resp1  Resp2\n1    Control   P1    H    1   8.12   3.06\n2    Control   P1    H    2  20.55  25.94\n3    Control   P1    H    3  27.49  29.85\n4    Control   P1    H    4  44.79  25.39\n5    Control   P1    M    1  20.99  20.31\n6    Control   P1    M    2  37.54  17.62\n7    Control   P1    M    3  61.46  98.44\n8    Control   P1    M    4  82.21 160.01\n9    Control   P2    H    1   8.14  23.93\n10   Control   P2    H    2  13.36  28.02\n11   Control   P2    H    3  33.37  37.17\n12   Control   P2    H    4  39.87  38.25\n13   Control   P2    M    1  19.95  19.73\n14   Control   P2    M    2  42.83  40.52\n15   Control   P2    M    3  62.46   4.81\n16   Control   P2    M    4  81.78 136.66\n17 Exclusion   P3    H    1  21.86  23.58\n18 Exclusion   P3    H    2  39.83  28.03\n19 Exclusion   P3    H    3  59.53  21.32\n20 Exclusion   P3    H    4  75.59  90.76\n21 Exclusion   P3    M    1  38.57  30.63\n22 Exclusion   P3    M    2  81.25  83.61\n23 Exclusion   P3    M    3 124.08 124.09\n24 Exclusion   P3    M    4 159.69 112.65\n25 Exclusion   P4    H    1  18.82  28.60\n26 Exclusion   P4    H    2  39.82  39.07\n27 Exclusion   P4    H    3  63.30  93.43\n28 Exclusion   P4    H    4  82.29  60.15\n29 Exclusion   P4    M    1  39.51  45.90\n30 Exclusion   P4    M    2  79.24  88.04\n31 Exclusion   P4    M    3 122.09  84.19\n32 Exclusion   P4    M    4 161.67 256.34\n\n\n\n\n\ndat.1 |&gt; filter(Resp1 &lt; 25)\n\n  Treatment Plot Dose Time Resp1 Resp2\n1   Control   P1    H    1  8.12  3.06\n2   Control   P1    H    2 20.55 25.94\n3   Control   P1    M    1 20.99 20.31\n4   Control   P2    H    1  8.14 23.93\n5   Control   P2    H    2 13.36 28.02\n6   Control   P2    M    1 19.95 19.73\n7 Exclusion   P3    H    1 21.86 23.58\n8 Exclusion   P4    H    1 18.82 28.60\n\n\n\n\n\ndat.1 |&gt; filter(between(Resp1, 15, 25))\n\n  Treatment Plot Dose Time Resp1 Resp2\n1   Control   P1    H    2 20.55 25.94\n2   Control   P1    M    1 20.99 20.31\n3   Control   P2    M    1 19.95 19.73\n4 Exclusion   P3    H    1 21.86 23.58\n5 Exclusion   P4    H    1 18.82 28.60\n\n\n\n\nKeep all cases after a value of Resp1 has exceeded 200\n\ndat.1 |&gt; filter(cumany(Resp1 &gt; 200))\n\n   Treatment Plot Dose Time  Resp1  Resp2\n1  Exclusion   P3    L    4 238.76  54.23\n2  Exclusion   P4    H    1  18.82  28.60\n3  Exclusion   P4    H    2  39.82  39.07\n4  Exclusion   P4    H    3  63.30  93.43\n5  Exclusion   P4    H    4  82.29  60.15\n6  Exclusion   P4    M    1  39.51  45.90\n7  Exclusion   P4    M    2  79.24  88.04\n8  Exclusion   P4    M    3 122.09  84.19\n9  Exclusion   P4    M    4 161.67 256.34\n10 Exclusion   P4    L    1  57.93  85.24\n11 Exclusion   P4    L    2 117.88 167.90\n12 Exclusion   P4    L    3 181.09 314.49\n13 Exclusion   P4    L    4 242.31 304.70\n\n\nKeep all cases until the first case of Resp1 &lt; 25\n\ndat.1 |&gt; filter(cumall(Resp1 &lt; 25))\n\n  Treatment Plot Dose Time Resp1 Resp2\n1   Control   P1    H    1  8.12  3.06\n2   Control   P1    H    2 20.55 25.94\n\n\n\n\n\ndat.1 |&gt; filter(Dose == \"H\", Resp1 &lt; 25)\n\n  Treatment Plot Dose Time Resp1 Resp2\n1   Control   P1    H    1  8.12  3.06\n2   Control   P1    H    2 20.55 25.94\n3   Control   P2    H    1  8.14 23.93\n4   Control   P2    H    2 13.36 28.02\n5 Exclusion   P3    H    1 21.86 23.58\n6 Exclusion   P4    H    1 18.82 28.60\n\n##OR\ndat.1 |&gt; filter(Dose == \"H\" & Resp1 &lt; 25)\n\n  Treatment Plot Dose Time Resp1 Resp2\n1   Control   P1    H    1  8.12  3.06\n2   Control   P1    H    2 20.55 25.94\n3   Control   P2    H    1  8.14 23.93\n4   Control   P2    H    2 13.36 28.02\n5 Exclusion   P3    H    1 21.86 23.58\n6 Exclusion   P4    H    1 18.82 28.60\n\n\n\n\n\ndat.1 |&gt; filter(Dose == \"H\" | Resp1 &lt; 25)\n\n   Treatment Plot Dose Time Resp1 Resp2\n1    Control   P1    H    1  8.12  3.06\n2    Control   P1    H    2 20.55 25.94\n3    Control   P1    H    3 27.49 29.85\n4    Control   P1    H    4 44.79 25.39\n5    Control   P1    M    1 20.99 20.31\n6    Control   P2    H    1  8.14 23.93\n7    Control   P2    H    2 13.36 28.02\n8    Control   P2    H    3 33.37 37.17\n9    Control   P2    H    4 39.87 38.25\n10   Control   P2    M    1 19.95 19.73\n11 Exclusion   P3    H    1 21.86 23.58\n12 Exclusion   P3    H    2 39.83 28.03\n13 Exclusion   P3    H    3 59.53 21.32\n14 Exclusion   P3    H    4 75.59 90.76\n15 Exclusion   P4    H    1 18.82 28.60\n16 Exclusion   P4    H    2 39.82 39.07\n17 Exclusion   P4    H    3 63.30 93.43\n18 Exclusion   P4    H    4 82.29 60.15\n\n\n\n\n\ndat.1 |&gt; filter(if_all(starts_with(\"Resp\"), ~ . &lt; 25))\n\n  Treatment Plot Dose Time Resp1 Resp2\n1   Control   P1    H    1  8.12  3.06\n2   Control   P1    M    1 20.99 20.31\n3   Control   P2    H    1  8.14 23.93\n4   Control   P2    M    1 19.95 19.73\n5 Exclusion   P3    H    1 21.86 23.58\n\n\n\n\n\ndat.1 |&gt; filter(if_any(starts_with(\"Resp\"), ~ . &lt; 25))\n\n   Treatment Plot Dose Time Resp1 Resp2\n1    Control   P1    H    1  8.12  3.06\n2    Control   P1    H    2 20.55 25.94\n3    Control   P1    M    1 20.99 20.31\n4    Control   P1    M    2 37.54 17.62\n5    Control   P1    L    1 31.73 21.22\n6    Control   P2    H    1  8.14 23.93\n7    Control   P2    H    2 13.36 28.02\n8    Control   P2    M    1 19.95 19.73\n9    Control   P2    M    3 62.46  4.81\n10 Exclusion   P3    H    1 21.86 23.58\n11 Exclusion   P3    H    3 59.53 21.32\n12 Exclusion   P4    H    1 18.82 28.60",
    "crumbs": [
      "R basics",
      "Data wrangling"
    ]
  },
  {
    "objectID": "04_data_wrangling.html#subset-of-rows-by-position-slice",
    "href": "04_data_wrangling.html#subset-of-rows-by-position-slice",
    "title": "Data wrangling (tidyverse and friends)",
    "section": "5.5 Subset of rows by position (slice)",
    "text": "5.5 Subset of rows by position (slice)\n\nKeep rows 1 through to 4Keep rows 1 through to 4 and 7\n\n\n\ndat.1 |&gt; slice(1:4)\n\n  Treatment Plot Dose Time Resp1 Resp2\n1   Control   P1    H    1  8.12  3.06\n2   Control   P1    H    2 20.55 25.94\n3   Control   P1    H    3 27.49 29.85\n4   Control   P1    H    4 44.79 25.39\n\n\n\n\n\ndat.1 |&gt; slice(c(1:4, 7))\n\n  Treatment Plot Dose Time Resp1 Resp2\n1   Control   P1    H    1  8.12  3.06\n2   Control   P1    H    2 20.55 25.94\n3   Control   P1    H    3 27.49 29.85\n4   Control   P1    H    4 44.79 25.39\n5   Control   P1    M    3 61.46 98.44",
    "crumbs": [
      "R basics",
      "Data wrangling"
    ]
  },
  {
    "objectID": "04_data_wrangling.html#random-selection-of-rows-sample_n",
    "href": "04_data_wrangling.html#random-selection-of-rows-sample_n",
    "title": "Data wrangling (tidyverse and friends)",
    "section": "5.6 Random selection of rows (sample_n)",
    "text": "5.6 Random selection of rows (sample_n)\nIn each of the examples, I will set the random seed to ensure we can all repeat the example exactly.\n\nRandom sample of rowsRandom fractional sample of rows\n\n\nRandom sample of 10 rows (without replacement)\n\nset.seed(123)\ndat.1 |&gt; sample_n(10, replace = FALSE)\n\n   Treatment Plot Dose Time  Resp1  Resp2\n1  Exclusion   P3    M    3 124.08 124.09\n2    Control   P2    H    3  33.37  37.17\n3    Control   P2    H    2  13.36  28.02\n4    Control   P1    H    3  27.49  29.85\n5  Exclusion   P4    M    2  79.24  88.04\n6  Exclusion   P4    M    3 122.09  84.19\n7  Exclusion   P4    H    1  18.82  28.60\n8  Exclusion   P4    L    2 117.88 167.90\n9  Exclusion   P3    H    1  21.86  23.58\n10 Exclusion   P3    H    2  39.83  28.03\n\n\n\n\nRandom sample of 25% of the rows (without replacement)\n\nset.seed(123)\ndat.1 |&gt; sample_frac(0.25, replace = FALSE)\n\n   Treatment Plot Dose Time  Resp1  Resp2\n1  Exclusion   P3    M    3 124.08 124.09\n2    Control   P2    H    3  33.37  37.17\n3    Control   P2    H    2  13.36  28.02\n4    Control   P1    H    3  27.49  29.85\n5  Exclusion   P4    M    2  79.24  88.04\n6  Exclusion   P4    M    3 122.09  84.19\n7  Exclusion   P4    H    1  18.82  28.60\n8  Exclusion   P4    L    2 117.88 167.90\n9  Exclusion   P3    H    1  21.86  23.58\n10 Exclusion   P3    H    2  39.83  28.03\n11 Exclusion   P3    H    3  59.53  21.32\n12   Control   P1    M    1  20.99  20.31",
    "crumbs": [
      "R basics",
      "Data wrangling"
    ]
  },
  {
    "objectID": "04_data_wrangling.html#effects-of-filtering-on-factor-levels",
    "href": "04_data_wrangling.html#effects-of-filtering-on-factor-levels",
    "title": "Data wrangling (tidyverse and friends)",
    "section": "5.7 Effects of filtering on factor levels",
    "text": "5.7 Effects of filtering on factor levels\nIn R, categorical variables (factors) are actually stored as vectors of integers (1, 2, 3, …) along with an attribute that registers the names (and order) of the levels. We can see this, if we ask to see the structure of any categorical variable:\n\nstr(dat.1$Dose)\n\n Factor w/ 3 levels \"H\",\"L\",\"M\": 1 1 1 1 3 3 3 3 2 2 ...\n\n\nIn the above, we see that the levels of the Dose variable are “H”, “L”, “M” (by default, levels of an unordered factor are alphabetical). The first four cases are all 1, which is mapped to “H”. The next four are 3 which maps to “M” and so on.\nAlternatively, we can explore the levels attribute of a factor via the levels() function:\n\nlevels(dat.1$Dose)\n\n[1] \"H\" \"L\" \"M\"\n\nlevels(dat.1$Plot)\n\n[1] \"P1\" \"P2\" \"P3\" \"P4\"\n\nlevels(dat.1$Treatment)\n\n[1] \"Control\"   \"Exclusion\"\n\n\nAlthough subsets of rows (filter, sample_n etc) may appear to completely remove particular levels of a categorical variable, these actions do not update factor attributes. Consequently, many routines that operate on such factors (such as plots and statistical models) will proceed as if all factor levels are present (e.g. plots will contain gaps where space has been provisioned for all levels even though there may be no data associated with all levels).\nTo illustrate this, we will filter the dat.1 data such that it only includes cases for which Plot equals “P1” and then explore the levels attribute of some of the categorical variables.\n\ndat.2 &lt;- dat.1 |&gt;\n    filter(Plot == \"P1\")\ndat.2\n\n   Treatment Plot Dose Time  Resp1  Resp2\n1    Control   P1    H    1   8.12   3.06\n2    Control   P1    H    2  20.55  25.94\n3    Control   P1    H    3  27.49  29.85\n4    Control   P1    H    4  44.79  25.39\n5    Control   P1    M    1  20.99  20.31\n6    Control   P1    M    2  37.54  17.62\n7    Control   P1    M    3  61.46  98.44\n8    Control   P1    M    4  82.21 160.01\n9    Control   P1    L    1  31.73  21.22\n10   Control   P1    L    2  59.08  37.51\n11   Control   P1    L    3  94.54 119.22\n12   Control   P1    L    4 121.17 116.45\n\nlevels(dat.2$Dose)\n\n[1] \"H\" \"L\" \"M\"\n\nlevels(dat.2$Plot)\n\n[1] \"P1\" \"P2\" \"P3\" \"P4\"\n\nlevels(dat.2$Treatment)\n\n[1] \"Control\"   \"Exclusion\"\n\n\nSo although the data only contains Plot values of “P1” (and Treatment values of “Control”), the levels are still listed as “P1”, “P2”, “P3”, and “P4”.\nTo ensure that the attributes reflect the subset data, it is necessary to use the droplevels() function:\n\ndat.2 &lt;- dat.1 |&gt;\n    filter(Plot == \"P1\") |&gt;\n    droplevels()\nlevels(dat.2$Dose)\n\n[1] \"H\" \"L\" \"M\"\n\nlevels(dat.2$Plot)\n\n[1] \"P1\"\n\nlevels(dat.2$Treatment)\n\n[1] \"Control\"",
    "crumbs": [
      "R basics",
      "Data wrangling"
    ]
  },
  {
    "objectID": "04_data_wrangling.html#mathematical-functions",
    "href": "04_data_wrangling.html#mathematical-functions",
    "title": "Data wrangling (tidyverse and friends)",
    "section": "6.1 Mathematical functions",
    "text": "6.1 Mathematical functions\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\n+,-,*,/,^,%/%, %%\ntypical arithmetic operators\n\n\nlog(),log2(),log10(), exp()\nlogarithms/exponentials\n\n\n&lt;,&lt;=,&gt;,&gt;=,!=,==\nlogical operators\n\n\nbetween()\nwhether a case is between to numbers\n\n\nnear()\na safe way of assessing equality in floating points\n\n\n\n\narithmetic operatorslogarithmic operatorslogical operatorsnearbetween\n\n\n\ndat.1 |&gt; mutate(Sum = Resp1 + Resp2,\n                 Div = Resp1 / Resp2) |&gt;\n    head()\n\n  Treatment Plot Dose Time Resp1 Resp2   Sum       Div\n1   Control   P1    H    1  8.12  3.06 11.18 2.6535948\n2   Control   P1    H    2 20.55 25.94 46.49 0.7922128\n3   Control   P1    H    3 27.49 29.85 57.34 0.9209380\n4   Control   P1    H    4 44.79 25.39 70.18 1.7640803\n5   Control   P1    M    1 20.99 20.31 41.30 1.0334810\n6   Control   P1    M    2 37.54 17.62 55.16 2.1305335\n\n\n\n\n\ndat.1 |&gt; mutate(logResp1 = log(Resp1),\n                 expResp2 = exp(Resp2)) |&gt;\n    head()\n\n  Treatment Plot Dose Time Resp1 Resp2 logResp1     expResp2\n1   Control   P1    H    1  8.12  3.06 2.094330 2.132756e+01\n2   Control   P1    H    2 20.55 25.94 3.022861 1.843312e+11\n3   Control   P1    H    3 27.49 29.85 3.313822 9.197934e+12\n4   Control   P1    H    4 44.79 25.39 3.801985 1.063499e+11\n5   Control   P1    M    1 20.99 20.31 3.044046 6.614864e+08\n6   Control   P1    M    2 37.54 17.62 3.625407 4.490232e+07\n\n\n\n\n\ndat.1 |&gt; mutate(largeResp1 = Resp1 &gt; 25) |&gt;\n    head()\n\n  Treatment Plot Dose Time Resp1 Resp2 largeResp1\n1   Control   P1    H    1  8.12  3.06      FALSE\n2   Control   P1    H    2 20.55 25.94      FALSE\n3   Control   P1    H    3 27.49 29.85       TRUE\n4   Control   P1    H    4 44.79 25.39       TRUE\n5   Control   P1    M    1 20.99 20.31      FALSE\n6   Control   P1    M    2 37.54 17.62       TRUE\n\n\n\n\n\ndat.1 |&gt; mutate(\n              A = Resp1 == 8.1,\n              B = near(Resp1, 8.1, tol = 0.1)) |&gt;\n    head()\n\n  Treatment Plot Dose Time Resp1 Resp2     A     B\n1   Control   P1    H    1  8.12  3.06 FALSE  TRUE\n2   Control   P1    H    2 20.55 25.94 FALSE FALSE\n3   Control   P1    H    3 27.49 29.85 FALSE FALSE\n4   Control   P1    H    4 44.79 25.39 FALSE FALSE\n5   Control   P1    M    1 20.99 20.31 FALSE FALSE\n6   Control   P1    M    2 37.54 17.62 FALSE FALSE\n\n\n\n\n\ndat.1 |&gt; mutate(mediumResp1 = between(Resp1, 15, 25)) |&gt;\n    head()\n\n  Treatment Plot Dose Time Resp1 Resp2 mediumResp1\n1   Control   P1    H    1  8.12  3.06       FALSE\n2   Control   P1    H    2 20.55 25.94        TRUE\n3   Control   P1    H    3 27.49 29.85       FALSE\n4   Control   P1    H    4 44.79 25.39       FALSE\n5   Control   P1    M    1 20.99 20.31        TRUE\n6   Control   P1    M    2 37.54 17.62       FALSE",
    "crumbs": [
      "R basics",
      "Data wrangling"
    ]
  },
  {
    "objectID": "04_data_wrangling.html#offset-functions",
    "href": "04_data_wrangling.html#offset-functions",
    "title": "Data wrangling (tidyverse and friends)",
    "section": "6.2 Offset functions",
    "text": "6.2 Offset functions\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\nlag()\nshift cases down one\n\n\nlead()\nshift cases up one\n\n\n\n\nlagsleads\n\n\n\ndat.1 |&gt; mutate(lagResp1 = lag(Resp1)) |&gt;\n    head()\n\n  Treatment Plot Dose Time Resp1 Resp2 lagResp1\n1   Control   P1    H    1  8.12  3.06       NA\n2   Control   P1    H    2 20.55 25.94     8.12\n3   Control   P1    H    3 27.49 29.85    20.55\n4   Control   P1    H    4 44.79 25.39    27.49\n5   Control   P1    M    1 20.99 20.31    44.79\n6   Control   P1    M    2 37.54 17.62    20.99\n\n\n\n\n\ndat.1 |&gt; mutate(leadResp1 = lead(Resp1)) |&gt;\n    tail()\n\n   Treatment Plot Dose Time  Resp1  Resp2 leadResp1\n43 Exclusion   P4    M    3 122.09  84.19    161.67\n44 Exclusion   P4    M    4 161.67 256.34     57.93\n45 Exclusion   P4    L    1  57.93  85.24    117.88\n46 Exclusion   P4    L    2 117.88 167.90    181.09\n47 Exclusion   P4    L    3 181.09 314.49    242.31\n48 Exclusion   P4    L    4 242.31 304.70        NA",
    "crumbs": [
      "R basics",
      "Data wrangling"
    ]
  },
  {
    "objectID": "04_data_wrangling.html#cumulative-aggregate-functions",
    "href": "04_data_wrangling.html#cumulative-aggregate-functions",
    "title": "Data wrangling (tidyverse and friends)",
    "section": "6.3 Cumulative aggregate functions",
    "text": "6.3 Cumulative aggregate functions\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\ncummax()\ncumulative maximum\n\n\ncummin()\ncumulative minimum\n\n\ncummean()\ncumulative mean\n\n\ncumprod()\ncumulative product\n\n\ncumsum()\ncumulative sum\n\n\nrank()\nrank of current case (ties averaged)\n\n\nmin_rank()\nrank of current case (minimum rank for ties)\n\n\ndense_rank()\nrank of current case (minimum rank for ties, no gaps)\n\n\npercent_rank()\nmin_rank of current case (scaled to [0,1])\n\n\ncume_dist()\ncumulative empirical distribution (prop. less than current rank)\n\n\nrow_number()\nrank of current case (first row for ties)\n\n\nntile()\nbin into (n) buckets\n\n\n\n\ncummmax/ cummin/ cummeancumsum/ cumprodrank/ min_rank/ dense_rank/ percent_rankcumulative empirical distributionrank row numberbin into buckets\n\n\n\ndat.1 |&gt; mutate(Cummin = cummin(Resp1),\n                 Cummax = cummax(Resp1),\n                 Cummean = cummean(Resp1)) |&gt;\n    head()\n\n  Treatment Plot Dose Time Resp1 Resp2 Cummin Cummax Cummean\n1   Control   P1    H    1  8.12  3.06   8.12   8.12  8.1200\n2   Control   P1    H    2 20.55 25.94   8.12  20.55 14.3350\n3   Control   P1    H    3 27.49 29.85   8.12  27.49 18.7200\n4   Control   P1    H    4 44.79 25.39   8.12  44.79 25.2375\n5   Control   P1    M    1 20.99 20.31   8.12  44.79 24.3880\n6   Control   P1    M    2 37.54 17.62   8.12  44.79 26.5800\n\n\n\n\n\ndat.1 |&gt; mutate(Cumsum = cumsum(Resp1),\n                 Cumprod = cumprod(Resp1)) |&gt;\n    head()\n\n  Treatment Plot Dose Time Resp1 Resp2 Cumsum      Cumprod\n1   Control   P1    H    1  8.12  3.06   8.12 8.120000e+00\n2   Control   P1    H    2 20.55 25.94  28.67 1.668660e+02\n3   Control   P1    H    3 27.49 29.85  56.16 4.587146e+03\n4   Control   P1    H    4 44.79 25.39 100.95 2.054583e+05\n5   Control   P1    M    1 20.99 20.31 121.94 4.312569e+06\n6   Control   P1    M    2 37.54 17.62 159.48 1.618939e+08\n\n\n\n\n\ndat.1 |&gt; mutate(Rank = rank(Resp1),\n                 minRank = min_rank(Resp1),\n                 denseRank = dense_rank(Resp1),\n                 percentRank = percent_rank(Resp1)) |&gt;\n    head()\n\n  Treatment Plot Dose Time Resp1 Resp2 Rank minRank denseRank percentRank\n1   Control   P1    H    1  8.12  3.06    1       1         1   0.0000000\n2   Control   P1    H    2 20.55 25.94    6       6         6   0.1063830\n3   Control   P1    H    3 27.49 29.85    9       9         9   0.1702128\n4   Control   P1    H    4 44.79 25.39   20      20        20   0.4042553\n5   Control   P1    M    1 20.99 20.31    7       7         7   0.1276596\n6   Control   P1    M    2 37.54 17.62   13      13        13   0.2553191\n\n\n\n\n\ndat.1 |&gt; mutate(cume_dist(Resp1)) |&gt;\n    head()\n\n  Treatment Plot Dose Time Resp1 Resp2 cume_dist(Resp1)\n1   Control   P1    H    1  8.12  3.06       0.02083333\n2   Control   P1    H    2 20.55 25.94       0.12500000\n3   Control   P1    H    3 27.49 29.85       0.18750000\n4   Control   P1    H    4 44.79 25.39       0.41666667\n5   Control   P1    M    1 20.99 20.31       0.14583333\n6   Control   P1    M    2 37.54 17.62       0.27083333\n\n\n\n\n\ndat.1 |&gt; mutate(row_number(Resp1)) |&gt;\n    head()\n\n  Treatment Plot Dose Time Resp1 Resp2 row_number(Resp1)\n1   Control   P1    H    1  8.12  3.06                 1\n2   Control   P1    H    2 20.55 25.94                 6\n3   Control   P1    H    3 27.49 29.85                 9\n4   Control   P1    H    4 44.79 25.39                20\n5   Control   P1    M    1 20.99 20.31                 7\n6   Control   P1    M    2 37.54 17.62                13\n\n\n\n\n\ndat.1 |&gt; mutate(ntile(Resp1, 5)) |&gt;\n    head()\n\n  Treatment Plot Dose Time Resp1 Resp2 ntile(Resp1, 5)\n1   Control   P1    H    1  8.12  3.06               1\n2   Control   P1    H    2 20.55 25.94               1\n3   Control   P1    H    3 27.49 29.85               1\n4   Control   P1    H    4 44.79 25.39               2\n5   Control   P1    M    1 20.99 20.31               1\n6   Control   P1    M    2 37.54 17.62               2",
    "crumbs": [
      "R basics",
      "Data wrangling"
    ]
  },
  {
    "objectID": "04_data_wrangling.html#miscellaneous",
    "href": "04_data_wrangling.html#miscellaneous",
    "title": "Data wrangling (tidyverse and friends)",
    "section": "6.4 Miscellaneous",
    "text": "6.4 Miscellaneous\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\nif_else()\nelementwise (case by case) if and else\n\n\ncase_when()\nelementwise multiple if_else\n\n\nna_if()\nelementwise replace nominated value with NA\n\n\npmax()\nelementwise maximum across multiple columns\n\n\npmin()\nelementwise minimum across multiple columns\n\n\n\n\nif_elsecase_when\n\n\n\ndat.1 |&gt; mutate(Size = if_else(Resp1 &gt; 25, \"Big\", \"Small\")) |&gt;\n    head()\n\n  Treatment Plot Dose Time Resp1 Resp2  Size\n1   Control   P1    H    1  8.12  3.06 Small\n2   Control   P1    H    2 20.55 25.94 Small\n3   Control   P1    H    3 27.49 29.85   Big\n4   Control   P1    H    4 44.79 25.39   Big\n5   Control   P1    M    1 20.99 20.31 Small\n6   Control   P1    M    2 37.54 17.62   Big\n\n\n\n\n\ndat.1 |&gt; mutate(Size = case_when(Resp1 &lt; 15 ~ \"Small\",\n                               Resp1 &lt; 25 ~ \"Medium\",\n                               Resp1 &gt;= 25 ~ \"Big\")) |&gt;\n    head()\n\n  Treatment Plot Dose Time Resp1 Resp2   Size\n1   Control   P1    H    1  8.12  3.06  Small\n2   Control   P1    H    2 20.55 25.94 Medium\n3   Control   P1    H    3 27.49 29.85    Big\n4   Control   P1    H    4 44.79 25.39    Big\n5   Control   P1    M    1 20.99 20.31 Medium\n6   Control   P1    M    2 37.54 17.62    Big",
    "crumbs": [
      "R basics",
      "Data wrangling"
    ]
  },
  {
    "objectID": "04_data_wrangling.html#summary-functions-1",
    "href": "04_data_wrangling.html#summary-functions-1",
    "title": "Data wrangling (tidyverse and friends)",
    "section": "6.5 Summary functions",
    "text": "6.5 Summary functions\nSummary functions (those that return a single value) are also permissible - the value will be recycled for the total length of the input vector. A table of useful summary functions can be found in the Summarise section.\n\ndat.1 |&gt; mutate(meanResp1 = mean(Resp1)) |&gt;\n    head()\n\n  Treatment Plot Dose Time Resp1 Resp2 meanResp1\n1   Control   P1    H    1  8.12  3.06  75.26604\n2   Control   P1    H    2 20.55 25.94  75.26604\n3   Control   P1    H    3 27.49 29.85  75.26604\n4   Control   P1    H    4 44.79 25.39  75.26604\n5   Control   P1    M    1 20.99 20.31  75.26604\n6   Control   P1    M    2 37.54 17.62  75.26604\n\n\nAnother important summary function is the n() function. This function returns the total number of rows. In the following example, we will use it to create a column that just provides a running row counter (e.g. a row index variable).\n\ndat.1 |&gt; mutate(N = 1:n()) |&gt;\n    head()\n\n  Treatment Plot Dose Time Resp1 Resp2 N\n1   Control   P1    H    1  8.12  3.06 1\n2   Control   P1    H    2 20.55 25.94 2\n3   Control   P1    H    3 27.49 29.85 3\n4   Control   P1    H    4 44.79 25.39 4\n5   Control   P1    M    1 20.99 20.31 5\n6   Control   P1    M    2 37.54 17.62 6",
    "crumbs": [
      "R basics",
      "Data wrangling"
    ]
  },
  {
    "objectID": "04_data_wrangling.html#multiple-mutations-across",
    "href": "04_data_wrangling.html#multiple-mutations-across",
    "title": "Data wrangling (tidyverse and friends)",
    "section": "6.6 Multiple mutations (across)",
    "text": "6.6 Multiple mutations (across)\nIn the section on select, a set of select helper functions were described to facilitate convenient ways to select columns based on properties of the column names etc. The across() function allows us to bring those same selection helper functions to mutate.\nThe across() function has the following form:\n\nacross(.cols, .fns, .names)\n\nwhere:\n\n.cols - a tidy selection (e.g. selection helper function)\n.fns - a function (or list of functions) to apply to each selected column\n.names - a glue specification determining the format of the new variable names. By default the glue will be either {.col} (when there is only a single function) or {.col}.{fn} (when a list of functions)\n\n\nSimple selectionsSimple selections with namingwhere selections with namingnum_range selections with namingMultiple selections and functionsAdditional arguments to the function(s)\n\n\n\ndat.1 |&gt; mutate(across(c(Resp1, Resp2), log)) |&gt;\n    head()\n\n  Treatment Plot Dose Time    Resp1    Resp2\n1   Control   P1    H    1 2.094330 1.118415\n2   Control   P1    H    2 3.022861 3.255786\n3   Control   P1    H    3 3.313822 3.396185\n4   Control   P1    H    4 3.801985 3.234355\n5   Control   P1    M    1 3.044046 3.011113\n6   Control   P1    M    2 3.625407 2.869035\n\n\n\n\n\ndat.1 |&gt; mutate(across(c(Resp1, Resp2),\n                        .fns = log,\n                        .names = \"l{.col}\")) |&gt;\n    head()\n\n  Treatment Plot Dose Time Resp1 Resp2   lResp1   lResp2\n1   Control   P1    H    1  8.12  3.06 2.094330 1.118415\n2   Control   P1    H    2 20.55 25.94 3.022861 3.255786\n3   Control   P1    H    3 27.49 29.85 3.313822 3.396185\n4   Control   P1    H    4 44.79 25.39 3.801985 3.234355\n5   Control   P1    M    1 20.99 20.31 3.044046 3.011113\n6   Control   P1    M    2 37.54 17.62 3.625407 2.869035\n\n\n\n\n\ndat.1 |&gt; mutate(across(where(is.numeric),\n                        .fns = log,\n                        .names = \"l{.col}\")) |&gt;\n    head()\n\n  Treatment Plot Dose Time Resp1 Resp2     lTime   lResp1   lResp2\n1   Control   P1    H    1  8.12  3.06 0.0000000 2.094330 1.118415\n2   Control   P1    H    2 20.55 25.94 0.6931472 3.022861 3.255786\n3   Control   P1    H    3 27.49 29.85 1.0986123 3.313822 3.396185\n4   Control   P1    H    4 44.79 25.39 1.3862944 3.801985 3.234355\n5   Control   P1    M    1 20.99 20.31 0.0000000 3.044046 3.011113\n6   Control   P1    M    2 37.54 17.62 0.6931472 3.625407 2.869035\n\n\n\n\n\ndat.1 |&gt; mutate(across(num_range(\"Resp\", 1:2),\n                        .fns = log,\n                        .names = \"l{.col}\")) |&gt;\n    head()\n\n  Treatment Plot Dose Time Resp1 Resp2   lResp1   lResp2\n1   Control   P1    H    1  8.12  3.06 2.094330 1.118415\n2   Control   P1    H    2 20.55 25.94 3.022861 3.255786\n3   Control   P1    H    3 27.49 29.85 3.313822 3.396185\n4   Control   P1    H    4 44.79 25.39 3.801985 3.234355\n5   Control   P1    M    1 20.99 20.31 3.044046 3.011113\n6   Control   P1    M    2 37.54 17.62 3.625407 2.869035\n\n\n\n\n\ndat.1 |&gt; mutate(across(c(Resp1, Resp2),\n                        .fns = list(l = log, s = sqrt),\n                        .names = \"{.fn}.{.col}\")) |&gt;\n    head()\n\n  Treatment Plot Dose Time Resp1 Resp2  l.Resp1  s.Resp1  l.Resp2  s.Resp2\n1   Control   P1    H    1  8.12  3.06 2.094330 2.849561 1.118415 1.749286\n2   Control   P1    H    2 20.55 25.94 3.022861 4.533211 3.255786 5.093133\n3   Control   P1    H    3 27.49 29.85 3.313822 5.243091 3.396185 5.463515\n4   Control   P1    H    4 44.79 25.39 3.801985 6.692533 3.234355 5.038849\n5   Control   P1    M    1 20.99 20.31 3.044046 4.581484 3.011113 4.506662\n6   Control   P1    M    2 37.54 17.62 3.625407 6.126989 2.869035 4.197618\n\n\n\n\nCentring all numeric variables (note the use of the purrr style lambda formula for functions that require additional arguments. When applying the function, the focal variable is assigned the name of .x.\n\ndat.1 |&gt; mutate(across(c(Resp1, Resp2),\n                        .fns = list(c =  ~ scale(.x, scale = FALSE)),\n                        .names = \"{.fn}{.col}\")) |&gt;\n    head()\n\n  Treatment Plot Dose Time Resp1 Resp2    cResp1    cResp2\n1   Control   P1    H    1  8.12  3.06 -67.14604 -78.64958\n2   Control   P1    H    2 20.55 25.94 -54.71604 -55.76958\n3   Control   P1    H    3 27.49 29.85 -47.77604 -51.85958\n4   Control   P1    H    4 44.79 25.39 -30.47604 -56.31958\n5   Control   P1    M    1 20.99 20.31 -54.27604 -61.39958\n6   Control   P1    M    2 37.54 17.62 -37.72604 -64.08958",
    "crumbs": [
      "R basics",
      "Data wrangling"
    ]
  },
  {
    "objectID": "04_data_wrangling.html#changing-vectors",
    "href": "04_data_wrangling.html#changing-vectors",
    "title": "Data wrangling (tidyverse and friends)",
    "section": "6.7 Changing vectors",
    "text": "6.7 Changing vectors\n\nCast to different classChange factor labelsChange factor levelsChange factor levels and labelsChange factor levels to reflect data orderChange factor levels according to a numeric variable\n\n\nConvert Time (a numeric) into a factor\n\ndat.1 |&gt; mutate(Time = factor(Time)) |&gt;\n    tibble()\n\n# A tibble: 48 × 6\n   Treatment Plot  Dose  Time  Resp1  Resp2\n   &lt;fct&gt;     &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 Control   P1    H     1      8.12   3.06\n 2 Control   P1    H     2     20.6   25.9 \n 3 Control   P1    H     3     27.5   29.8 \n 4 Control   P1    H     4     44.8   25.4 \n 5 Control   P1    M     1     21.0   20.3 \n 6 Control   P1    M     2     37.5   17.6 \n 7 Control   P1    M     3     61.5   98.4 \n 8 Control   P1    M     4     82.2  160.  \n 9 Control   P1    L     1     31.7   21.2 \n10 Control   P1    L     2     59.1   37.5 \n# ℹ 38 more rows\n\n\n\n\nChange the labels of the “H” and “M” levels of Dose\n\ndat.1 |&gt; mutate(Dose = fct_recode(Dose, High = 'H',  Medium = 'M')) |&gt;\n    tibble()\n\n# A tibble: 48 × 6\n   Treatment Plot  Dose    Time Resp1  Resp2\n   &lt;fct&gt;     &lt;fct&gt; &lt;fct&gt;  &lt;int&gt; &lt;dbl&gt;  &lt;dbl&gt;\n 1 Control   P1    High       1  8.12   3.06\n 2 Control   P1    High       2 20.6   25.9 \n 3 Control   P1    High       3 27.5   29.8 \n 4 Control   P1    High       4 44.8   25.4 \n 5 Control   P1    Medium     1 21.0   20.3 \n 6 Control   P1    Medium     2 37.5   17.6 \n 7 Control   P1    Medium     3 61.5   98.4 \n 8 Control   P1    Medium     4 82.2  160.  \n 9 Control   P1    L          1 31.7   21.2 \n10 Control   P1    L          2 59.1   37.5 \n# ℹ 38 more rows\n\ndat.1 |&gt; mutate(Dose = fct_recode(Dose, High = 'H',  Medium = 'M')) |&gt;\n    str()\n\n'data.frame':   48 obs. of  6 variables:\n $ Treatment: Factor w/ 2 levels \"Control\",\"Exclusion\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Plot     : Factor w/ 4 levels \"P1\",\"P2\",\"P3\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ Dose     : Factor w/ 3 levels \"High\",\"L\",\"Medium\": 1 1 1 1 3 3 3 3 2 2 ...\n $ Time     : int  1 2 3 4 1 2 3 4 1 2 ...\n $ Resp1    : num  8.12 20.55 27.49 44.79 20.99 ...\n $ Resp2    : num  3.06 25.94 29.85 25.39 20.31 ...\n - attr(*, \"out.attrs\")=List of 2\n  ..$ dim     : Named int [1:3] 4 3 4\n  .. ..- attr(*, \"names\")= chr [1:3] \"Time\" \"Dose\" \"Plot\"\n  ..$ dimnames:List of 3\n  .. ..$ Time: chr [1:4] \"Time=1\" \"Time=2\" \"Time=3\" \"Time=4\"\n  .. ..$ Dose: chr [1:3] \"Dose=H\" \"Dose=M\" \"Dose=L\"\n  .. ..$ Plot: chr [1:4] \"Plot=P1\" \"Plot=P2\" \"Plot=P3\" \"Plot=P4\"\n\n\n\n\nChange the level order of the Dose factor to something more natural.\n\ndat.1 |&gt; pull(Dose)\n\n [1] H H H H M M M M L L L L H H H H M M M M L L L L H H H H M M M M L L L L H H\n[39] H H M M M M L L L L\nLevels: H L M\n\ndat.1 |&gt;\n    mutate(Dose = fct_relevel(Dose, c(\"L\", \"M\", \"H\"))) |&gt; \n    as_tibble() |&gt;\n    pull(Dose)\n\n [1] H H H H M M M M L L L L H H H H M M M M L L L L H H H H M M M M L L L L H H\n[39] H H M M M M L L L L\nLevels: L M H\n\n\n\n\nChange the labels and level order of the Dose factor to something more natural.\n\ndat.1 |&gt; pull(Dose)\ndat.1 |&gt; mutate(\n              Dose = fct_relevel(Dose, c(\"L\", \"M\", \"H\")),\n              Dose = fct_recode(Dose, High = 'H',  Medium = 'M')\n              ) |&gt;\n    as_tibble |&gt; \n    pull(Dose)\n\nError: The pipe operator requires a function call as RHS (&lt;text&gt;:6:5)\n\n\n\n\n\ndat.1 |&gt; pull(Dose)\ndat.1 |&gt; mutate(Dose = fct_reorder(Dose, 1:n())) |&gt;\n    as_tibble |&gt; \n    pull(Dose)\n\nError: The pipe operator requires a function call as RHS (&lt;text&gt;:3:5)\n\n\n\n\nChange the order of Dose levels according to the median Resp1 values\n\ndat.1 |&gt; pull(Dose)\ndat.1 |&gt; mutate(Dose = fct_reorder(Dose, Resp1, median)) |&gt;\n    as_tibble |&gt; \n    pull(Dose)\n\nError: The pipe operator requires a function call as RHS (&lt;text&gt;:3:5)",
    "crumbs": [
      "R basics",
      "Data wrangling"
    ]
  },
  {
    "objectID": "04_data_wrangling.html#count",
    "href": "04_data_wrangling.html#count",
    "title": "Data wrangling (tidyverse and friends)",
    "section": "7.1 Count",
    "text": "7.1 Count\nThe count() function provides a convenient way to count up the number of unique combinations of factors.\n\nCount single factorCount multiple factors\n\n\n\ndat.1 |&gt; count(Dose)\n\n  Dose  n\n1    H 16\n2    L 16\n3    M 16\n\n\n\n\n\ndat.1 |&gt; count(Dose, between(Resp1, 30, 50))\n\n  Dose between(Resp1, 30, 50)  n\n1    H                  FALSE 11\n2    H                   TRUE  5\n3    L                  FALSE 14\n4    L                   TRUE  2\n5    M                  FALSE 12\n6    M                   TRUE  4",
    "crumbs": [
      "R basics",
      "Data wrangling"
    ]
  },
  {
    "objectID": "04_data_wrangling.html#wide-to-long-pivot_longer",
    "href": "04_data_wrangling.html#wide-to-long-pivot_longer",
    "title": "Data wrangling (tidyverse and friends)",
    "section": "9.1 Wide to long (pivot_longer)",
    "text": "9.1 Wide to long (pivot_longer)\nWhilst wide data formats are often more compact and typically easier to manage for data entry (particularly in the field), the data are not in the appropriate format for most analyses (traditional repeated measures and multivariate analyses are two exceptions). Most analyses require that each replicate is in its own row and thus it is necessary to be rearrange or reshape (melt) the data from this wide format to the long (molten) format.\n\n\n\n\n\n\n\n\n\nWhilst there are numerous routines in R for reshaping data, we will only explore those that are formally part of the tidyverse ecosystem.\nThe pivot_longer() function (tidyr package) is very useful for converting wide (repeated measures-like) into long format. The important parameters to specify are:\n\npivot_longer(data, cols, names_to, values_to, values_drop_na)\n\nwhere:\n\ndata - the input dataframe or tibble\ncols - a tidy select specification of the columns to be lengthened into a single column\nnames_to - a name (string) to give a new column into which to store the names of the old wide column names\nvalues_to - a name (string) to give the new column containing the values that were previously in the old columns.\nvalues_drop_na - whether or not to drop rows that only contain NA values in the new value column.\n\nTo illustrate, we will use the dat.w dataframe.\n\n\n\n\nThe `data.w` data.frame\n\n\n\nPlot\nBetween\nTime.0\nTime.1\nTime.2\n\n\n\n\nR1\nP1\nA1\n8\n14\n14\n\n\nR2\nP2\nA1\n10\n12\n11\n\n\nR3\nP3\nA2\n7\n11\n8\n\n\nR4\nP4\nA2\n11\n9\n2\n\n\n\n\n\n\n\n\n\nPivot longerPivot longer (with starts_with)\n\n\n\ndata.w |&gt; pivot_longer(Time.0:Time.2,\n                        names_to = \"Time\",\n                        values_to = \"Count\")\n\n# A tibble: 12 × 4\n   Plot  Between Time   Count\n   &lt;fct&gt; &lt;fct&gt;   &lt;chr&gt;  &lt;int&gt;\n 1 P1    A1      Time.0     8\n 2 P1    A1      Time.1    14\n 3 P1    A1      Time.2    14\n 4 P2    A1      Time.0    10\n 5 P2    A1      Time.1    12\n 6 P2    A1      Time.2    11\n 7 P3    A2      Time.0     7\n 8 P3    A2      Time.1    11\n 9 P3    A2      Time.2     8\n10 P4    A2      Time.0    11\n11 P4    A2      Time.1     9\n12 P4    A2      Time.2     2\n\n\n\n\n\ndata.w |&gt; pivot_longer(starts_with(\"Time\"),\n                        names_to = \"Time\",\n                        names_prefix = \"Time.\",\n                        values_to = \"Count\"\n                        )\n\n# A tibble: 12 × 4\n   Plot  Between Time  Count\n   &lt;fct&gt; &lt;fct&gt;   &lt;chr&gt; &lt;int&gt;\n 1 P1    A1      0         8\n 2 P1    A1      1        14\n 3 P1    A1      2        14\n 4 P2    A1      0        10\n 5 P2    A1      1        12\n 6 P2    A1      2        11\n 7 P3    A2      0         7\n 8 P3    A2      1        11\n 9 P3    A2      2         8\n10 P4    A2      0        11\n11 P4    A2      1         9\n12 P4    A2      2         2",
    "crumbs": [
      "R basics",
      "Data wrangling"
    ]
  },
  {
    "objectID": "04_data_wrangling.html#long-to-wide-pivot_wider",
    "href": "04_data_wrangling.html#long-to-wide-pivot_wider",
    "title": "Data wrangling (tidyverse and friends)",
    "section": "9.2 Long to wide (pivot_wider)",
    "text": "9.2 Long to wide (pivot_wider)\nThe opposite of making a data set longer is to make a data set wider. Whilst analytical and graphical routines might require data to be in long format, wide tabular summaries are typically more compact and familiar.\n\n\n\n\n\n\n\n\n\nWidening is performed via the pivot_wider() function, the most important parameters of which are:\n\npivot_wider(data, id_cols, names_from, values_from\n\nwhere:\n\ndata - the input dataframe or tibble\nid_cols - a tidy select specification of the columns that uniquely identify the case - these columns will not be widened.\nnames_from - a tidy select specification of the column(s) that contain the names to be used as new column names.\nvalues_from - a tidy select specification of the column(s) that contain the values to be used as values in the new columns (e.g, the data to be widened).\n\nTo illustrate, we will use the dat.w dataframe.\n\n\n\n\nThe `data` data.frame\n\n\nResp1\nResp2\nBetween\nPlot\nSubplot\nWithin\n\n\n\n\n8\n17\nA1\nP1\nS1\nB1\n\n\n10\n18\nA1\nP1\nS1\nB2\n\n\n7\n17\nA1\nP1\nS2\nB1\n\n\n11\n21\nA1\nP1\nS2\nB2\n\n\n14\n19\nA2\nP2\nS3\nB1\n\n\n12\n13\nA2\nP2\nS3\nB2\n\n\n11\n24\nA2\nP2\nS4\nB1\n\n\n9\n18\nA2\nP2\nS4\nB2\n\n\n14\n25\nA3\nP3\nS5\nB1\n\n\n11\n18\nA3\nP3\nS5\nB2\n\n\n8\n27\nA3\nP3\nS6\nB1\n\n\n2\n22\nA3\nP3\nS6\nB2\n\n\n8\n17\nA1\nP4\nS7\nB1\n\n\n10\n22\nA1\nP4\nS7\nB2\n\n\n7\n16\nA1\nP4\nS8\nB1\n\n\n12\n13\nA1\nP4\nS8\nB2\n\n\n11\n23\nA2\nP5\nS9\nB1\n\n\n12\n19\nA2\nP5\nS9\nB2\n\n\n12\n23\nA2\nP5\nS10\nB1\n\n\n10\n21\nA2\nP5\nS10\nB2\n\n\n3\n17\nA3\nP6\nS11\nB1\n\n\n11\n16\nA3\nP6\nS11\nB2\n\n\n13\n26\nA3\nP6\nS12\nB1\n\n\n7\n28\nA3\nP6\nS12\nB2\n\n\n\n\n\n\n\n\nNote, these data are not as long as they could be. Purely “long” data should have each observation in its own row. The data dataframe has two observations (one for “Resp1” and one for “Resp2”) per row.\n\nPivot wider for Resp1 onlyPivot wider for Resp1 and Resp2\n\n\nWiden the “Resp1” variable by the levels of the Between factor.\n\ndata |&gt; select(-Resp2) |&gt;\n    pivot_wider(names_from = Within,\n                values_from = c(Resp1))\n\n# A tibble: 12 × 5\n   Between Plot  Subplot    B1    B2\n   &lt;fct&gt;   &lt;fct&gt; &lt;fct&gt;   &lt;int&gt; &lt;int&gt;\n 1 A1      P1    S1          8    10\n 2 A1      P1    S2          7    11\n 3 A2      P2    S3         14    12\n 4 A2      P2    S4         11     9\n 5 A3      P3    S5         14    11\n 6 A3      P3    S6          8     2\n 7 A1      P4    S7          8    10\n 8 A1      P4    S8          7    12\n 9 A2      P5    S9         11    12\n10 A2      P5    S10        12    10\n11 A3      P6    S11         3    11\n12 A3      P6    S12        13     7\n\n\n\n\nWiden the “Resp1” and “Resp2” variables by the levels of the Between factor.\n\ndata |&gt; pivot_wider(names_from = Within,\n                values_from = starts_with(\"Resp\"))\n\n# A tibble: 12 × 7\n   Between Plot  Subplot Resp1_B1 Resp1_B2 Resp2_B1 Resp2_B2\n   &lt;fct&gt;   &lt;fct&gt; &lt;fct&gt;      &lt;int&gt;    &lt;int&gt;    &lt;int&gt;    &lt;int&gt;\n 1 A1      P1    S1             8       10       17       18\n 2 A1      P1    S2             7       11       17       21\n 3 A2      P2    S3            14       12       19       13\n 4 A2      P2    S4            11        9       24       18\n 5 A3      P3    S5            14       11       25       18\n 6 A3      P3    S6             8        2       27       22\n 7 A1      P4    S7             8       10       17       22\n 8 A1      P4    S8             7       12       16       13\n 9 A2      P5    S9            11       12       23       19\n10 A2      P5    S10           12       10       23       21\n11 A3      P6    S11            3       11       17       16\n12 A3      P6    S12           13        7       26       28\n\n\nAlternatively we could make the data longer before widening\n\ndata |&gt;\n    pivot_longer(cols = starts_with(\"Resp\")) |&gt;\n    pivot_wider(names_from = c(name, Within),\n                values_from = value)\n\n# A tibble: 12 × 7\n   Between Plot  Subplot Resp1_B1 Resp2_B1 Resp1_B2 Resp2_B2\n   &lt;fct&gt;   &lt;fct&gt; &lt;fct&gt;      &lt;int&gt;    &lt;int&gt;    &lt;int&gt;    &lt;int&gt;\n 1 A1      P1    S1             8       17       10       18\n 2 A1      P1    S2             7       17       11       21\n 3 A2      P2    S3            14       19       12       13\n 4 A2      P2    S4            11       24        9       18\n 5 A3      P3    S5            14       25       11       18\n 6 A3      P3    S6             8       27        2       22\n 7 A1      P4    S7             8       17       10       22\n 8 A1      P4    S8             7       16       12       13\n 9 A2      P5    S9            11       23       12       19\n10 A2      P5    S10           12       23       10       21\n11 A3      P6    S11            3       17       11       16\n12 A3      P6    S12           13       26        7       28",
    "crumbs": [
      "R basics",
      "Data wrangling"
    ]
  },
  {
    "objectID": "04_data_wrangling.html#vlookup-in-r",
    "href": "04_data_wrangling.html#vlookup-in-r",
    "title": "Data wrangling (tidyverse and friends)",
    "section": "12.1 VLOOKUP in R",
    "text": "12.1 VLOOKUP in R\nLookup tables provide a way of inserting a column of data into a large data set such that the entries in the new column are determined by a relational match within another data set (the lookup table). For example, the main data set might contain data collected from a number of sites (Plots). Elsewhere we may have a data set that just contains the set of sites and their corresponding latitudes and longitudes (geographical lookup table). We could incorporate these latitudes and longitudes into the main data set by merging against the geographical lookup table. In Excel, this is referred to as vlookup, in a relational database (and in tidyverse) it is referred to as a join.\nIf we again consider our data.bio data, but this time also consider the data.geo data. This later dataset contains the latitude and longitude of each of the plots.\n\n\n\n\n\n\n\n\nResp1\nResp2\nBetween\nPlot\nSubplot\n\n\n\n\n8\n18\nA1\nP1\nS1\n\n\n10\n21\nA1\nP1\nS2\n\n\n11\n23\nA1\nP2\nS4\n\n\n14\n22\nA2\nP3\nS5\n\n\n12\n24\nA2\nP3\nS6\n\n\n11\n23\nA2\nP4\nS7\n\n\n9\n20\nA2\nP4\nS8\n\n\n14\n11\nA3\nP5\nS9\n\n\n11\n22\nA3\nP5\nS10\n\n\n8\n24\nA3\nP6\nS11\n\n\n2\n16\nA3\nP6\nS12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot\nLAT\nLONG\n\n\n\n\nP1\n17.9605\n145.4326\n\n\nP2\n17.5210\n146.1983\n\n\nP3\n17.0011\n146.3839\n\n\nP4\n18.2350\n146.7934\n\n\nP5\n18.9840\n146.0345\n\n\nP6\n20.1154\n146.4672\n\n\n\n\n\n\n\n\n\n\n\nKeep all bio information (left_join)\n\n\n\ndata.bio |&gt; left_join(data.geo)\n\n   Resp1 Resp2 Between Plot Subplot     LAT     LONG\n1      8    18      A1   P1      S1 17.9605 145.4326\n2     10    21      A1   P1      S2 17.9605 145.4326\n3     11    23      A1   P2      S4 17.5210 146.1983\n4     14    22      A2   P3      S5 17.0011 146.3839\n5     12    24      A2   P3      S6 17.0011 146.3839\n6     11    23      A2   P4      S7 18.2350 146.7934\n7      9    20      A2   P4      S8 18.2350 146.7934\n8     14    11      A3   P5      S9 18.9840 146.0345\n9     11    22      A3   P5     S10 18.9840 146.0345\n10     8    24      A3   P6     S11 20.1154 146.4672\n11     2    16      A3   P6     S12 20.1154 146.4672",
    "crumbs": [
      "R basics",
      "Data wrangling"
    ]
  },
  {
    "objectID": "05_grammar_of_graphics.html",
    "href": "05_grammar_of_graphics.html",
    "title": "The grammar of graphics (ggplot2)",
    "section": "",
    "text": "This Tutorial has been thrown together a little hastily and is therefore not very well organised - sorry! Graphical features are demonstrated either via tables of properties or as clickable graphics that reveal the required R code. Click on a graphic to reveal/toggle the source code or to navigate to an expanded section.\nThis tutorial is intended to be viewed sequentially. It begins with the basic ggplot framework and then progressively builds up more and more features as default elements are gradually replaced to yeild more customized graphics.\nHaving said that, I am going to start with a sort of showcase of graphics which should act as quick navigation to entire sections devoted to the broad series of graphs related to each of the featured graphics. I have intentionally titled each graph according to the main feature it encapsulates rather than any specific functions that are used to produce the features as often a single graphic requires a combination of features and thus functions. Furthermore, the grammar of graphics specifications are sufficiently unfamiliar to many that the relationships between the types of graphical features a researcher wishes to produce and the specific syntax required to achieve the desired result can be difficult to recognise.\nEach graphic is intended to encapsulate a broad series of related graph types.",
    "crumbs": [
      "R basics",
      "The grammar of graphics"
    ]
  },
  {
    "objectID": "05_grammar_of_graphics.html#primitive-geoms",
    "href": "05_grammar_of_graphics.html#primitive-geoms",
    "title": "The grammar of graphics (ggplot2)",
    "section": "2.1 Primitive geoms",
    "text": "2.1 Primitive geoms\nPrimitive geoms are simple plotting shapes that typically represent direct mapping of data variables to the shapes - that is, they do not require specific stat functions. Hence, all primitive geoms use stat_identity. Nevertheless, it is possible to have the shapes mapped via alternative stats functions is appropriate (see points).\n\nBlank Points Text Paths Polygons Areas Ribbons \n\n\nAlthough this might seem pointless, it can be useful for forcing axes scales to conform to a particular format - since axes scales are determined by the first layer (which can be blank) defined in the sequence of expressions.\nTo help illustrate this, I will introduce a fabricated data set comprising the length (mm) of 10 day old frog tadpoles incubated at three different temperatures (Low, Medium, High).\n\n\n\ntadpole &lt;- tribble(\n    ~length, ~temp,\n    2.1,     \"Low\",\n    2.0,     \"Low\",\n    1.8,     \"Low\",\n    2.2,     \"Medium\",\n    2.3,     \"Medium\",\n    2.3,     \"Medium\",\n    2.5,     \"High\",\n    2.7,     \"High\",\n    2.8,     \"High\",\n    ) %&gt;%\n    mutate(temp = factor(temp))\n\n\n\nNow imagine you wish to produce a scatterplot (with length mapped to the y-axis and day mapped to the x-axis) to explore these data. Since although temp is categorical, it is ordered, we would also like to plot a line representing the overall trend in tadpole length in relation to temperature. Doing so would introduce one of two problems:\n\nlines can only be plotted when both x and y are mapped to continuous variables\nin order to plot a line, we would need to convert temperature into a numeric variable in some way, however doing so would mean that the axes labels loose their meaning.\n\n\n\nUsing a geom_blank allows us to define a line and maintain useful axes labels. The second and third examples below will illustrate the problem and solution respectively.\n\n\n\n\nFeature\n\n\ngeom\n\n\nstat\n\n\nposition\n\n\nAesthetic parameters / Notes\n\n\nExample plot\n\n\n\n\n\n\nBlank layer\n\n\n_blank\n\n\n_identity\n\n\nidentity\n\n\nx,y\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data=tadpole, aes(y = length, x = temp)) +\n    geom_blank()\n\n\n\n\n\nBlank layer\n\n\n_blank\n\n\n_summary\n\n\nidentity\n\n\nx,y\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data=tadpole, aes(y = length, x = as.numeric(temp))) +\n    geom_line(stat = 'summary', fun = mean) \n\n\n\n\n\nBlank layer\n\n\n_blank\n\n\n_summary\n\n\nidentity\n\n\nx,y\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data=tadpole, aes(y = length, x = temp))+\n    geom_blank() +\n    geom_line(aes(x = as.numeric(temp)),\n                  stat = 'summary', fun = mean) \n\n\n\n\n\n\n\ngeom_point draws points (scatterplot). Typically the stat used is stat_identity as we wish to use the values in two continuous vectors as the coordinates of each point.\nThe following list describes the mapping aesthetic properties associated with geom_point. The entries in bold are compulsory.\n\n\nShow attributes\n\n\n\n\n\n\n\n\nParameter\ngeom_point\n\n\n\n\naesthetics\n\n\n\nx - variable to map to the x-axis\n✔\n\n\ny - variable to map to the y-axis\n✔\n\n\ngroup - plot separate series without aesthetic differences\n✔\n\n\nalpha - transparency\n✔\n\n\ncolour - colour of the points/lines\n✔\n\n\nfill - inner colour of points/shapes\n✔\n\n\nlinetype - type of lines used to construct points/lines\n✔\n\n\nsize - thickness of the line\n✔\n\n\nshape - the plotting symbol/character\n✔\n\n\nweight - weightings of values\n✔\n\n\n\n\n\n\n\n\nFeature\n\n\ngeom\n\n\nstat\n\n\nposition\n\n\nAesthetic parameters / Notes\n\n\nExample plot\n\n\n\n\n\n\npoints layer\n\n\n_point\n\n\n_identity\n\n\nidentity\n\n\nx,ygeom_point forms the basis of various plots such as scatterplots, maps and others\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data=df, aes(y = y, x = x)) +\n    geom_point()\n\n\n\n\n\nmeans points layer\n\n\n_point\n\n\n_identity\n\n\nidentity\n\n\nx,y,funplots points based on the values provided\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data=CO2, aes(x = conc, y = uptake)) +\n    geom_point()\n\n\n\n\n\nmeans points layer\n\n\n_point\n\n\n_summary\n\n\nidentity\n\n\nx,y,funplots the result of the specified summary function\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data=CO2, aes(x = conc, y = uptake)) +\n    geom_point(stat = \"summary\", fun = mean)\n\n\n\n\n\nThe plotting symbols are specified by either a number (index of a plotting symbol - see below) or a single character (printed literally).\n\n\n\n\n\n\n\n\n\n\n\nThe following list describes the mapping aesthetic properties associated with geom_text. The entries in bold are compulsory.\n\n\nShow attributes\n\n\n\n\n\n\n\n\nParameter\ngeom_text\n\n\n\n\n- x - variable to map to the x-axis\n✔\n\n\n- y - variable to map to the y-axis\n✔\n\n\n- label - text to use as labels\n✔\n\n\n- group - plot separate series without aesthetic differences\n✔\n\n\n- alpha - transparency\n✔\n\n\n- colour - colour of the points/lines\n✔\n\n\n- fill - inner colour of points/shapes\n✔\n\n\n- linetype - type of lines used to construct points/lines\n✔\n\n\n- shape - symbol shape for points\n✔\n\n\n- size - size of symbol\n✔\n\n\n- family - font family\n✔\n\n\n- fontface - bold, italic, normal etc\n✔\n\n\n- hjust - horizontal justification\n✔\n\n\n- vjust - vertical justification\n✔\n\n\n\n\n\n\nadditional parameters\n\n\n\n- parse - whether to parse labels into expressions (to include special characters)\nFALSE\n\n\n- nudge_x - horizontal adjustments to label positions\n0\n\n\n- nudge_y - vertical adjustments to label positions\n0\n\n\n- check_overlap - whether to plot text that overlaps other text in layer\nFALSE\n\n\n\n\n\n\n\n\nFeature\n\n\ngeom\n\n\nstat\n\n\nposition\n\n\nAesthetic parameters / Notes\n\n\nExample plot\n\n\n\n\n\n\nText layer\n\n\n_text\n\n\n_identity\n\n\nidentity\n\n\nx,y,labelText on a plot - useful for depicting the location of observations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data=df, aes(y = y, x = x)) +\n    geom_text(aes(label = z))\n\n\n\n\n\nText layer\n\n\n_text\n\n\n_identity\n\n\nidentity\n\n\nx,y,labelText on a plot - useful for depicting the location of observations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data=CO2, aes(y = uptake, x = conc)) +\n    geom_text(aes(label = Treatment))\n\n\n\n\n\nText layer\n\n\n_text\n\n\n_identity\n\n\nidentity\n\n\nx,y,labelText on a plot - useful for depicting the location of observations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data=CO2, aes(y = uptake, x = conc)) +\n    geom_text(aes(label = toupper(substr(Treatment, 1, 1))))\n\n\n\n\n\nHorizontal (hjust) and vertical (vjust) text justification controls are often a source of confusion and this is further exacerbated when combined with angle control. The following excellent demonstration from here provides a visual aid to understanding the use of these controls.\n\ntd &lt;- expand.grid(\n    hjust=c(0, 0.5, 1),\n    vjust=c(0, 0.5, 1),\n    angle=c(0, 45, 90),\n    text=\"text\"\n)\n\nggplot(td, aes(x=hjust, y=vjust)) + \n    geom_point() +\n    geom_text(aes(label=text, angle=angle, hjust=hjust, vjust=vjust)) + \n    facet_grid(~angle) +\n    scale_x_continuous(breaks=c(0, 0.5, 1), expand=c(0, 0.2)) +\n    scale_y_continuous(breaks=c(0, 0.5, 1), expand=c(0, 0.2))\n\n\n\n\n\n\n\n\n\n\ngeom_path draws paths (line plots). Paths order the coordinates according to the order in the data frame (c.f. geom_line and geom_step)\n\n\nShow attributes\n\n\n\n\n\n\n\n\nParameter\ngeom_path\n\n\n\n\n- x - variable to map to the x-axis\n✔\n\n\n- y - variable to map to the y-axis\n✔\n\n\n- group - plot separate series without aesthetic differences\n✔\n\n\n- alpha - transparency\n✔\n\n\n- colour - colour of the points/lines\n✔\n\n\n- fill - inner colour of points/shapes\n✔\n\n\n- linetype - type of lines used to construct points/lines\n✔\n\n\n- shape - symbol shape for points\n✔\n\n\n- size - size of symbol\n✔\n\n\n\n\n\n\nadditional parameters\n\n\n\n- lineend - line end style (round, butt, squate)\n‘butt’\n\n\n- linejoin - line join style (round, mitre, bevel)\n‘round’\n\n\n- linemitre - line mitre limit\n10\n\n\n- arrow - arrow specification (grid::arrow())\nNULL\n\n\n\n\n\n\n\n\nFeature\n\n\ngeom\n\n\nstat\n\n\nposition\n\n\nAesthetic parameters / Notes\n\n\nExample plot\n\n\n\n\n\n\npaths layer\n\n\n_path\n\n\n_identity\n\n\nidentity\n\n\nx,ygeom_path draws lines connecting coordinates in the order present in the data frame\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data=df, aes(y = y, x = x)) +\n    geom_path()\n\n\n\n\n\nThe simple line types available are highlighted in the following figure:\n\n\n\n\n\n\n\n\n\n\n\ngeom_polygon draws polygons with the coordinates ordered according to the order in the data frame.\n\n\nShow attributes\n\n\n\n\n\n\n\n\nParameter\ngeom_polygon\n\n\n\n\n- x - variable to map to the x-axis\n✔\n\n\n- y - variable to map to the y-axis\n✔\n\n\n- group - plot separate series without aesthetic differences\n✔\n\n\n- alpha - transparency\n✔\n\n\n- colour - colour of the points/lines\n✔\n\n\n- fill - inner colour of points/shapes\n✔\n\n\n- linetype - type of lines used to construct points/lines\n✔\n\n\n- shape - symbol shape for points\n✔\n\n\n- size - size of symbol\n✔\n\n\n\n\n\n\nadditional parameters\n\n\n\n- rule - determines how holes in polygons are treated\n‘evenodd’\n\n\n\n\n\n\n\n\nFeature\n\n\ngeom\n\n\nstat\n\n\nposition\n\n\nAesthetic parameters / Notes\n\n\nExample plot\n\n\n\n\n\n\npolygon layer\n\n\n_polygon\n\n\n_identity\n\n\nidentity\n\n\nx,ygeom_polygon draws polygons using coordinates in the order present in the data frame\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data=df, aes(y = y, x = x)) +\n    geom_polygon()\n\n\n\n\n\n\n\ngeom_area draws areas under curves with the coordinates ordered according to the order in the data frame.\n\n\nShow attributes\n\n\n\n\n\n\n\n\nParameter\ngeom_area\n\n\n\n\n- x - variable to map to the x-axis\n✔\n\n\n- y - variable to map to the y-axis\n✔\n\n\n- group - plot separate series without aesthetic differences\n✔\n\n\n- alpha - transparency\n✔\n\n\n- colour - colour of the points/lines\n✔\n\n\n- fill - inner colour of points/shapes\n✔\n\n\n- linetype - type of lines used to construct points/lines\n✔\n\n\n- shape - symbol shape for points\n✔\n\n\n- size - size of symbol\n✔\n\n\n\n\n\n\nadditional parameters\n\n\n\n- outline.type - determines the type of outline to draw around area\n‘both’\n\n\n\n\n\n\n\n\nFeature\n\n\ngeom\n\n\nstat\n\n\nposition\n\n\nAesthetic parameters / Notes\n\n\nExample plot\n\n\n\n\n\n\narea layer\n\n\n_area\n\n\n_identity\n\n\nidentity\n\n\nx,ygeom_area draws areas under a curve using coordinates in the order present in the data frame\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data=df, aes(y = y, x = x)) +\n    geom_area()\n\n\n\n\n\n\n\ngeom_ribbon draws ribbons (polygons) based on upper (max) and lower (min) levels of y associated with each level of x and are typically used to represent uncertainty in trends.\n\n\nShow attributes\n\n\n\n\n\n\n\n\nParameter\ngeom_ribbon\n\n\n\n\n- x - variable to map to the x-axis\n✔\n\n\n- y - variable to map to the y-axis\n✔\n\n\n- group - plot separate series without aesthetic differences\n✔\n\n\n- alpha - transparency\n✔\n\n\n- colour - colour of the points/lines\n✔\n\n\n- fill - inner colour of points/shapes\n✔\n\n\n- linetype - type of lines used to construct points/lines\n✔\n\n\n- shape - symbol shape for points\n✔\n\n\n- size - size of symbol\n✔\n\n\n\n\n\n\nadditional parameters\n\n\n\n- outline.type - determines the type of outline to draw around area\n‘both’\n\n\n\n\n\n\n\n\nFeature\n\n\ngeom\n\n\nstat\n\n\nposition\n\n\nAesthetic parameters / Notes\n\n\nExample plot\n\n\n\n\n\n\nribbon layer\n\n\n_ribbon\n\n\n_identity\n\n\nidentity\n\n\nx,ygeom_ribbon draws ribbons on a plot - useful for depicting uncertainty (confidence/credibility) intervals\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data=df, aes(ymin = y -1, ymax = y + 1, x = x)) +\n    geom_ribbon()\n\n\n\n\n\nribbon layer\n\n\n_ribbon\n\n\n_identity\n\n\nidentity\n\n\nx,ygeom_ribbon draws ribbons on a plot - useful for depicting uncertainty (confidence/credibility) intervals\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBOD.lm &lt;- lm(demand ~ Time, data = BOD)\nnewdata &lt;- with(BOD, data.frame(Time = seq(min(Time), max(Time),\n                                           length = 100)))\nnewdata &lt;- newdata %&gt;% cbind(predict(BOD.lm, newdata = newdata,\n                                     interval = 'confidence'))\nggplot(data=newdata) +\n    geom_ribbon(aes(x = Time, ymin = lwr, ymax = upr))\n\n\n\n\n\nribbon layer\n\n\n_ribbon\n\n\n_identity\n\n\nidentity\n\n\nx,ygeom_ribbon draws ribbons on a plot - useful for depicting uncertainty (confidence/credibility) intervals\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBOD.lm &lt;- lm(demand ~ Time, data = BOD)\nnewdata &lt;- with(BOD, data.frame(Time = seq(min(Time), max(Time),\n                                           length = 100)))\nnewdata &lt;- newdata %&gt;% cbind(predict(BOD.lm, newdata = newdata,\n                                     interval = 'confidence'))\nggplot(data=newdata, aes(x = Time)) +\n    geom_ribbon(aes(x = Time, ymin = lwr, ymax = upr),\n                fill='orange') +\n    geom_line(aes(y = fit)) +\n    geom_point(data = BOD, aes(y=demand))",
    "crumbs": [
      "R basics",
      "The grammar of graphics"
    ]
  },
  {
    "objectID": "05_grammar_of_graphics.html#visualising-distributions-1",
    "href": "05_grammar_of_graphics.html#visualising-distributions-1",
    "title": "The grammar of graphics (ggplot2)",
    "section": "2.2 Visualising distributions",
    "text": "2.2 Visualising distributions\n\nBoxplots Histograms Densityplots Violinplots QQplots Barplots Dotplots Scatterplotmatrix \n\n\ngeom_boxplot constructs boxplots. The values of the various elements of the boxplot (quantiles, whiskers etc) are calculated by its main pairing function (stat_boxplot). The following list describes the mapping aesthetic properties associated with geom_boxplot. The entries in bold are compulsory. Note that boxplots are usually specified via the geom_boxplot function which will engage the stat_boxplot to calculate the quantiles, whiskers and outliers. Therefore, confusingly, when calling geom_boxplot, the compulsory parameters are actually those required by stat_boxplot (unless you indicated to use stat_identity).\n\n\nShow attributes\n\n\n\n\n\n\n\n\n\nParameter\ngeom_boxplot\nstat_boxplot\n\n\n\n\naesthetics\n\n\n\n\nx - variable to map to the x-axis\n✔\n✔\n\n\ny - variable to map to the other axis\n✔\n✔\n\n\nlower - value of the lower box line (25th percentile)\n✔\n\n\n\nmiddle - value of the middle box line (50th percentile)\n✔\n\n\n\nupper - value of the upper box line (75th percentile)\n✔\n\n\n\nymin - value of lower whisker\n✔\n\n\n\nymax - value of upper whisker\n✔\n\n\n\ngroup - plot separate series without aesthetic differences\n✔\n✔\n\n\nalpha - transparency\n✔\n✔\n\n\ncolour - colour of the points/lines\n✔\n✔\n\n\nfill - inner colour of points/shapes\n✔\n✔\n\n\nlinetype - type of lines used to construct points/lines\n✔\n✔\n\n\nsize - thickness of the line\n✔\n✔\n\n\nweight - weightings of values\n✔\n✔\n\n\n\n\n\n\n\nadditional parameters\n\n\n\n\noutlier.colour - color of symbols for outliers\nNULL\nNULL\n\n\noutlier.fill - fill of symbols for outliers\nNULL\nNULL\n\n\noutlier.shape - shape of symbols for outliers\nNULL\nNULL\n\n\noutlier.size - size of symbols for outliers\nNULL\nNULL\n\n\noutlier.stroke - colour of lines in symbols for outliers\nNULL\nNULL\n\n\noutlier.alpha - transparency of symbols for outliers\nNULL\nNULL\n\n\nnotch - whether to notch the boxplots\nFALSE\nFALSE\n\n\nnotchwidth - width of notch\n0.5\n0.5\n\n\n\n\n\n\n\nComputed variables\n\n\n\n\nlower - value of the lower box line (25th percentile)\n✔\n\n\n\nmiddle - value of the middle box line (50th percentile)\n✔\n\n\n\nupper - value of the upper box line (75th percentile)\n✔\n\n\n\nymin - value of lower whisker\n✔\n\n\n\nymax - value of upper whisker\n✔\n\n\n\n\n\n\n\n\n\nFeature\n\n\ngeom\n\n\nstat\n\n\nposition\n\n\nAesthetic parameters / Notes\n\n\nExample plot\n\n\n\n\n\n\nboxplot\n\n\n_boxplot\n\n\n_boxplot\n\n\ndodge\n\n\nxplot of quantiles, whiskers and outliers.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data=diamonds, aes(x = carat)) +\n    geom_boxplot()\n\n\n\n\n\nboxplot\n\n\n_boxplot\n\n\n_boxplot\n\n\ndodge\n\n\nyplot of quantiles, whiskers and outliers.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data=diamonds, aes(y = carat)) +\n    geom_boxplot()\n\n\n\n\n\nconditional boxplot\n\n\n_boxplot\n\n\n_boxplot\n\n\ndodge\n\n\ny,xplot of quantiles, whiskers and outliers.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data=diamonds, aes(y = carat, x = cut)) +\n    geom_boxplot()\n\n\n\n\n\n\n\ngeom_histogram draws histograms of continuous data after binning the data,\nThe following list describes the mapping aesthetic properties associated with geom_histogram. The entries in bold are compulsory.\n\n\nShow attributes\n\n\n\n\n\n\n\n\n\nParameter\ngeom_histogram\nstat_bin\n\n\n\n\naesthetics\n\n\n\n\nx - variable to map to the x-axis\n✔\n✔\n\n\ngroup - plot separate series without aesthetic differences\n✔\n✔\n\n\nalpha - transparency\n✔\n✔\n\n\ncolour - colour of the points/lines\n✔\n✔\n\n\nfill - inner colour of points/shapes\n✔\n✔\n\n\nlinetype - type of lines used to construct points/lines\n✔\n✔\n\n\nsize - thickness of the line\n✔\n✔\n\n\nweight - weightings of values\n✔\n✔\n\n\n\n\n\n\n\nadditional parameters\n\n\n\n\nbinwidth - width of the bins\nNULL\nNULL\n\n\nbins - number of bins\nNULL\nNULL\n\n\nbreaks - vector of bin boundaries\n\nNULL\n\n\ncenter - bin position specifier\n\nNULL\n\n\nboundary - bin position specifier\n\nNULL\n\n\nclosed - which bin edge is included\n\nc(‘right’,‘left’)\n\n\npad - whether to include empty bins at either end of x\n\nFALSE\n\n\norientation - which axis (x or y) to operate on\nNA\nNA\n\n\n\n\n\n\n\nComputed variables\n\n\n\n\ncount - number of points in bin\n✔\n\n\n\ndensity - density of points in bin\n✔\n\n\n\nncount - counts scaled to max of 1\n✔\n\n\n\nndensity - density scaled to max of 1\n✔\n\n\n\n\n\n\n\n\n\n\nFeature\n\n\ngeom\n\n\nstat\n\n\nposition\n\n\nAesthetic parameters / Notes\n\n\nExample plot\n\n\n\n\n\n\nhistogram layer\n\n\n_histogram\n\n\n_bin\n\n\nstack\n\n\nx or ygeom_histogram bins continuous data and uses the number of cases in each bin as the height of a set of rectangles.Computed variables:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = diamonds, aes(x = carat)) +\n    geom_histogram()\n\n\n\n\n\nhistogram layer\n\n\n_histogram\n\n\n_bin\n\n\nstack\n\n\nx or ythe granularity of the histogram can be altered by changing the binwidth.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = diamonds, aes(x = carat)) +\n    geom_histogram(binwidth = 0.05)\n\n\n\n\n\nhistogram layer\n\n\n_histogram\n\n\n_bin\n\n\nstack\n\n\nx or ythe granularity of the histogram can be altered by changing the number of bins.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = diamonds, aes(x = carat)) +\n    geom_histogram(bins = 10)\n\n\n\n\n\nhistogram layer\n\n\n_histogram\n\n\n_bin\n\n\nstack\n\n\nx or ygeom_histogram bins continuous data and uses the number of cases in each bin as the height of a set of rectangles.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = diamonds, aes(x = carat)) +\n    geom_histogram(aes(fill = cut))\n\n\n\n\n\nhistogram layer\n\n\n_histogram\n\n\n_bin\n\n\ndodge\n\n\nx or ygeom_histogram bins continuous data and uses the number of cases in each bin as the height of a set of rectangles.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = diamonds, aes(x = carat)) +\n    geom_histogram(aes(fill = cut), position = 'dodge')\n\n\n\n\n\n\n\ngeom_density constructs smooth density distributions from continuous vectors. The actual smoothed densities are calculated by its main pairing function (stat_density). The following list describes the mapping aesthetic properties associated with geom_density and stat_density. The entries in bold are compulsory. Note that density plots are usually specified via the geom_density function which will engage the stat_density. Therefore, confusingly, when calling geom_density, the compulsory paramaters are actually those required by stat_density (unless you indicated to use stat_identity).\n\n\nShow attributes\n\n\n\n\n\n\n\n\n\nParameter\ngeom_density\nstat_density\n\n\n\n\naesthetics\n\n\n\n\nx - variable to map to the x-axis\n✔\n✔\n\n\ngroup - plot separate series without aesthetic differences\n✔\n✔\n\n\nalpha - transparency\n✔\n✔\n\n\ncolour - colour of the points/lines\n✔\n✔\n\n\nfill - inner colour of points/shapes\n✔\n✔\n\n\nlinetype - type of lines used to construct points/lines\n✔\n✔\n\n\nsize - thickness of the line\n✔\n✔\n\n\nweight - weightings of values\n✔\n✔\n\n\n\n\n\n\n\nadditional parameters\n\n\n\n\nbw - bandwidth smoothing (either sd of kernel or name of function)\n“nrd0”\n“nrd0”\n\n\nadjust - multiplicate bandwidth adjustment\n1\n1\n\n\nkernel - density() kernel to use\n“gaussian”\n“gaussian”\n\n\nn - number of equ-spaced points to estimate density\n512\n512\n\n\ntrim - whether to trim the range of data\nFALSE\nFALSE\n\n\norientation - which axis (x or y) to operate on\nNA\nNA\n\n\n\n\n\n\n\nComputed variables\n\n\n\n\ndensity - density of points in bin\n✔\n\n\n\ncount - density * number of points\n✔\n\n\n\nscaled density - density scaled to max of 1\n✔\n\n\n\nndensity - density scaled to max of 1\n✔\n\n\n\n\n\n\n\n\n\nFeature\n\n\ngeom\n\n\nstat\n\n\nposition\n\n\nAesthetic parameters / Notes\n\n\nExample plot\n\n\n\n\n\n\ndensity layer\n\n\n_density\n\n\n_density\n\n\nidentity\n\n\nx or y\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = diamonds, aes(x = carat)) +\n    geom_density()\n\n\n\n\n\ndensity layer\n\n\n_density\n\n\n_density\n\n\nidentity\n\n\nx or y\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = diamonds, aes(x = carat)) +\n    geom_density(bw=0.1)\n\n\n\n\n\ndensity layer\n\n\n_density\n\n\n_density\n\n\nidentity\n\n\nx or y, fill or other groupingMultiple densities with overlapping ranges\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = diamonds, aes(x = carat)) +\n    geom_density(aes(fill = cut), alpha=0.5)\n\n\n\n\n\ndensity layer\n\n\n_density\n\n\n_density\n\n\nstack\n\n\nx or y, fill or other groupingMultiple densities stacked on top of one another\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = diamonds, aes(x = carat)) +\n    geom_density(aes(fill = cut), position = 'stack')\n\n\n\n\n\n\n\ngeom_violin constructs violin plots. Violin plots are a blend on boxplot and density plots in that they are a density plot mirrored across a central axis and displayed similarly to a boxplot. Since they are derived from density plots, geom_violin and its stat (stat_ydensity) have most of the same parameters as geom_density/stat_density. Violin plots are a useful way to present continuous distributions that have greater granularity than boxplots.\n\n\nShow attributes\n\n\n\n\n\n\n\n\n\nParameter\ngeom_violin\nstat_ydensity\n\n\n\n\naesthetics\n\n\n\n\nx - variable to map to the x-axis\n✔\n✔\n\n\ngroup - plot separate series without aesthetic differences\n✔\n✔\n\n\nalpha - transparency\n✔\n✔\n\n\ncolour - colour of the points/lines\n✔\n✔\n\n\nfill - inner colour of points/shapes\n✔\n✔\n\n\nlinetype - type of lines used to construct points/lines\n✔\n✔\n\n\nsize - thickness of the line\n✔\n✔\n\n\nweight - weightings of values\n✔\n✔\n\n\n\n\n\n\n\nadditional parameters\n\n\n\n\nbw - bandwidth smoothing (either sd of kernel or name of function)\n“nrd0”\n“nrd0”\n\n\nadjust - multiplicate bandwidth adjustment\n1\n1\n\n\nkernel - density() kernel to use\n“gaussian”\n“gaussian”\n\n\nn - number of equ-spaced points to estimate density\n512\n512\n\n\ntrim - whether to trim the range of data\nFALSE\nFALSE\n\n\norientation - which axis (x or y) to operate on\nNA\nNA\n\n\n\n\n\n\n\nComputed variables\n\n\n\n\ndensity - density of points in bin\n✔\n\n\n\ncount - density * number of points\n✔\n\n\n\nscaled density - density scaled to max of 1\n✔\n\n\n\nndensity - density scaled to max of 1\n✔\n\n\n\n\n\n\n\n\n\nFeature\n\n\ngeom\n\n\nstat\n\n\nposition\n\n\nAesthetic parameters / Notes\n\n\nExample plot\n\n\n\n\n\n\nviolin\n\n\n_violin\n\n\n_ydensity\n\n\ndodge\n\n\nxplot of quantiles, whiskers and outliers.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(diamonds) +\n    geom_violin(aes(y = carat, x = carat), orientation = 'x')\n\n\n\n\n\nviolin\n\n\n_violin\n\n\n_ydensity\n\n\ndodge\n\n\nyplot of quantiles, whiskers and outliers.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(diamonds) +\n    geom_violin(aes(y = carat, x = carat), orientation = 'y')\n\n\n\n\n\nconditional violin\n\n\n_violin\n\n\n_ydensity\n\n\ndodge\n\n\ny,xplot of quantiles, whiskers and outliers.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(diamonds) +\n    geom_violin(aes(y = carat, x = cut))\n\n\n\n\n\n\n\ngeom_qq constructs quantile-quantile (QQ) plots. QQ plots illustrate the quantiles of the sample distribution against the quantiles expected for the theoretical distribution. A straight line implies that the sample distribution matches the theoretical distribution well. Deviations (typically at the tails) imply a lack of match. To help assess deviations from linearity, the geom_qq_line/stat_qq_line function depicts the ideal match as a straight line for comparison.\n\n\n\n\nShow attributes\n\n\n\n\n\n\n\n\n\nParameter\ngeom_qq\nstat_qq\n\n\n\n\naesthetics\n\n\n\n\nsample - variable to map to the x-axis\n✔\n✔\n\n\ngroup - plot separate series without aesthetic differences\n✔\n✔\n\n\nalpha - transparency\n✔\n✔\n\n\ncolour - colour of the points/lines\n✔\n✔\n\n\nfill - inner colour of points/shapes\n✔\n✔\n\n\nlinetype - type of lines used to construct points/lines\n✔\n✔\n\n\nsize - thickness of the line\n✔\n✔\n\n\nweight - weightings of values\n✔\n✔\n\n\n\n\n\n\n\nadditional parameters\n\n\n\n\ndistribution - function that calculates quantiles for a distribution\nstats::qnorm\nstats::qnorm\n\n\ndparams - additional parameters for the distribution\nlist()\nlist()\n\n\norientation - which axis (x or y) to operate on\nNA\nNA\n\n\n\n\n\n\n\nComputed variables\n\n\n\n\nsample - sample quantiles\n✔\n\n\n\ntheoretical - theoretical quantiles\n✔\n\n\n\n\n\n\n\n\n\nFeature\n\n\ngeom\n\n\nstat\n\n\nposition\n\n\nAesthetic parameters / Notes\n\n\nExample plot\n\n\n\n\n\n\nQQ plot\n\n\n_qq\n\n\n_qq\n\n\nidentity\n\n\nsample\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(diamonds, aes(sample = carat)) +\n    geom_qq() +\n    geom_qq_line()\n\n\n\n\n\nQQ plot\n\n\n_qq\n\n\n_qq\n\n\nidentity\n\n\nsamplethis example will explore a Gamma theoretical distribution (rather than the default Gaussian). When specifying an alternative distribution, it may be necessary to supply certain distribution parameters. In this case, the appropriate shape parameter for the Gamma distribution is required. This is first estimated via the fitdistr() function from the MASS _package`.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nshape &lt;- MASS::fitdistr(diamonds$carat,\n                        densfun = \"gamma\")$estimate[\"shape\"]\nggplot(diamonds, aes(sample = carat)) +\n    geom_qq(distribution = stats::qgamma,\n            dparams = list(shape = shape)) +\n    geom_qq_line(distribution = stats::qgamma,\n                 dparams = list(shape = shape))\n\n\n\n\n\nQQ plot\n\n\n_qq\n\n\n_qq\n\n\nidentity\n\n\nsample\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(diamonds, aes(sample = carat, colour = cut)) +\n    geom_qq() +\n    geom_qq_line()\n\n\n\n\n\n\n\ngeom_bar constructs barcharts. By default, the bins of each bar along with the associated bar heights are calculated by the stats_count function. The following list describes the mapping aesthetic properties associated with geom_bar and stats_count. The entries in bold are compulsory.\n\n\n\n\nShow attributes\n\n\n\n\n\n\n\n\n\nParameter\ngeom_bar\nstat_count\n\n\n\n\naesthetics\n\n\n\n\nx or y - variable to map to the x/y-axis\n✔\n✔\n\n\ngroup - plot separate series without aesthetic differences\n✔\n✔\n\n\nalpha - transparency\n✔\n✔\n\n\ncolour - colour of the points/lines\n✔\n✔\n\n\nfill - inner colour of points/shapes\n✔\n✔\n\n\nlinetype - type of lines used to construct points/lines\n✔\n✔\n\n\nsize - thickness of the line\n✔\n✔\n\n\nweight - weightings of values\n✔\n✔\n\n\n\n\n\n\n\nadditional parameters\n\n\n\n\nwidth - width of the bars\nNULL\nNULL\n\n\norientation - which axis (x or y) to operate on\nNA\nNA\n\n\n\n\n\n\n\nComputed variables\n\n\n\n\ncount - number of points to bin\n✔\n\n\n\nprop - groupwise proportion\n✔\n\n\n\n\n\n\n\n\n\nFeature\n\n\ngeom\n\n\nstat\n\n\nposition\n\n\nAesthetic parameters / Notes\n\n\nExample plot\n\n\n\n\n\n\nBar plot\n\n\n_bar\n\n\n_count\n\n\nstack\n\n\nsample\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(diamonds, aes(x = cut)) +\n    geom_bar()\n\n\n\n\n\nStacked bar plot\n\n\n_bar\n\n\n_count\n\n\nstack\n\n\nsampleby default a viridis colour blind safe palette is applied.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(diamonds, aes(x = cut, fill = clarity)) +\n    geom_bar()\n\n\n\n\n\nConditional bar plot\n\n\n_bar\n\n\n_count\n\n\ndodge2\n\n\nsampleby default a viridis colour blind safe palette is applied.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(diamonds, aes(x = cut, fill = clarity)) +\n    geom_bar(position = \"dodge2\")\n\n\n\n\n\n\n\n\n\ngeom_dotplot draws dotplots of continuous data after binning the data. This geom is unusual in that it does not have an associated stat. Instead, it calls its own density function. Dotplots are really only useful for small datasets. The reason for this is that the counts in each bin are represented by circles (the size of which is determined by the bin width) and thus if the counts are high, the stack of circles will extend beyond the y-axis (unless the stack ratio is altered). Moreover, the y-axis scale is meaningless.\nThe following list describes the mapping aesthetic properties associated with geom_dotplot. The entries in bold are compulsory.\n\n\nShow attributes\n\n\n\n\n\n\n\n\nParameter\ngeom_histogram\n\n\n\n\naesthetics\n\n\n\nx - variable to map to the x-axis\n✔\n\n\ngroup - plot separate series without aesthetic differences\n✔\n\n\nalpha - transparency\n✔\n\n\ncolour - colour of the points/lines\n✔\n\n\nfill - inner colour of points/shapes\n✔\n\n\nlinetype - type of lines used to construct points/lines\n✔\n\n\nsize - thickness of the line\n✔\n\n\nweight - weightings of values\n✔\n\n\n\n\n\n\nadditional parameters\n\n\n\nbinwidth - (maximum) bin width\nNULL\n\n\nbinaxis - the axis to bin along\n“x”\n\n\nmethod - method for binning\n“dotdensity”\n\n\nbinpositions - method for determining the position of dots\n“bygroup”\n\n\nstackdir - which direction to stack dots\n“up”\n\n\nstackratio - how close to stack dots\n1\n\n\ndotsize - diameter of dot relative to binwidth\n1\n\n\nstackgroups - whether to stack dots across groups\nFALSE\n\n\norigin - origin of first bin\nNULL\n\n\nright - should bin intervals be closed on the right (a, b] or not [a, b)\nTRUE\n\n\nwidth - when binaxis is “y”, the spacing of dot stacks for dodging\n0.9\n\n\ndrop - whether to remove bins with zero counts\nFALSE\n\n\n\n\n\n\nComputed variables\n\n\n\ncount - number of points in bin\n✔\n\n\ndensity - density of points in bin\n✔\n\n\nncount - counts scaled to max of 1\n✔\n\n\nndensity - density scaled to max of 1\n✔\n\n\n\n\n\n\n\n\nFeature\n\n\ngeom\n\n\nstat\n\n\nposition\n\n\nAesthetic parameters / Notes\n\n\nExample plot\n\n\n\n\n\n\ndotplot layer\n\n\n_dotplot\n\n\n\n\nidentity\n\n\nx or y\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = CO2, aes(x = uptake)) +\n    geom_dotplot()\n\n\n\n\n\ndotplot layer\n\n\n_dotplot\n\n\n\n\nidentity\n\n\nx or y\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = CO2, aes(x = uptake)) +\n    geom_dotplot(binwidth = 3)\n\n\n\n\n\ndotplot layer\n\n\n_dotplot\n\n\n\n\nidentity\n\n\nx or y\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = CO2, aes(x = uptake, fill = Type)) +\n    geom_dotplot(stackgroups = TRUE,\n                 binpositions = 'all',\n                 binwidth=3, dotsize=1) \n\n\n\n\n\n\n\nThe ggpairs() function is not actually part of the ggplot2 package. Rather it is part of its own package (GGally). Nevertheless, this graphic, in which all pairs of variables are plotted in a matrix of panels can be a very useful way to explore the individual and relational characteristics of multiple variables simultaneously.\n\n\n\n\nShow attributes\n\n\n\n\n\n\n\n\nParameter\ngeom_histogram\n\n\n\n\naesthetics\n\n\n\ngroup - plot separate series without aesthetic differences\n✔\n\n\nalpha - transparency\n✔\n\n\ncolour - colour of the points/lines\n✔\n\n\nfill - inner colour of points/shapes\n✔\n\n\nlinetype - type of lines used to construct points/lines\n✔\n\n\nsize - thickness of the line\n✔\n\n\nweight - weightings of values\n✔\n\n\n\n\n\n\nadditional parameters\n\n\n\ncolumns - which columns to include\n1:ncol(data)\n\n\ntitle,xlab,ylab - titles\nNULL\n\n\nupper,lower - functions for plots to appear in off diagonal panels\n…\n\n\ndiag - function for plots to appear in diagonal panels\n…\n\n\naxisLabels - whether and how to display axis labels\nc(“show”, “internal”, “none”)\n\n\ncolumnLabels - label names to display\ncolnames(data[columns])\n\n\nlabeller - facet labeller function\n“label_value”\n\n\nswitch - which (if any) facet labels to switch position\nNULL\n\n\nshowStrips - whether to display all strips (panel banners)\nNULL\n\n\nlegend - whether or position of legend\nNULL\n\n\ncardinality_threshold - max number of factor levels permitted\n15\n\n\nprogress - whether to show a progress bar while computing (if &gt;15 panels)\nNULL\n\n\nproportions - amount of area given to each plot\nNULL\n\n\n\n\n\n\n\n\nFeature\n\n\ngeom\n\n\nstat\n\n\nposition\n\n\nAesthetic parameters / Notes\n\n\nExample plot\n\n\n\n\n\n\ndotplot layer\n\n\n_dotplot\n\n\n\n\nidentity\n\n\nx or y\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggpairs(iris, colour = \"Species\", upper = list(continuous = \"density\",\n    combo = \"box\"), diag = list(continuous = \"density\"),\n    lower = list(continuous = \"smooth\"), axisLabels = \"show\")",
    "crumbs": [
      "R basics",
      "The grammar of graphics"
    ]
  },
  {
    "objectID": "05_grammar_of_graphics.html#visualising-trends-1",
    "href": "05_grammar_of_graphics.html#visualising-trends-1",
    "title": "The grammar of graphics (ggplot2)",
    "section": "2.3 Visualising trends",
    "text": "2.3 Visualising trends\n\nScatterplots Lineplots Smootherplots Tiles Raster Contours Filledcontour \n\n\nsee points\n\n\ngeom_line draws lines joining coordinates. Typically the stat used is stat_identity as we wish to use the values in two continuous vectors as the coordinates of each line segment. geom_line differs from geom_path in that the former first orders the data according to the x-axis.\n\n\nShow attributes\n\n\n\n\n\n\n\n\nParameter\ngeom_line\n\n\n\n\naesthetics\n\n\n\nx - variable to map to the x-axis\n✔\n\n\ny - variable to map to the other axis\n✔\n\n\ngroup - plot separate series without aesthetic differences\n✔\n\n\nalpha - transparency\n✔\n\n\ncolour - colour of the points/lines\n✔\n\n\nfill - inner colour of points/shapes\n✔\n\n\nlinetype - type of lines used to construct points/lines\n✔\n\n\nsize - thickness of the line\n✔\n\n\nweight - weightings of values\n✔\n\n\n\n\n\n\nadditional parameters\n\n\n\n\n\n\n\nComputed variables\n\n\n\n\n\n\n\n\n\nFeature\n\n\ngeom\n\n\nstat\n\n\nposition\n\n\nAesthetic parameters / Notes\n\n\nExample plot\n\n\n\n\n\n\nline plot\n\n\n_line\n\n\n_identity\n\n\nidentity\n\n\nx,yscatterplot on only the first level of Plant.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(filter(CO2, Plant == first(Plant))) +\n    geom_line(aes(x = conc, y = uptake)) \n\n\n\n\n\nline plot\n\n\n_line\n\n\n_identity\n\n\nidentity\n\n\nx,yscatterplot of each Plant level.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = CO2, aes(x = conc, y = uptake)) +\n    geom_line(aes(group = Plant))\n\n\n\n\n\nline plot\n\n\n_line\n\n\n_identity\n\n\nidentity\n\n\nx,yscatterplot of each Plant level coloured separately.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = CO2, aes(x = conc, y = uptake)) +\n    geom_line(aes(colour = Plant))\n\n\n\n\n\nline plot\n\n\n_line\n\n\n_identity\n\n\nidentity\n\n\nx,yscatterplot of mean across all Plants.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = CO2, aes(x = conc, y = uptake)) +\n    geom_line(stat = \"summary\", fun = mean)\n\n\n\n\n\nline plot\n\n\n_line\n\n\n_identity\n\n\nidentity\n\n\nx,yscatterplot of mean across all Plants.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = CO2, aes(x = conc, y = uptake)) +\n    geom_line(stat = \"summary\", fun = mean, aes(color = Type))\n\n\n\n\n\n\n\ngeom_smooth draws smooths lines (and 95% confidence intervals) through data clouds. Typically the stat used is stat_smooth which in turn engages one of the available smoothing methods (e.g. lm, glm, gam, loess or rlm).\n\n\nShow attributes\n\n\n\n\n\n\n\n\n\nParameter\ngeom_smooth\ngeom_smooth\n\n\n\n\naesthetics\n\n\n\n\nx - variable to map to the x-axis\n✔\n✔\n\n\ny - variable to map to the other axis\n✔\n✔\n\n\ngroup - plot separate series without aesthetic differences\n✔\n✔\n\n\nalpha - transparency\n✔\n✔\n\n\ncolour - colour of the points/lines\n✔\n✔\n\n\nfill - inner colour of points/shapes\n✔\n✔\n\n\nlinetype - type of lines used to construct points/lines\n✔\n✔\n\n\nsize - thickness of the line\n✔\n✔\n\n\nweight - weightings of values\n✔\n✔\n\n\n\n\n\n\n\nadditional parameters\n\n\n\n\nmethod - smoothing method (modelling function)\nNULL\nNULL\n\n\nformula - formula to use in smoothing function\nNULL\nNULL\n\n\nse - whether to display confidence intervals\nTRUE\nTRUE\n\n\nn - number of points at which to evaluate the smoother\n\n80\n\n\nspan - degree of smoothing for loess smoother\n\n0.75\n\n\nfullrange - whether the fit should span full range (or just data)\n\nFALSE\n\n\nmethod.args - additional arguments passed on to modelling function\n\nlist()\n\n\n\n\n\n\n\nComputed variables\n\n\n\n\ny or x - the predicted value\n✔\n\n\n\nymin or xmin - lower confidence interval\n✔\n\n\n\nymax or xmax - upper confidence interval\n✔\n\n\n\nse - standard error\n✔\n\n\n\n\n\n\n\n\n\nFeature\n\n\ngeom\n\n\nstat\n\n\nposition\n\n\nAesthetic parameters / Notes\n\n\nExample plot\n\n\n\n\n\n\nline plot\n\n\n_line\n\n\n_identity\n\n\nidentity\n\n\nx,yscatterplot on only the first level of Plant.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(filter(CO2, Plant == first(Plant))) +\n    geom_smooth(aes(x = conc, y = uptake)) \n\n\n\n\n\nline plot\n\n\n_line\n\n\n_identity\n\n\nidentity\n\n\nx,yscatterplot of each Plant level.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = CO2, aes(x = conc, y = uptake)) +\n    geom_smooth(aes(colour = Type))\n\n\n\n\n\nline plot\n\n\n_line\n\n\n_identity\n\n\nidentity\n\n\nx,yscatterplot of each Plant level coloured separately.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = BOD, aes(x = Time, y = demand)) +\n    geom_smooth(method = \"lm\", formula = \"y ~ x\")\n\n\n\n\n\nline plot\n\n\n_line\n\n\n_identity\n\n\nidentity\n\n\nx,yscatterplot of each Plant level coloured separately.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = BOD, aes(x = Time, y = demand)) +\n    geom_smooth(method = \"lm\", formula = \"y ~ x\")\n\n\n\n\n\n\n\ngeom_tile constructs heat maps given x,y coordinates and a z value to associate with the fill of each tile. Note, the data must be a complete grid.\n\n\nShow attributes\n\n\n\n\n\n\n\n\nParameter\ngeom_tile\n\n\n\n\naesthetics\n\n\n\nx - variable to map to the x-axis\n✔\n\n\ny - variable to map to the other axis\n✔\n\n\ngroup - plot separate series without aesthetic differences\n✔\n\n\nalpha - transparency\n✔\n\n\ncolour - colour of the points/lines\n✔\n\n\nfill - inner colour of points/shapes\n✔\n\n\nlinetype - type of lines used to construct points/lines\n✔\n\n\nsize - thickness of the line\n✔\n\n\nweight - weightings of values\n✔\n\n\n\n\n\n\nadditional parameters\n\n\n\nlinejoin - line join style\n“mitre”\n\n\n\n\n\n\nComputed variables\n\n\n\n\n\n\n\n\n\nFeature\n\n\ngeom\n\n\nstat\n\n\nposition\n\n\nAesthetic parameters / Notes\n\n\nExample plot\n\n\n\n\n\n\ntile plot\n\n\n_tile\n\n\n_identity\n\n\nidentity\n\n\nx,yheatmap of 2d density.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(faithfuld, aes(y = eruptions, x = waiting)) +\n    geom_tile(aes(fill = density)) \n\n\n\n\n\n\n\ngeom_raster is similar to geom_tile in that it constructs heat maps given x,y coordinates and a z value to associate with the fill of each tile. However, unlike geom_tile, a full grid is not required as geom_raster is able to interpolate over the grid. The interpolation can also smooth out the grid surface. This interpolation does make geom_raster slower than geom_tile.\n\n\nShow attributes\n\n\n\n\n\n\n\n\nParameter\ngeom_raster\n\n\n\n\naesthetics\n\n\n\nx - variable to map to the x-axis\n✔\n\n\ny - variable to map to the other axis\n✔\n\n\ngroup - plot separate series without aesthetic differences\n✔\n\n\nalpha - transparency\n✔\n\n\ncolour - colour of the points/lines\n✔\n\n\nfill - inner colour of points/shapes\n✔\n\n\nlinetype - type of lines used to construct points/lines\n✔\n\n\nsize - thickness of the line\n✔\n\n\nweight - weightings of values\n✔\n\n\n\n\n\n\nadditional parameters\n\n\n\nhjust - line join style\n0.5\n\n\nvjust - line join style\n0.5\n\n\ninterpolate - line join style\nFALSE\n\n\n\n\n\n\nComputed variables\n\n\n\n\n\n\n\n\n\nFeature\n\n\ngeom\n\n\nstat\n\n\nposition\n\n\nAesthetic parameters / Notes\n\n\nExample plot\n\n\n\n\n\n\nraster plot\n\n\n_raster\n\n\n_identity\n\n\nidentity\n\n\nx,yheatmap of 2d density.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(faithfuld, aes(y = eruptions, x = waiting)) +\n    geom_raster(aes(fill = density)) \n\n\n\n\n\n\n\ngeom_contour constructs contour maps given x,y coordinates and a z value from which to calculate each contour.\n\n\nShow attributes\n\n\n\n\n\n\n\n\n\nParameter\ngeom_contour\nstat_contour\n\n\n\n\naesthetics\n\n\n\n\nx - variable to map to the x-axis\n✔\n✔\n\n\ny - variable to map to the y axis\n✔\n✔\n\n\nz - variable to map to the z axis\n✔\n✔\n\n\ngroup - plot separate series without aesthetic differences\n✔\n✔\n\n\nalpha - transparency\n✔\n✔\n\n\ncolour - colour of the points/lines\n✔\n✔\n\n\nfill - inner colour of points/shapes\n✔\n✔\n\n\nlinetype - type of lines used to construct points/lines\n✔\n✔\n\n\nsize - thickness of the line\n✔\n✔\n\n\nweight - weightings of values\n✔\n✔\n\n\n\n\n\n\n\nadditional parameters\n\n\n\n\nbins - number of contour bins\nNULL\nNULL\n\n\nbinwidth - width of contour bins\nNULL\nNULL\n\n\nbreaks - numer of contour bins (alternative to bins)\nNULL\nNULL\n\n\nlineend - line end style\n“butt”\n“butt”\n\n\nlinejoin - line join style\n“round”\n“round”\n\n\nlinemitre - line mitre style\n10\n10\n\n\n\n\n\n\n\nComputed variables\n\n\n\n\nlevel - bin boundaries\n✔\n\n\n\nlevel_low, level_high, level_mid - bin boundaries per band\n✔\n\n\n\nnlevel - height of contour, scaled to max of 1\n✔\n\n\n\npiece - contour piece\n✔\n\n\n\n\n\n\n\n\n\nFeature\n\n\ngeom\n\n\nstat\n\n\nposition\n\n\nAesthetic parameters / Notes\n\n\nExample plot\n\n\n\n\n\n\ncontour plot\n\n\n_contour\n\n\n_identity\n\n\nidentity\n\n\nx,ycontour plot of 2d density in relation to eruption and waiting times.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(faithfuld, aes(y = eruptions, x = waiting)) +\n    geom_contour(aes(z = density)) \n\n\n\n\n\n\n\ngeom_contour constructs contour maps given x,y coordinates and a z value from which to calculate each contour.\n\n\nShow attributes\n\n\n\n\n\n\n\n\n\nParameter\ngeom_contour_filled\nstat_contour_filled\n\n\n\n\naesthetics\n\n\n\n\nx - variable to map to the x-axis\n✔\n✔\n\n\ny - variable to map to the y axis\n✔\n✔\n\n\nz - variable to map to the z axis\n✔\n✔\n\n\ngroup - plot separate series without aesthetic differences\n✔\n✔\n\n\nalpha - transparency\n✔\n✔\n\n\ncolour - colour of the points/lines\n✔\n✔\n\n\nfill - inner colour of points/shapes\n✔\n✔\n\n\nlinetype - type of lines used to construct points/lines\n✔\n✔\n\n\nsize - thickness of the line\n✔\n✔\n\n\nweight - weightings of values\n✔\n✔\n\n\n\n\n\n\n\nadditional parameters\n\n\n\n\nbins - number of contour bins\nNULL\nNULL\n\n\nbinwidth - width of contours\nNULL\nNULL\n\n\nbreaks - sets contour breaks (alternative to bins)\nNULL\nNULL\n\n\nlineend - line end style\n“butt”\n“butt”\n\n\nlinejoin - line join style\n“round”\n“round”\n\n\nlinemitre - line mitre style\n10\n10\n\n\n\n\n\n\n\nComputed variables\n\n\n\n\nlevel - bin boundaries\n✔\n\n\n\nnlevel - height of filled_contour, scaled to max of 1\n✔\n\n\n\npiece - filled_contour piece\n✔\n\n\n\n\n\n\n\n\n\nFeature\n\n\ngeom\n\n\nstat\n\n\nposition\n\n\nAesthetic parameters / Notes\n\n\nExample plot\n\n\n\n\n\n\nfilled_contour plot\n\n\n_filled_contour\n\n\n_identity\n\n\nidentity\n\n\nx,yfilled contour plot of 2d density in relation to eruption and waiting times.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(faithfuld, aes(y = eruptions, x = waiting)) +\n    geom_contour_filled(aes(z = density))",
    "crumbs": [
      "R basics",
      "The grammar of graphics"
    ]
  },
  {
    "objectID": "05_grammar_of_graphics.html#visualising-uncertainty",
    "href": "05_grammar_of_graphics.html#visualising-uncertainty",
    "title": "The grammar of graphics (ggplot2)",
    "section": "2.4 Visualising uncertainty",
    "text": "2.4 Visualising uncertainty\n\nErrorbars Lineranges Pointranges Ribbons \n\n\ngeom_errorbar draws error bars based on upper and lower levels of y associated with each level of x.\n\n\nShow attributes\n\n\n\n\n\n\n\n\nParameter\ngeom_errorbar\n\n\n\n\naesthetics\n\n\n\nx - variable to map to the x-axis\n✔\n\n\ny - variable to map to the y axis\n✔\n\n\ngroup - plot separate series without aesthetic differences\n✔\n\n\nalpha - transparency\n✔\n\n\ncolour - colour of the points/lines\n✔\n\n\nfill - inner colour of points/shapes\n✔\n\n\nlinetype - type of lines used to construct points/lines\n✔\n\n\nsize - thickness of the line\n✔\n\n\nweight - weightings of values\n✔\n\n\n\n\n\n\nadditional parameters\n\n\n\nwidth - width of the caps on the bars\n1\n\n\n\n\n\n\nComputed variables\n\n\n\n\n\n\n\n\n\nFeature\n\n\ngeom\n\n\nstat\n\n\nposition\n\n\nAesthetic parameters / Notes\n\n\nExample plot\n\n\n\n\n\n\nerror bars\n\n\n_errorbar\n\n\n_identity\n\n\nidentity\n\n\nx,y\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwarpbreaks.sum &lt;- warpbreaks %&gt;%\n    group_by(wool) %&gt;%\n    summarise(mean_se(breaks))\nggplot(warpbreaks.sum, aes(y = y, x = wool, ymin = ymin, ymax = ymax)) +\n    geom_errorbar() \n\n\n\n\n\nerror bars\n\n\n_errorbar\n\n\n_identity\n\n\nidentity\n\n\nx,y\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwarpbreaks.sum &lt;- warpbreaks %&gt;%\n    group_by(wool) %&gt;%\n    summarise(mean_se(breaks))\nggplot(warpbreaks.sum, aes(y = y, x = wool, ymin = ymin, ymax = ymax)) +\n    geom_errorbar(width = 0.25) \n\n\n\n\n\nerror bars\n\n\n_errorbar\n\n\n_identity\n\n\nidentity\n\n\nx,y\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwarpbreaks.sum &lt;- warpbreaks %&gt;%\n    group_by(wool) %&gt;%\n    summarise(mean_se(breaks))\nggplot(warpbreaks.sum, aes(y = y, x = wool, ymin = ymin, ymax = ymax)) +\n    geom_crossbar(width = 0.25) \n\n\n\n\n\nerror bars\n\n\n_errorbar\n\n\n_identity\n\n\nidentity\n\n\nx,y\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwarpbreaks.sum &lt;- warpbreaks %&gt;%\n    group_by(wool, tension) %&gt;%\n    summarise(mean_se(breaks))\nggplot(warpbreaks.sum, aes(y = y, x = tension, ymin = ymin, ymax = ymax)) +\n    geom_errorbar(aes(colour = wool),\n                  position = position_dodge(width = 0.5),\n                  width = 0.25) \n\n\n\n\n\n\n\ngeom_lineranges draws uncertainty bars based on upper and lower levels of y associated with each level of x.\n\n\nShow attributes\n\n\n\n\n\n\n\n\nParameter\ngeom_linerange\n\n\n\n\naesthetics\n\n\n\nx - variable to map to the x-axis\n✔\n\n\ny - variable to map to the y axis\n✔\n\n\ngroup - plot separate series without aesthetic differences\n✔\n\n\nalpha - transparency\n✔\n\n\ncolour - colour of the points/lines\n✔\n\n\nfill - inner colour of points/shapes\n✔\n\n\nlinetype - type of lines used to construct points/lines\n✔\n\n\nsize - thickness of the line\n✔\n\n\nweight - weightings of values\n✔\n\n\n\n\n\n\nadditional parameters\n\n\n\n\n\n\n\nComputed variables\n\n\n\n\n\n\n\n\n\nFeature\n\n\ngeom\n\n\nstat\n\n\nposition\n\n\nAesthetic parameters / Notes\n\n\nExample plot\n\n\n\n\n\n\nline range\n\n\n_linerange\n\n\n_identity\n\n\nidentity\n\n\nx,y\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwarpbreaks.sum &lt;- warpbreaks %&gt;%\n    group_by(wool) %&gt;%\n    summarise(mean_se(breaks))\nggplot(warpbreaks.sum, aes(y = y, x = wool, ymin = ymin, ymax = ymax)) +\n    geom_linerange() \n\n\n\n\n\nline ranges\n\n\n_lineranges\n\n\n_identity\n\n\nidentity\n\n\nx,y\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwarpbreaks.sum &lt;- warpbreaks %&gt;%\n    group_by(wool, tension) %&gt;%\n    summarise(mean_se(breaks))\nggplot(warpbreaks.sum, aes(y = y, x = tension, ymin = ymin, ymax = ymax)) +\n    geom_linerange(aes(colour = wool),\n                  position = position_dodge(width = 0.5)) \n\n\n\n\n\n\n\ngeom_lineranges draws uncertainty bars based on upper and lower levels of y associated with each level of x.\n\n\nShow attributes\n\n\n\n\n\n\n\n\nParameter\ngeom_pointrange\n\n\n\n\naesthetics\n\n\n\nx - variable to map to the x-axis\n✔\n\n\ny - variable to map to the y axis\n✔\n\n\ngroup - plot separate series without aesthetic differences\n✔\n\n\nalpha - transparency\n✔\n\n\ncolour - colour of the points/lines\n✔\n\n\nfill - inner colour of points/shapes\n✔\n\n\nlinetype - type of lines used to construct points/lines\n✔\n\n\nsize - thickness of the line\n✔\n\n\nweight - weightings of values\n✔\n\n\n\n\n\n\nadditional parameters\n\n\n\nflatten - a multiplicative factor to increase the point size\n1\n\n\n\n\n\n\nComputed variables\n\n\n\n\n\n\n\n\n\nFeature\n\n\ngeom\n\n\nstat\n\n\nposition\n\n\nAesthetic parameters / Notes\n\n\nExample plot\n\n\n\n\n\n\npoint range\n\n\n_pointrange\n\n\n_identity\n\n\nidentity\n\n\nx,y\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwarpbreaks.sum &lt;- warpbreaks %&gt;%\n    group_by(wool) %&gt;%\n    summarise(mean_se(breaks))\nggplot(warpbreaks.sum, aes(y = y, x = wool, ymin = ymin, ymax = ymax)) +\n    geom_pointrange() \n\n\n\n\n\npoint ranges\n\n\n_pointranges\n\n\n_identity\n\n\nidentity\n\n\nx,y\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwarpbreaks.sum &lt;- warpbreaks %&gt;%\n    group_by(wool) %&gt;%\n    summarise(mean_se(breaks))\nggplot(warpbreaks.sum, aes(y = y, x = wool, ymin = ymin, ymax = ymax)) +\n    geom_pointrange(fatten = 5) \n\n\n\n\n\npoint ranges\n\n\n_pointranges\n\n\n_identity\n\n\nidentity\n\n\nx,y\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwarpbreaks.sum &lt;- warpbreaks %&gt;%\n    group_by(wool, tension) %&gt;%\n    summarise(mean_se(breaks))\nggplot(warpbreaks.sum, aes(y = y, x = tension, ymin = ymin, ymax = ymax)) +\n    geom_pointrange(aes(colour = wool),\n                  position = position_dodge(width = 0.5)) \n\n\n\n\n\n\n\ngeom_ribbon draws uncertainty envelopes based on upper and lower levels of y associated with each level of x.\nsee ribbons",
    "crumbs": [
      "R basics",
      "The grammar of graphics"
    ]
  },
  {
    "objectID": "05_grammar_of_graphics.html#other-features-1",
    "href": "05_grammar_of_graphics.html#other-features-1",
    "title": "The grammar of graphics (ggplot2)",
    "section": "2.5 Other features",
    "text": "2.5 Other features\n\nStraightlines Segments Text \n\n\ngeom_vline and geom_hline draw vertical and horizontal lines respectively. These are particularly useful for:\n\nmarking cross-hairs in ordination plots\nproviding extended guides along either axes.\n\ngeom_abline is used for adding linear regression lines in the form of y = bx + a where b is the slope and a is the intercept (this is where the ab in abline comes from).\n\n\nShow attributes\n\n\n\n\n\n\n\n\n\n\nParameter\ngeom_vline\ngeom_hline\ngeom_abline\n\n\n\n\naesthetics\n\n\n\n\n\nxintercept - x-axis coordinate crossed by vline\n✔\n\n\n\n\nyintercept - y-axis coordinate crossed by hline\n\n✔\n\n\n\nintercept - y-axis coordinate crossed by abline\n\n\n✔\n\n\nslope - slope (gradient) of abline\n\n\n✔\n\n\ngroup - plot separate series without aesthetic differences\n✔\n\n\n\n\nalpha - transparency\n✔\n\n\n\n\ncolour - colour of the points/lines\n✔\n\n\n\n\nfill - inner colour of points/shapes\n✔\n\n\n\n\nlinetype - type of lines used to construct points/lines\n✔\n\n\n\n\nsize - thickness of the line\n✔\n\n\n\n\nweight - weightings of values\n✔\n\n\n\n\n\n\n\n\n\n\nadditional parameters\n\n\n\n\n\n\n\n\n\n\n\nComputed variables\n\n\n\n\n\n\n\n\n\n\n\nFeature\n\n\ngeom\n\n\nstat\n\n\nposition\n\n\nAesthetic parameters / Notes\n\n\nExample plot\n\n\n\n\n\n\nstraight lines\n\n\n_vline\n\n\n_identity\n\n\nidentity\n\n\nx,y\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(filter(CO2, Plant == first(Plant))) +\n    geom_vline(xintercept = 500, linetype = 'dashed') +\n    geom_point(aes(x = conc, y = uptake)) \n\n\n\n\n\nstraight lines\n\n\n_hline\n\n\n_identity\n\n\nidentity\n\n\nx,y\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(filter(CO2, Plant == first(Plant))) +\n    geom_hline(yintercept = 25, linetype = 'dashed') +\n    geom_point(aes(x = conc, y = uptake)) \n\n\n\n\n\nstraight lines\n\n\n_abline\n\n\n_identity\n\n\nidentity\n\n\nx,y\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(filter(CO2, Plant == first(Plant))) +\n    geom_abline(slope = 0.01, intercept = 30, linetype = 'dashed') +\n    geom_point(aes(x = conc, y = uptake)) \n\n\n\n\n\n\n\ngeom_segment draws segments (separate lines) joining pairs of coordinates. These can be useful to represent vectors (e.g. wind speed and direction on a map) or movement, differences etc (particularly if given an arrow head).\n\n\nShow attributes\n\n\n\n\n\n\n\n\nParameter\ngeom_segment\n\n\n\n\naesthetics\n\n\n\nx - x-axis coordinates for the start of lines\n✔\n\n\ny - y-axis coordinates for the start of lines\n✔\n\n\nxend - x-axis coordinates for the end of lines\n✔\n\n\nyend - y-axis coordinates for the end of lines\n✔\n\n\ngroup - plot separate series without aesthetic differences\n✔\n\n\nalpha - transparency\n✔\n\n\ncolour - colour of the points/lines\n✔\n\n\nfill - inner colour of points/shapes\n✔\n\n\nlinetype - type of lines used to construct points/lines\n✔\n\n\nsize - thickness of the line\n✔\n\n\nweight - weightings of values\n✔\n\n\n\n\n\n\nadditional parameters\n\n\n\narrow - specification of arrow heads\nNULL\n\n\narrow.fill - fill colour of arrow head\nNULL\n\n\nlineend - style of line end\n“butt”\n\n\nlinejoin - style of line join\n“round”\n\n\n\n\n\n\nComputed variables\n\n\n\n\n\n\n\n\n\nFeature\n\n\ngeom\n\n\nstat\n\n\nposition\n\n\nAesthetic parameters / Notes\n\n\nExample plot\n\n\n\n\n\n\nline segements\n\n\n_segement\n\n\n_identity\n\n\nidentity\n\n\nx,y\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBOD.lm &lt;- lm(demand ~ Time, data = BOD)\nBOD$fitted &lt;- fitted(BOD.lm)\nBOD$resid &lt;- resid(BOD.lm)\nggplot(BOD)+\n    geom_segment(aes(x=Time,y=demand, xend=Time,yend=fitted),\n                 arrow = arrow(length=unit(0.5, \"cm\"), ends = \"first\")) \n\n\n\n\n\n\n\nsee text",
    "crumbs": [
      "R basics",
      "The grammar of graphics"
    ]
  },
  {
    "objectID": "05_grammar_of_graphics.html#types-of-coordinate-systems",
    "href": "05_grammar_of_graphics.html#types-of-coordinate-systems",
    "title": "The grammar of graphics (ggplot2)",
    "section": "3.1 Types of coordinate systems",
    "text": "3.1 Types of coordinate systems\n\nCartesiancoordinates Polarcoordinates Flipcoordinates Fixedcoordinates Transformedcoordinates \n\n\n\n\nShow attributes\n\n\n\n\n\n\n\n\nParameter\ngeom_segment\n\n\n\n\nxlim - limits for the x axis\nNULL\n\n\nylim - limits for the y axis\nNULL\n\n\nexpand - whether to add a small expansion to the axes to ensure geoms and axes do not overlapp\nTRUE\n\n\nexpand - whether or not to provide a warning when the default coordinate system is being replaced\nFALSE\n\n\nclip - whether or not to clip plotting to within the plotting margins (“on”) or to extend into the margins\n“on”\n\n\n\n\n\n\n\n\nFeature\n\n\ncoord\n\n\nNotes\n\n\nExample plot\n\n\n\n\n\n\nCartesian coordinates\n\n\n_cartesian\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = BOD, aes(x = Time, y = demand)) +\n    geom_line() +\n    coord_cartesian()\n\n\n\n\n\n\n\n\n\nShow attributes\n\n\n\n\n\n\n\n\nParameter\ngeom_segment\n\n\n\n\ntheta - map angle to either ‘x’ or ‘y’\n‘x’\n\n\nstart - offset (applied clockwise by default) from 12 o’clock in radians\n0\n\n\ndirection - which direction (‘clockwise’: 1 or ‘anticlockwise’: -1)\n1\n\n\nclip - whether or not to clip plotting to within the plotting margins (“on”) or to extend into the margins\n“on”\n\n\n\n\n\n\n\n\nFeature\n\n\ncoord\n\n\nNotes\n\n\nExample plot\n\n\n\n\n\n\nPolar coordinates\n\n\n_polar\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = BOD, aes(x = Time, y = demand)) +\n    geom_line() +\n    coord_polar()\n\n\n\n\n\n\n\n\n\nShow attributes\n\n\n\n\n\n\n\n\nParameter\ngeom_segment\n\n\n\n\nxlim - limits for the x axis\nNULL\n\n\nylim - limits for the y axis\nNULL\n\n\nexpand - whether to add a small expansion to the axes to ensure geoms and axes do not overlapp\nTRUE\n\n\nexpand - whether or not to provide a warning when the default coordinate system is being replaced\nFALSE\n\n\nclip - whether or not to clip plotting to within the plotting margins (“on”) or to extend into the margins\n“on”\n\n\n\n\n\n\n\n\nFeature\n\n\ncoord\n\n\nNotes\n\n\nExample plot\n\n\n\n\n\n\nFlip coordinates\n\n\n_flip\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = BOD, aes(x = Time, y = demand)) +\n    geom_line() +\n    coord_flip()\n\n\n\n\n\n\n\n\n\nShow attributes\n\n\n\n\n\n\n\n\nParameter\ngeom_segment\n\n\n\n\nratio - aspect ratio (y/x)\n1\n\n\nxlim - limits for the x axis\nNULL\n\n\nylim - limits for the y axis\nNULL\n\n\nexpand - whether to add a small expansion to the axes to ensure geoms and axes do not overlapp\nTRUE\n\n\nexpand - whether or not to provide a warning when the default coordinate system is being replaced\nFALSE\n\n\nclip - whether or not to clip plotting to within the plotting margins (“on”) or to extend into the margins\n“on”\n\n\n\n\n\n\n\n\nFeature\n\n\ncoord\n\n\nNotes\n\n\nExample plot\n\n\n\n\n\n\nFixed coordinates\n\n\n_fixed\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = BOD, aes(x = Time, y = demand)) +\n    geom_line() +\n    coord_fixed()\n\n\n\n\n\nFixed ratio of coordinates\n\n\n_fixed\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = BOD, aes(x = Time, y = demand)) +\n    geom_line() +\n    coord_fixed(ratio = 0.5)\n\n\n\n\n\n\n\n\n\nShow attributes\n\n\n\n\n\n\n\n\nParameter\ngeom_segment\n\n\n\n\nx - the transformation to apply to the x-axis\n“identity”\n\n\ny - the transformation to apply to the y-axis\n“identity”\n\n\nxlim - limits for the x axis\nNULL\n\n\nylim - limits for the y axis\nNULL\n\n\nexpand - whether or not to provide a warning when the default coordinate system is being replaced\nFALSE\n\n\nclip - whether or not to clip plotting to within the plotting margins (“on”) or to extend into the margins\n“on”\n\n\n\n\n\n\n\n\nFeature\n\n\ncoord\n\n\nNotes\n\n\nExample plot\n\n\n\n\n\n\nTransformed coordinates\n\n\n_trans\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = BOD, aes(x = Time, y = demand)) +\n    geom_line() +\n    coord_trans(x = \"log10\")\n\n\n\n\n\nTrans ratio of coordinates\n\n\n_trans\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(data = BOD, aes(x = Time, y = demand)) +\n    geom_line() +\n    coord_trans(x = scales::exp_trans(2))",
    "crumbs": [
      "R basics",
      "The grammar of graphics"
    ]
  },
  {
    "objectID": "05_grammar_of_graphics.html#altering-axes-scales-via-the-coordinate-system",
    "href": "05_grammar_of_graphics.html#altering-axes-scales-via-the-coordinate-system",
    "title": "The grammar of graphics (ggplot2)",
    "section": "3.2 Altering axes scales via the coordinate system",
    "text": "3.2 Altering axes scales via the coordinate system\n\n3.2.1 Zooming\nModifying scales with coords_ affects the zoom on the graph. That is, it defines the extent and nature of the axes coordinates. By contrast, altering limits via scale_ routines will alter the scope of data included in a manner analogous to operating on a subset of the data.\nTo illustrate this, lets produce a linear smoother for the BOD data. To help us appreciate the differences between coords_ and scale_ when altering one axis (x in this case), we will ensure that all each plot has the same range of the other axis (y) and that the aspect ratio for the axes are all the same.\n\ng &lt;- ggplot(data = BOD, aes(x = Time, y = demand)) +\n    geom_smooth(method = \"lm\") +\n    geom_point()\n\n\n\nShow default plot\n\n\ng + coord_fixed(ratio = 0.1, ylim = c(-5, 35))\n\n\n\n\n\n\n\n\n\n\n\n\n\nScale via coords scale\n\ng + coord_fixed(ratio = 0.1,\n                ylim = c(-5, 35),\n                xlim = c(2, 6))\n\n\n\n\n\n\n\n\n\n\n\n\nScale via scale scale\n\ng + coord_fixed(ratio = 0.1,\n                ylim = c(-5, 35)) +\n    scale_x_continuous(limits = c(2, 6))\n\n\n\n\n\n\n\n\n\n\n\n\nNotice that the left hand figure (that restricts the x-axis scale) simply zooms in on the plot thereby cutting some of the data off. By contrast, scale_ (right hand figure) removes data that are outside the range and thus also alters the output of the smoother (linear model).\n\n\n3.2.2 Geom re-scaling\nIn addition to altering the zoom of the axes, axes (coordinate system) scales can be transformed to other scales via the coord_trans function. Such transformations of the coordinate system take place after statistics have been calculated and geoms derived. Therefore the shape of geoms are altered.\nTo illustrate the difference between coord_trans and scale_, lets create a fabricated data set of 50 points in which y is an exponential function of x (with noise).\n\nset.seed(1)\nn&lt;-50\ndat &lt;- data.frame(x = exp((1:n+rnorm(n,sd=2))/10), y = 1:n+rnorm(n,sd=2))\n\ng &lt;- ggplot(data = dat, aes(x = x, y = y))\n\n\n\n\n\nLinear scales\n\n\ncoord_trans\n\n\nscales_\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ng + geom_point()\n\n\n\n\n\n\n\n\n\n\n\n\n\ng + geom_point() +\n    coord_trans(x=\"log10\")\n\n\n\n\n\n\n\n\n\n\n\n\n\ng + geom_point() +\n    scale_x_continuous(trans=\"log10\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ng + geom_point() +\n    geom_smooth(method=\"lm\")\n\n\n\n\n\n\n\n\n\n\n\n\n\ng + geom_point() +\n    geom_smooth(method=\"lm\") +\n    coord_trans(x=\"log10\") \n\n\n\n\n\n\n\n\n\n\n\n\n\ng + geom_point() +\n    geom_smooth(method=\"lm\") +\n    scale_x_continuous(trans=\"log10\") \n\n\n\n\n\nIn the above, the log10 transformer function was applied to either the coordinates or the axes scales. More information about this and other transformer functions is provided in the scales section.",
    "crumbs": [
      "R basics",
      "The grammar of graphics"
    ]
  },
  {
    "objectID": "10_git.html",
    "href": "10_git.html",
    "title": "Git and version control",
    "section": "",
    "text": "Other useful tutorials or resources\n\nhttps://git-scm.com/book/en/v2\nhttps://www.atlassian.com/git/tutorials\nhttps://marklodato.github.io/visual-git-guide/index-en.html\nhttps://git-scm.com/docs/gittutorial\nhttps://marklodato.github.io/visual-git-guide/index-en.html\nhttps://try.github.io/levels/1/challenges/1\nhttps://onlywei.github.io/explain-git-with-d3/\nhttp://git-school.github.io/visualizing-git/\nhttps://github.com/sensorflo/git-draw\nThis tutorial will take a modular approach. The first section will provide an overview of the basic concepts of git. The second section will provide a quick overview of basic usage and the third and final section will cover intermediate level usage. In an attempt to ease understanding, the tutorial will blend together git commands and output, schematic diagrams and commentary in an attempt to ease understanding.\nThe following table surves as both a key and overview of the most common actions and git ‘verbs’.",
    "crumbs": [
      "Reproducible research",
      "Introduction to git"
    ]
  },
  {
    "objectID": "index.html#bash-shell-code",
    "href": "index.html#bash-shell-code",
    "title": "ReefCloud R/Statistics resources",
    "section": "2.2 Bash (shell) code",
    "text": "2.2 Bash (shell) code\nBash is a command-line shell that gives you access and control over your computer’s operating system. Whilst the majority of the tutorials in this series are focused on R, there are a couple of tutorials on other supporting topics (like version control) that might be interfaced via a shell (commands entered on a terminal). Furthermore, some software installation and/or configuration instructions might be in the form of shell commands.\nSuch commands are presented as:\n\npwd\n\n/home/runner/work/workshops/workshops/tut",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "10_git.html#initialize-local-repository",
    "href": "10_git.html#initialize-local-repository",
    "title": "Git and version control",
    "section": "5.1 Initialize local repository",
    "text": "5.1 Initialize local repository\n\n\nTerminalRStudioEmacs (magit)\n\n\nWe will start by creating a new directory (folder) which we will call Repo1 in which to place our repository. All usual directory naming rules apply since it is just a regular directory.\n\nmkdir ~/tmp/Repo1\n\nTo create (or initialize) a new local repository, issue the git init command in the root of the working directory you wish to contain the git repository. This can be either an empty directory or contain an existing directory/file structure. The git init command will add a folder called .git to the directory. This is a one time operation.\n\ncd ~/tmp/Repo1\ngit init \n\nInitialized empty Git repository in /home/runner/tmp/Repo1/.git/\n\n\nThe .git folder contains all the necessary metadata to manage the repository.\n\nls -al\n\ntotal 12\ndrwxr-xr-x 3 runner docker 4096 Mar 28 03:37 .\ndrwxr-xr-x 3 runner docker 4096 Mar 28 03:37 ..\ndrwxr-xr-x 7 runner docker 4096 Mar 28 03:37 .git\n\n\n\ntree -a --charset unicode\n\n.\n`-- .git\n    |-- HEAD\n    |-- branches\n    |-- config\n    |-- description\n    |-- hooks\n    |   |-- applypatch-msg.sample\n    |   |-- commit-msg.sample\n    |   |-- fsmonitor-watchman.sample\n    |   |-- post-update.sample\n    |   |-- pre-applypatch.sample\n    |   |-- pre-commit.sample\n    |   |-- pre-merge-commit.sample\n    |   |-- pre-push.sample\n    |   |-- pre-rebase.sample\n    |   |-- pre-receive.sample\n    |   |-- prepare-commit-msg.sample\n    |   |-- push-to-checkout.sample\n    |   |-- sendemail-validate.sample\n    |   `-- update.sample\n    |-- info\n    |   `-- exclude\n    |-- objects\n    |   |-- info\n    |   `-- pack\n    `-- refs\n        |-- heads\n        `-- tags\n\n10 directories, 18 files\n\n\n\nconfig\n\nthis file stores settings such as the location of a remote repository that this repository is linked to.\n\ndescription\n\nlists the name (and version) of a repository\n\nHEAD\n\nlists a reference to the current checked out commit.\n\nhooks\n\na directory containing scripts that are executed at various stages (e.g. pre-push.sample is an example of a script executed prior to pushing)\n\ninfo\n\ncontains a file exclude that lists exclusions (files not to be tracked). This is like .gitignore, except is not versioned.\n\nobjects\n\nthis directory contains SHA indexed files being tracked\n\nrefs\n\na master copy of all the repository refs\n\nlogs\n\ncontains a history of each branch\n\n\n\n\nThe repository that we are going to create in this demonstration could be considered to be a new standalone analysis. In Rstudio, this would be considered a project. So, we will initialise the git repository while we create a new Rstudio project. To do so:\n\nclick on the Project selector in the top right of the Rstudio window (as highlighted by the red ellipse in the image below.\n\nselect New Project from the dropdown menu\nselect New Directory form the Create Project panel\nselect New Project from the Project Type panel\nProvide a name for the new directory to be created and use the Browse button to locate a suitable position for this new directory. Ensure that the Create a git repository checkbox is checked\n\nClick the Create Project button\n\nIf successful, you should notice a couple of changes - these are highlighted in the following figure:\n\n\na new Git tab will appear in the top right panel\nthe contents of this newly created project/repository will appear in the Files tab of the bottom right panel\n\nIf the files and directories that begin with a . do not appear, click on the More file commands cog and make sure the Show Hidden Files option is ticked.\nThe newly created files/folders are:\n\n.git - this directory houses the repository information and should not generally be edited directly\n.gitignore - this file defines files/folders to be excluded from the repository. We will discuss this file more later\n.Rhistory - this file will accrue a history of the commands you have evaluated in R within this project\n.Rproj.user - this folder stores some project-specific temporary files\nRepo1.Rproj - contains the project specific settings\n\nNote that on the left side of the Rstudio window there are two panels - one called “Console”, the other called “Terminal”. The console window is for issuing R commands and the terminal window is for issuing system (bash, shell) commands. Throughout this tutorial, as an alternative to using the point and click Rstudio methods, you could instead issue the Terminal instructions into the “Terminal” panel. Indeed, there are some git commands that are not supported directly by Rstudio and can only be entered into the terminal\n\n\n\n\n\n\nNote, at this stage, no files are being tracked, that is, they are not part of the repository.\nTo assist in gaining a greater understanding of the workings of git, we will use a series of schematics diagrams representing the contents of four important sections of the repository. Typically, these figures will be contained within callout panels that expand/collapse upon clicking. However, for this first time, they will be standalone.\nIn the first figure below, the left hand panel represents the contents of the root directory (excluding the .git folder) - this is the workspace and is currently empty.\nThe three white panels represent three important parts of the inner structure of the .git folder. A newly initialized repository is relatively devoid of any specific metadata since there are no staged or committed files. In the root of the .git folder, there is a file called HEAD.\nThe figure is currently very sparse. However, as the repository grows, so the figure will become more complex.\n\n\n\n\n\n\n\n\n\nThe second figure provides the same information, yet via a network diagram. Again, this will not be overly meaningful until the repository contains some content.",
    "crumbs": [
      "Reproducible research",
      "Introduction to git"
    ]
  },
  {
    "objectID": "10_git.html#initializing-other-types-of-repositories",
    "href": "10_git.html#initializing-other-types-of-repositories",
    "title": "Git and version control",
    "section": "5.2 Initializing other types of repositories",
    "text": "5.2 Initializing other types of repositories\nThe above demonstrated how to initialise a new local repository from scratch. However, there are times when we instead want to:\n\ncreate a git repository from an existing directory or project\ncollaborate with someone on an existing repository\ncreate a remote repository\n\nThese situations are briefly demonstrated in the following sections.\n\n5.2.1 Initializing a shared (remote) repository\n\nThe main repository for sharing should not contain the working directory as such - only the .git tree and the .gitignore file. Typically the point of a remote repository is to act as a perminantly available repository from which multiple uses can exchange files. Consequently, those accessing this repository should only be able to interact with the .git metadata - they do not directly modify any files.\nSince a remote repository is devode of the working files and directories, it is referred to as bare.\n\nTerminalRstudio\n\n\nTo create a bare remote repository, issue the git init --bare command after logging in to the remote location.\n\ngit init --bare\n\n\n\nUse the instructions for the Terminal\n\n\n\n\n\n5.2.2 Cloning an existing repository\n\nTo get your own local copy of an existing repository, issue the git clone &lt;repo url&gt; command in the root of the working directory you wish to contain the git repository. The repo url points to the location of the existing repository to be cloned. This is also a one time operation and should be issued in an otherwise empty directory.\nThe repo url can be located on any accessible filesytem (local or remote). The cloning process also stores a link back to the original location of the repository (called origin). This provides a convenient way for the system to keep track of where the local repository should exchange files.\nMany git repositories are hosted on sites such as github, gitlab or bitbucket. Within an online git repository, these sites provide url links for cloning.\n\nTerminalRstudio\n\n\n\ngit clone \"url.git\"\n\nwhere \"url.git\" is the url of the hosted repository.\n\n\n\nclick on the Project selector in the top right of the Rstudio window (as highlighted by the red ellipse in the image below.\nselect New Project from the dropdown menu\nselect Version Control form the Create Project panel\nselect Git from the Create Project from Version Control panel\npaste in the address of the repository that you want to clone, optionally a name for this repository (if you do not like the original name) and use the Browse button to locate a suitable position for this new directory.\nClick the Create Project button\n\n\n\n\n\n\n5.2.3 Initializing a repository in an existing directory\n\nTerminalRstudio\n\n\nThis is the same as for a new directory.\n\ngit init\n\n\n\n\nclick on the Project selector in the top right of the Rstudio window (as highlighted by the red ellipse in the image below.\nselect New Project from the dropdown menu\nselect Existing Directory form the Create Project panel\nuse the Browse button to locate the existing directory\nClick the Create Project button",
    "crumbs": [
      "Reproducible research",
      "Introduction to git"
    ]
  },
  {
    "objectID": "10_git.html#staging-files",
    "href": "10_git.html#staging-files",
    "title": "Git and version control",
    "section": "6.1 Staging files",
    "text": "6.1 Staging files\nWhen a file is first added to the staging area, a full copy of that file is added to the staging area (not just the file diffs as in other versioning systems).\n\nTerminalRstudio\n\n\nTo demonstrate lets create a file (a simple text file containing the string saying ‘File 1’) and add it to the staging area.\n\necho 'File 1' &gt; file1\n\nNow lets add this file to the staging area\n\ngit add file1\n\nTo see the status of the repository (that is, what files are being tracked), we issue the git status command\n\ngit status\n\nOn branch main\n\nNo commits yet\n\nChanges to be committed:\n  (use \"git rm --cached &lt;file&gt;...\" to unstage)\n    new file:   file1\n\n\nThis indicates that there is a single file (file1) in the staging area\n\n\nTo demonstrate lets create a file (a simple text file containing the string saying ‘File 1’) and add it to the staging area.\n\nClick the green “New File” button followed by the “Text File” option (or click the equivalent option from the “File” menu) \nType File 1 in the panel with the flashing cursor. This panel represents the contents of the yet to be named file that we are creating.\n\nClick the “Save” or “Save all” buttons (or select the equivalent items from the “File” menu) and name the file “file1”\nSwitch to the Git tab and you should notice a number of items (including the file we just created) in the panel. These are files that git is aware of, but not yet tracking. This panel acts as a status window. The yellow “?” symbol indicates that git considers these files “untracked”\n\nTo stage a file, click on the corresponding checkbox - the status symbol should change to a green “A” (for added)\n\n\n\n\n\nOur simple overview schematic represents the staging of file 1.\n\n\n\n\n\n\n\n\n\nA schematic of the internal working of git shows in .git/objects a blob has been created. This is a compressed version of file1. Its filename is a 40 digit SHA-1 checksum has representing the contents of the file1. To re-iterate, the blob name is a SHA-1 hash of the file contents (actually, the first two digits form a folder and the remaining 38 form the filename).\nWe can look at the contents of this blob using the git cat-file command. This command outputs the contents of a compressed object (blob, tree, commit) from either the objects name (or unique fraction thereof) or its tag (we will discuss tags later).\n\ngit cat-file blob 50fcd\n\nFile 1\n\n\nThe add (staging) process also created a index file. This file simply points to the blob that is part of the snapshot. The git internals schematic illustrates the internal changes in response to staging a file.\n\n\n\n\n\n\n\n\n\n\nAnother visual representation of the git",
    "crumbs": [
      "Reproducible research",
      "Introduction to git"
    ]
  },
  {
    "objectID": "10_git.html#commit-to-local-repository",
    "href": "10_git.html#commit-to-local-repository",
    "title": "Git and version control",
    "section": "6.2 Commit to local repository",
    "text": "6.2 Commit to local repository\n\n\nTerminalRstudio\n\n\nTo commit a set of changes from the staging area to the local repository, we issue the git commit command. We usually add the -m switch to explicitly supply a message to be associated with the commit. This message should ideally describe what the changes the commit introduces to the repository.\n\ngit commit -m 'Initial repo and added file1'\n\n[main (root-commit) 3e8fd80] Initial repo and added file1\n 1 file changed, 1 insertion(+)\n create mode 100644 file1\n\n\nWe now see that the status has changed. It indicates that the tree in the workspace is in sync with the repository.\n\ngit status\n\nOn branch main\nnothing to commit, working tree clean\n\n\n\n\nTo commit a set of changes from the staging area to the local repository:\n\nclick on the “Commit” button to open the “Review Changes” window\n\nThis box will list the files to be committed (in this case “file1”), the changes in this file since the previous commit (as this is the first time this file has been committed, the changes are the file contents)\nyou should also provide a commit message (in the figure above, I entered “Initial commit”. This message should ideally describe what the changes the commit introduces to the repository.\nclick the “Commit” button and you will be presented with a popup message.\n\nThis message provides feedback to confirm that your commit was successful.\n\nclose the popup window and the “Review Changes” window\n\nfile1 should now have disappeared from the git status panel.\n\n\n\nOur simple overview schematic represents the staging of file 1.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional details about the commit\n\n\n\n\n\nThe following modifications have occurred (in reverse order to how they actually occur):\n\nThe main branch reference was created. There is currently only a single branch (more on branches later). The branch reference point to (indicates) which commit is the current commit within a branch.\n\ncat .git/refs/heads/main\n\n3e8fd80ba671c95c5c8e9f306f45f0cd1b1a2749\n\n\nA commit was created. This points to a tree (which itself points to the blob representing file1) as well as other important metadata (such as who made the commit and when). Since the time stamp will be unique each time a snapshot is commited, so too the name of the commit (as a SHA-1 checksum hash) will differ. To reiterate, the names of blobs and trees are determined by contents alone, commit names are also incorporate commit timestamp and details of the committer - and are thus virtually unique.\n\ngit cat-file commit 3e8fd\n\ntree 07a941b332d756f9a8acc9fdaf58aab5c7a43f64\nauthor pcinereus &lt;i.obesulus@gmail.com&gt; 1711597057 +0000\ncommitter pcinereus &lt;i.obesulus@gmail.com&gt; 1711597057 +0000\n\nInitial repo and added file1\n\n\nA tree object was created. This represents the directory tree of the snapshot (commit) and thus points to the blobs.\n\ngit ls-tree 3e8fd\n\n100644 blob 50fcd26d6ce3000f9d5f12904e80eccdc5685dd1  file1\n\n\nOr most commonly (if interested in the latest commit):\n\ngit ls-tree HEAD\n\n100644 blob 50fcd26d6ce3000f9d5f12904e80eccdc5685dd1  file1\n\n\n\n\n\n\nThe schematic now looks like\n\n\n\n\n\n\n\n\n\n\nAnother visual representation of the git\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMore information about committing changes to the repository\n\n\n\n\n\nCommitting staged changes creates an object under the .git tree.\n\ntree -a --charset unicode\n\n.\n|-- .git\n|   |-- COMMIT_EDITMSG\n|   |-- HEAD\n|   |-- branches\n|   |-- config\n|   |-- description\n|   |-- hooks\n|   |   |-- applypatch-msg.sample\n|   |   |-- commit-msg.sample\n|   |   |-- fsmonitor-watchman.sample\n|   |   |-- post-update.sample\n|   |   |-- pre-applypatch.sample\n|   |   |-- pre-commit.sample\n|   |   |-- pre-merge-commit.sample\n|   |   |-- pre-push.sample\n|   |   |-- pre-rebase.sample\n|   |   |-- pre-receive.sample\n|   |   |-- prepare-commit-msg.sample\n|   |   |-- push-to-checkout.sample\n|   |   |-- sendemail-validate.sample\n|   |   `-- update.sample\n|   |-- index\n|   |-- info\n|   |   `-- exclude\n|   |-- logs\n|   |   |-- HEAD\n|   |   `-- refs\n|   |       `-- heads\n|   |           `-- main\n|   |-- objects\n|   |   |-- 07\n|   |   |   `-- a941b332d756f9a8acc9fdaf58aab5c7a43f64\n|   |   |-- 3e\n|   |   |   `-- 8fd80ba671c95c5c8e9f306f45f0cd1b1a2749\n|   |   |-- 50\n|   |   |   `-- fcd26d6ce3000f9d5f12904e80eccdc5685dd1\n|   |   |-- info\n|   |   `-- pack\n|   `-- refs\n|       |-- heads\n|       |   `-- main\n|       `-- tags\n`-- file1\n\n16 directories, 27 files\n\n\n\ngit cat-file -p HEAD\n\ntree 07a941b332d756f9a8acc9fdaf58aab5c7a43f64\nauthor pcinereus &lt;i.obesulus@gmail.com&gt; 1711597057 +0000\ncommitter pcinereus &lt;i.obesulus@gmail.com&gt; 1711597057 +0000\n\nInitial repo and added file1\n\n\n\ngit cat-file -p HEAD^{tree}\n\n100644 blob 50fcd26d6ce3000f9d5f12904e80eccdc5685dd1    file1\n\n\n\ngit log --oneline\n\n3e8fd80 Initial repo and added file1",
    "crumbs": [
      "Reproducible research",
      "Introduction to git"
    ]
  },
  {
    "objectID": "10_git.html#more-changes",
    "href": "10_git.html#more-changes",
    "title": "Git and version control",
    "section": "6.3 More changes",
    "text": "6.3 More changes\n\nWhenever a file is added or modified, if the changes are to be tracked, the file needs to be added to the staging area. Lets demonstrate by modifying file1 and adding an additional file (file2), this time to a subdirectory (dir1).\n\nTerminalRstudio\n\n\n\necho '---------------' &gt;&gt; file1\nmkdir dir1\necho '* Notes' &gt; dir1/file2\ngit add file1 dir1/file2\n\nNow if we re-examine the status:\n\ngit status\n\nOn branch main\nChanges to be committed:\n  (use \"git restore --staged &lt;file&gt;...\" to unstage)\n    new file:   dir1/file2\n    modified:   file1\n\n\n\n\n\nmodify file1 by adding a number of hyphens under the File 1 like in the figure below\n\nsave the file. As you do so, you should notice that the file reappears in the status panel (this time with a blue “M” to signify that the file has been modified)\nto create the subdirectory, click on the “Add a new folder” icon and then enter a name for the subdirectory in the popup box (as per figure below)\n\nnavigate to this new directory (dir1)\nclick the “Create a new blank file in current directory” button and select “Text file”\nenter a new filename (file2) into the popup box\nenter some text into this file (like in the figure below)\n\nsave the file and notice that the dir1 directory is now also in the git status panel (yet its status is “untracked”)\nstage both file1 and dir1 (click on the corresponding checkboxes)\n\n\n\n\n\nAnd now our schematic looks like:\n\n\n\n\n\n\n\n\n\n\nAnother visual representation of the git\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMore information about staging changes to the repository\n\n\n\n\n\nSo when staging, the following has been performed:\n\nthe index file has been updated\n\ngit ls-files --stage\n\n100644 4fcc8f85f738deb6cbb17db1ed3da241ad6cdf39 0 dir1/file2\n100644 28ed2456cbfa8a18a280c8af5b422e91e88ff64d 0 file1\n\n\ntwo new blobs have been generated. One representing the modified file1 and the other representing the newly created file2 in the dir1 folder. The blob that represented the original file1 contents is still present and indeed is still the one currently committed. Blobs are not erased or modified.\n\n\n\n\nNow we will commit this snapshot.\n\nTerminalRstudio\n\n\n\ngit commit -m 'Modified file1 and added file2 (in dir1)'\n\n[main aa930b4] Modified file1 and added file2 (in dir1)\n 2 files changed, 2 insertions(+)\n create mode 100644 dir1/file2\n\n\n\n\n\nclick the “Commit” button\nyou might like to explore the changes associated with each file\nenter a commit message (as in the figure below)\n\nclick the “Commit” button\nafter checking that the “Git Commit” popup does not contain any errors, close the popup\nto explore the repository history, click towards the “History” button on the top left corner of the “Review Changes” window\n\nThis provides a graphical list of commits (in reverse chronological order)\nonce you have finished exploring the history, you can close the “Review Changes” window\n\n\n\n\n\n\n\n\n\n\nMore information about changes to the repository\n\n\n\n\n\nThe following modifications occur:\n\nthe master branch now points to the new commit.\n\ncat .git/refs/heads/main\n\naa930b4feb68e9ba069153dd25e9fe0342bd88af\n\n\n\ngit reflog\n\naa930b4 HEAD@{0}: commit: Modified file1 and added file2 (in dir1)\n3e8fd80 HEAD@{1}: commit (initial): Initial repo and added file1\n\n\na new commit was created. This points to a new root tree object and also points to the previous commit (its parent).\n\ngit cat-file commit aa930\n\ntree 2b61e2b3db9d1708269cf9d1aeaae2b0a2af1a23\nparent 3e8fd80ba671c95c5c8e9f306f45f0cd1b1a2749\nauthor pcinereus &lt;i.obesulus@gmail.com&gt; 1711597063 +0000\ncommitter pcinereus &lt;i.obesulus@gmail.com&gt; 1711597063 +0000\n\nModified file1 and added file2 (in dir1)\n\n\nnew root tree was created. This points to a blob representing the modified file1 as well as a newly created sub-directory tree representing the dir1 folder.\n\ngit ls-tree 2b61e\n\n040000 tree f2fa54609fe5e918f365e0d5ffaf9a3aea88d541  dir1\n100644 blob 28ed2456cbfa8a18a280c8af5b422e91e88ff64d  file1\n\n\n\ngit cat-file -p HEAD^{tree}\n\n040000 tree f2fa54609fe5e918f365e0d5ffaf9a3aea88d541  dir1\n100644 blob 28ed2456cbfa8a18a280c8af5b422e91e88ff64d  file1\n\n\na new sub-directory root tree was created. This points to a blob representing the modified file1 as well as a newly created subtree tree representing the file2 file within the dir1 folder.\n\ngit ls-tree aa930\n\n040000 tree f2fa54609fe5e918f365e0d5ffaf9a3aea88d541  dir1\n100644 blob 28ed2456cbfa8a18a280c8af5b422e91e88ff64d  file1\n\n\nOR,\n\ngit ls-tree HEAD\n\n040000 tree f2fa54609fe5e918f365e0d5ffaf9a3aea88d541  dir1\n100644 blob 28ed2456cbfa8a18a280c8af5b422e91e88ff64d  file1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnother visual representation of the git\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMore information about committing changes to the repository\n\n\n\n\n\nCommitting staged changes creates an object under the .git tree.\n\ntree -a --charset unicode\n\n.\n|-- .git\n|   |-- COMMIT_EDITMSG\n|   |-- HEAD\n|   |-- branches\n|   |-- config\n|   |-- description\n|   |-- hooks\n|   |   |-- applypatch-msg.sample\n|   |   |-- commit-msg.sample\n|   |   |-- fsmonitor-watchman.sample\n|   |   |-- post-update.sample\n|   |   |-- pre-applypatch.sample\n|   |   |-- pre-commit.sample\n|   |   |-- pre-merge-commit.sample\n|   |   |-- pre-push.sample\n|   |   |-- pre-rebase.sample\n|   |   |-- pre-receive.sample\n|   |   |-- prepare-commit-msg.sample\n|   |   |-- push-to-checkout.sample\n|   |   |-- sendemail-validate.sample\n|   |   `-- update.sample\n|   |-- index\n|   |-- info\n|   |   `-- exclude\n|   |-- logs\n|   |   |-- HEAD\n|   |   `-- refs\n|   |       `-- heads\n|   |           `-- main\n|   |-- objects\n|   |   |-- 07\n|   |   |   `-- a941b332d756f9a8acc9fdaf58aab5c7a43f64\n|   |   |-- 28\n|   |   |   `-- ed2456cbfa8a18a280c8af5b422e91e88ff64d\n|   |   |-- 2b\n|   |   |   `-- 61e2b3db9d1708269cf9d1aeaae2b0a2af1a23\n|   |   |-- 3e\n|   |   |   `-- 8fd80ba671c95c5c8e9f306f45f0cd1b1a2749\n|   |   |-- 4f\n|   |   |   `-- cc8f85f738deb6cbb17db1ed3da241ad6cdf39\n|   |   |-- 50\n|   |   |   `-- fcd26d6ce3000f9d5f12904e80eccdc5685dd1\n|   |   |-- aa\n|   |   |   `-- 930b4feb68e9ba069153dd25e9fe0342bd88af\n|   |   |-- f2\n|   |   |   `-- fa54609fe5e918f365e0d5ffaf9a3aea88d541\n|   |   |-- info\n|   |   `-- pack\n|   `-- refs\n|       |-- heads\n|       |   `-- main\n|       `-- tags\n|-- dir1\n|   `-- file2\n`-- file1\n\n22 directories, 33 files\n\n\n\ngit cat-file -p HEAD\n\ntree 2b61e2b3db9d1708269cf9d1aeaae2b0a2af1a23\nparent 3e8fd80ba671c95c5c8e9f306f45f0cd1b1a2749\nauthor pcinereus &lt;i.obesulus@gmail.com&gt; 1711597063 +0000\ncommitter pcinereus &lt;i.obesulus@gmail.com&gt; 1711597063 +0000\n\nModified file1 and added file2 (in dir1)\n\n\n\ngit cat-file -p HEAD^{tree}\n\n040000 tree f2fa54609fe5e918f365e0d5ffaf9a3aea88d541    dir1\n100644 blob 28ed2456cbfa8a18a280c8af5b422e91e88ff64d    file1\n\n\n\ngit log --oneline\n\naa930b4 Modified file1 and added file2 (in dir1)\n3e8fd80 Initial repo and added file1\n\n\n\n\n\nNow you might be wondering… What if I have modified many files and I want to stage them all. Do I really have to add each file individually? Is there not some way to add multiple files at a time? The answer of course is yes. To stage all files (including those in subdirectories) we issue the git add . command (notice the dot).\n\ngit add .",
    "crumbs": [
      "Reproducible research",
      "Introduction to git"
    ]
  },
  {
    "objectID": "10_git.html#gitignore",
    "href": "10_git.html#gitignore",
    "title": "Git and version control",
    "section": "6.4 .gitignore",
    "text": "6.4 .gitignore\nWhilst it is convenient to not have to list every file that you want to be staged (added), what about files that we don’t want to get staged and committed. It is also possible to define a file (called .gitignore) that is a list of files (or file patterns) that are to be excluded when we request all files be added. This functionality is provided via the .gitignore file that must be in the root of the repository working directory.\nFor example, we may have temporary files or automatic backup files or files generated as intermediates in a compile process etc that get generated. These files are commonly generated in the process of working with files in a project, yet we do not necessarily wish for them to be tracked. Often these files have very predictable filename pattern (such as ending with a # or ~ symbol or having a specific file extension such as .aux.\nAs an example, when working with a project in Rstudio, files (such as .Rhistory) and directories (such as .Rproj.user) are automatically added to the file system and thus appear as untracked files in git status.\nHence, we can create a.gitignore to exclude these files/directories. Indeed, if you are using Rstudio, you might have noticed that a .gitignore file was automatically created when you created the project.\nLets start by modifying the file2 and creating a new file f.tmp (that we want to ignore).\n\nTerminalRstudio\n\n\n\necho '---' &gt;&gt; dir1/file2\necho 'temp' &gt; dir1/f.tmp\n\n\n\n\nnavigate to the dir1 directory and open file2 for editing (or just make sure you are on the file2 tab.\nedit the file such that it just contains three hyphens (---) before saving the file\nin the same dir1 directory add another new text file (f.tmp) and edit this file to contain the word temp (then save the file)\n\nThe Git status panel should display both of these as untracked files.\n\n\n\n\nTo ignore the f.tmp file, we could either explicitly add this file as a row in a .gitignore file, or else we could supply a wildcard version that will ignore all files ending in .tmp.\n\nTerminalRstudio\n\n\n\necho '*.tmp' &gt; .gitignore\ncat .gitignore\n\n*.tmp\n\n\n\n\n\nnavigate back to the root of the project\nclick on the gitignore file to open it up for editing\nnavigate to the end of this file and add a newline containing the text *.tmp\n\n\nYou will notice that this .gitignore file already had items in it before you started editing it. These were added by Rstudio when you first created the new project.\nThe first item is .Rproj.user and its presence in this file is why it does not appear in the git status panel.\nOnce we save the .gitignore file, notice how the f.tmp file is similarly removed from the git status panel - since via .gitignore we have indicated that we want to ignore this file (not track it as part of our version control system).\n\n\n\n\n\n\n\n\n\nMore information about exclusions (.gitignore)\n\n\n\n\n\n\n\n\n\n\n\n\nEntry\nMeaning\n\n\n\n\nfile1\nDO NOT stage (add) file1\n\n\n*.tmp\nDO NOT stage (add) any file ending in .tmp\n\n\n/dir1/*\nDO NOT stage (add) the folder called dir1 (or any of its contents) unless this is specifically negated (see next line)\n\n\n!/dir1/file2\nDO stage (add) the file called file2 in the dir1 folder\n\n\n\n\n\n\nNow when we go to add all files to the staging area, those that fall under the exclude rules will be ignored\n\nTerminalRstudio\n\n\n\ngit add .\n\n\ngit status\n\nOn branch main\nChanges to be committed:\n  (use \"git restore --staged &lt;file&gt;...\" to unstage)\n    new file:   .gitignore\n    modified:   dir1/file2\n\n\nYou will notice that .gitignore was added as a new file and dir1/file2 was marked as modified yet dir1/f.tmp was totally ignored.\n\n\nYou will notice that .gitignore was added as a new file and dir1/file2 was marked as modified yet dir1/f.tmp was totally ignored.\n\ncheck the boxes next to each of the files listed in the status panel\n\n\n\n\n\nLets now commit these changes.\n\nTerminalRstudio\n\n\n\ngit commit -m 'Modified file2, added .gitignore'\n\n[main b165944] Modified file2, added .gitignore\n 2 files changed, 2 insertions(+)\n create mode 100644 .gitignore\n\n\n\ngit status\n\nOn branch main\nnothing to commit, working tree clean\n\n\n\n\n\nclick on the “Commit” button\nadd a commit message (such as Modified file2, added .gitignore)\nclick the “Commit” button\nclose the popup\nclose the “Review Changes” window\n\n\n\n\nFor those still interested in the schematic…\n\n\n\n\n\n\n\n\n\n\nAnother visual representation of the git\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMore information about committing changes to the repository\n\n\n\n\n\nCommitting staged changes creates an object under the .git tree.\n\ntree -a --charset unicode\n\n.\n|-- .git\n|   |-- COMMIT_EDITMSG\n|   |-- HEAD\n|   |-- branches\n|   |-- config\n|   |-- description\n|   |-- hooks\n|   |   |-- applypatch-msg.sample\n|   |   |-- commit-msg.sample\n|   |   |-- fsmonitor-watchman.sample\n|   |   |-- post-update.sample\n|   |   |-- pre-applypatch.sample\n|   |   |-- pre-commit.sample\n|   |   |-- pre-merge-commit.sample\n|   |   |-- pre-push.sample\n|   |   |-- pre-rebase.sample\n|   |   |-- pre-receive.sample\n|   |   |-- prepare-commit-msg.sample\n|   |   |-- push-to-checkout.sample\n|   |   |-- sendemail-validate.sample\n|   |   `-- update.sample\n|   |-- index\n|   |-- info\n|   |   `-- exclude\n|   |-- logs\n|   |   |-- HEAD\n|   |   `-- refs\n|   |       `-- heads\n|   |           `-- main\n|   |-- objects\n|   |   |-- 07\n|   |   |   `-- a941b332d756f9a8acc9fdaf58aab5c7a43f64\n|   |   |-- 14\n|   |   |   `-- 3a8bb5a2cc05a91f83a87af18c8eb5885a375c\n|   |   |-- 19\n|   |   |   `-- 44fd61e7c53bcc19e6f3eb94cc800508944a25\n|   |   |-- 28\n|   |   |   `-- ed2456cbfa8a18a280c8af5b422e91e88ff64d\n|   |   |-- 2b\n|   |   |   `-- 61e2b3db9d1708269cf9d1aeaae2b0a2af1a23\n|   |   |-- 3c\n|   |   |   `-- 7af0d3ccea71c9af82fa0ce68532272edcf1b8\n|   |   |-- 3e\n|   |   |   `-- 8fd80ba671c95c5c8e9f306f45f0cd1b1a2749\n|   |   |-- 4f\n|   |   |   `-- cc8f85f738deb6cbb17db1ed3da241ad6cdf39\n|   |   |-- 50\n|   |   |   `-- fcd26d6ce3000f9d5f12904e80eccdc5685dd1\n|   |   |-- aa\n|   |   |   `-- 930b4feb68e9ba069153dd25e9fe0342bd88af\n|   |   |-- b1\n|   |   |   `-- 659447a2ced4fb31ca13a9a7d95fe6b0630059\n|   |   |-- c4\n|   |   |   `-- 26a67af50d13828ec73b3c560b2648e2f3dc08\n|   |   |-- f2\n|   |   |   `-- fa54609fe5e918f365e0d5ffaf9a3aea88d541\n|   |   |-- info\n|   |   `-- pack\n|   `-- refs\n|       |-- heads\n|       |   `-- main\n|       `-- tags\n|-- .gitignore\n|-- dir1\n|   |-- f.tmp\n|   `-- file2\n`-- file1\n\n27 directories, 40 files\n\n\n\ngit cat-file -p HEAD\n\ntree 3c7af0d3ccea71c9af82fa0ce68532272edcf1b8\nparent aa930b4feb68e9ba069153dd25e9fe0342bd88af\nauthor pcinereus &lt;i.obesulus@gmail.com&gt; 1711597065 +0000\ncommitter pcinereus &lt;i.obesulus@gmail.com&gt; 1711597065 +0000\n\nModified file2, added .gitignore\n\n\n\ngit cat-file -p HEAD^{tree}\n\n100644 blob 1944fd61e7c53bcc19e6f3eb94cc800508944a25    .gitignore\n040000 tree c426a67af50d13828ec73b3c560b2648e2f3dc08    dir1\n100644 blob 28ed2456cbfa8a18a280c8af5b422e91e88ff64d    file1\n\n\n\ngit log --oneline\n\nb165944 Modified file2, added .gitignore\naa930b4 Modified file1 and added file2 (in dir1)\n3e8fd80 Initial repo and added file1",
    "crumbs": [
      "Reproducible research",
      "Introduction to git"
    ]
  },
  {
    "objectID": "10_git.html#status-of-workspace-and-staging-area",
    "href": "10_git.html#status-of-workspace-and-staging-area",
    "title": "Git and version control",
    "section": "7.1 Status of workspace and staging area",
    "text": "7.1 Status of workspace and staging area\nRecall that within the .git environment, files can be in one of four states:\n\nuntracked\nmodified\nstaged\ncommitted\n\nTo inspect the status of files in your workspace, you can issue the git status command (as we have done on numerous occasions above). This command displays the current state of the workspace and staging area.\n\nTerminalRstudio\n\n\n\ngit status\n\nOn branch main\nnothing to commit, working tree clean\n\n\nThe output of git status partitions all the files into (staged: Changes to be committed, unstaged: Changes not staged for commit and Untracked) as well as hints on how to either promote or demote the status of these files.\n\n\nExamine the git status panel - ideally it should be empty thereby signalling that all your important files are tracked andcommitted.\n\n\n\n\n7.1.1 log of commits\n\nTerminalRstudio\n\n\nThe git log command allows us to review the history of committed snapshots\n\ngit log\n\ncommit b1659447a2ced4fb31ca13a9a7d95fe6b0630059\nAuthor: pcinereus &lt;i.obesulus@gmail.com&gt;\nDate:   Thu Mar 28 03:37:45 2024 +0000\n\n    Modified file2, added .gitignore\n\ncommit aa930b4feb68e9ba069153dd25e9fe0342bd88af\nAuthor: pcinereus &lt;i.obesulus@gmail.com&gt;\nDate:   Thu Mar 28 03:37:43 2024 +0000\n\n    Modified file1 and added file2 (in dir1)\n\ncommit 3e8fd80ba671c95c5c8e9f306f45f0cd1b1a2749\nAuthor: pcinereus &lt;i.obesulus@gmail.com&gt;\nDate:   Thu Mar 28 03:37:37 2024 +0000\n\n    Initial repo and added file1\n\n\nWe can see that in my case some fool called ‘Murray Logan’ has made a total of three commits. We can also see the date/time that the commits were made as well as the supplied commit comment.\nOver time repositories accumulate a large number of commits, to only review the last 2 commits, we could issue the git log -n 2 command.\n\ngit log -n 2 \n\ncommit b1659447a2ced4fb31ca13a9a7d95fe6b0630059\nAuthor: pcinereus &lt;i.obesulus@gmail.com&gt;\nDate:   Thu Mar 28 03:37:45 2024 +0000\n\n    Modified file2, added .gitignore\n\ncommit aa930b4feb68e9ba069153dd25e9fe0342bd88af\nAuthor: pcinereus &lt;i.obesulus@gmail.com&gt;\nDate:   Thu Mar 28 03:37:43 2024 +0000\n\n    Modified file1 and added file2 (in dir1)\n\n\n\n\n\n\n\n\nAdditional useful (git log) options\n\n\n\n\n\n\n\n\nOption\n\n\nExample\n\n\n\n\n\n--onelineCondensed view\n\n\n\ngit log --oneline\n\nb165944 Modified file2, added .gitignore\naa930b4 Modified file1 and added file2 (in dir1)\n3e8fd80 Initial repo and added file1\n\n\n\n\n\n\n--statIndicates number of changes\n\n\n\ngit log --stat\n\ncommit b1659447a2ced4fb31ca13a9a7d95fe6b0630059\nAuthor: pcinereus &lt;i.obesulus@gmail.com&gt;\nDate:   Thu Mar 28 03:37:45 2024 +0000\n\n    Modified file2, added .gitignore\n\n .gitignore | 1 +\n dir1/file2 | 1 +\n 2 files changed, 2 insertions(+)\n\ncommit aa930b4feb68e9ba069153dd25e9fe0342bd88af\nAuthor: pcinereus &lt;i.obesulus@gmail.com&gt;\nDate:   Thu Mar 28 03:37:43 2024 +0000\n\n    Modified file1 and added file2 (in dir1)\n\n dir1/file2 | 1 +\n file1      | 1 +\n 2 files changed, 2 insertions(+)\n\ncommit 3e8fd80ba671c95c5c8e9f306f45f0cd1b1a2749\nAuthor: pcinereus &lt;i.obesulus@gmail.com&gt;\nDate:   Thu Mar 28 03:37:37 2024 +0000\n\n    Initial repo and added file1\n\n file1 | 1 +\n 1 file changed, 1 insertion(+)\n\n\n\n\n\n\n-pDisplays the full diff of each commit\n\n\n\ngit log -p\n\ncommit b1659447a2ced4fb31ca13a9a7d95fe6b0630059\nAuthor: pcinereus &lt;i.obesulus@gmail.com&gt;\nDate:   Thu Mar 28 03:37:45 2024 +0000\n\n    Modified file2, added .gitignore\n\ndiff --git a/.gitignore b/.gitignore\nnew file mode 100644\nindex 0000000..1944fd6\n--- /dev/null\n+++ b/.gitignore\n@@ -0,0 +1 @@\n+*.tmp\ndiff --git a/dir1/file2 b/dir1/file2\nindex 4fcc8f8..143a8bb 100644\n--- a/dir1/file2\n+++ b/dir1/file2\n@@ -1 +1,2 @@\n * Notes\n+---\n\ncommit aa930b4feb68e9ba069153dd25e9fe0342bd88af\nAuthor: pcinereus &lt;i.obesulus@gmail.com&gt;\nDate:   Thu Mar 28 03:37:43 2024 +0000\n\n    Modified file1 and added file2 (in dir1)\n\ndiff --git a/dir1/file2 b/dir1/file2\nnew file mode 100644\nindex 0000000..4fcc8f8\n--- /dev/null\n+++ b/dir1/file2\n@@ -0,0 +1 @@\n+* Notes\ndiff --git a/file1 b/file1\nindex 50fcd26..28ed245 100644\n--- a/file1\n+++ b/file1\n@@ -1 +1,2 @@\n File 1\n+---------------\n\ncommit 3e8fd80ba671c95c5c8e9f306f45f0cd1b1a2749\nAuthor: pcinereus &lt;i.obesulus@gmail.com&gt;\nDate:   Thu Mar 28 03:37:37 2024 +0000\n\n    Initial repo and added file1\n\ndiff --git a/file1 b/file1\nnew file mode 100644\nindex 0000000..50fcd26\n--- /dev/null\n+++ b/file1\n@@ -0,0 +1 @@\n+File 1\n\n\n\n\n\n\n--author=\"&lt;name&gt;\"Filter by author\n\n\n\ngit log --author=\"Murray\"\n\n\n\n\n\n--grep=\"&lt;pattern&gt;\"Filter by regex pattern of commit message\n\n\n\ngit log --grep=\"Modified\"\n\ncommit b1659447a2ced4fb31ca13a9a7d95fe6b0630059\nAuthor: pcinereus &lt;i.obesulus@gmail.com&gt;\nDate:   Thu Mar 28 03:37:45 2024 +0000\n\n    Modified file2, added .gitignore\n\ncommit aa930b4feb68e9ba069153dd25e9fe0342bd88af\nAuthor: pcinereus &lt;i.obesulus@gmail.com&gt;\nDate:   Thu Mar 28 03:37:43 2024 +0000\n\n    Modified file1 and added file2 (in dir1)\n\n\n\n\n\n\n&lt;file&gt;Filter by filename\n\n\n\ngit log file1\n\ncommit aa930b4feb68e9ba069153dd25e9fe0342bd88af\nAuthor: pcinereus &lt;i.obesulus@gmail.com&gt;\nDate:   Thu Mar 28 03:37:43 2024 +0000\n\n    Modified file1 and added file2 (in dir1)\n\ncommit 3e8fd80ba671c95c5c8e9f306f45f0cd1b1a2749\nAuthor: pcinereus &lt;i.obesulus@gmail.com&gt;\nDate:   Thu Mar 28 03:37:37 2024 +0000\n\n    Initial repo and added file1\n\n\n\n\n\n\n--decorate --graph\n\n\n\ngit log --graph --decorate --oneline\n\n* b165944 (HEAD -&gt; main) Modified file2, added .gitignore\n* aa930b4 Modified file1 and added file2 (in dir1)\n* 3e8fd80 Initial repo and added file1\n\n\n\n\n\n\n--allAll branches\n\n\n\ngit log --all\n\ncommit b1659447a2ced4fb31ca13a9a7d95fe6b0630059\nAuthor: pcinereus &lt;i.obesulus@gmail.com&gt;\nDate:   Thu Mar 28 03:37:45 2024 +0000\n\n    Modified file2, added .gitignore\n\ncommit aa930b4feb68e9ba069153dd25e9fe0342bd88af\nAuthor: pcinereus &lt;i.obesulus@gmail.com&gt;\nDate:   Thu Mar 28 03:37:43 2024 +0000\n\n    Modified file1 and added file2 (in dir1)\n\ncommit 3e8fd80ba671c95c5c8e9f306f45f0cd1b1a2749\nAuthor: pcinereus &lt;i.obesulus@gmail.com&gt;\nDate:   Thu Mar 28 03:37:37 2024 +0000\n\n    Initial repo and added file1\n\n\n\n\n\n\n\n\n\n\n\nTo explore the history of a repository, click on the clock icon (“View history of previous commits” button). This will open up the “Review Changes” window in the “History” tab.\n\nAlong with the reverse chronological list of commits, for each commit (and file thereof), you can explore the changes (diffs) that occurred.\nText that appears over a green background represents text that have been added as part of the current commit. Text that appears over a red background represents text that have been removed.\nIf we scroll down and explore the changes in dir1/file2 for the most recent commit, we see that the text * Notes was removed and then * Notes and --- were added. At first this might seem a bit odd - why was * Notes deleted and then added back?\n\nGit works on entire lines of text. So the first line was replaced because in the newer version, the first line had a carriage return (newline character). Although we cant see this character, it is there - we see it more via its effect (sending the text after it to the next line). Hence, in fact, two lines of text were actually changed in the most recent commit.\n\n\n\n\n\n7.1.2 reflog\n\nTerminalRstudio\n\n\nAnother way to explore the commit history is to look at the reflog. This is a log of the branch references. This approach is more useful when we have multiple branches and so will be visited in the section on branching. It displays all repository activity, not just the commits.\n\ngit reflog\n\nb165944 HEAD@{0}: commit: Modified file2, added .gitignore\naa930b4 HEAD@{1}: commit: Modified file1 and added file2 (in dir1)\n3e8fd80 HEAD@{2}: commit (initial): Initial repo and added file1\n\n\n\n\nSome of this sort of information can be gleaned from the git “History”. Just make sure you select (“all branches”) from the “Switch branch” menu.\n\n\n\n\n\n\n7.1.3 diff\nWhilst some of these actions described in this section are available from the “History” tab of the “Review Changes” window in Rstudio, most are only available as terminal commands.\n\n\n\n\n\n\n\n\n\nTwo of the three commits in our repository involved modifications to a file (dir1/file2). To further help illustrate commands to compare files indifferent states, we will additionally make a further change to dir1/file2. The git diff allows us to explore differences between:\n\nthe workspace and the staging area (index)\n\n# lets modify dir1/file2\necho 'Notes' &gt;&gt; dir1/file2\ngit diff\n\ndiff --git a/dir1/file2 b/dir1/file2\nindex 143a8bb..f12af0a 100644\n--- a/dir1/file2\n+++ b/dir1/file2\n@@ -1,2 +1,3 @@\n * Notes\n ---\n+Notes\n\n\nThe output indicates that we are comparing the blob representing dir1/file2 in the index (staging area) with the newly modified dir1/file2. The next couple of rows indicate that the indexed version will be represented by a ‘-’ sign and the new version will be represented by a ‘+’ sign. The next row (which is surrounded in a pair of @ signs, indicates that there are two lines that have changed. Finally the next two rows show that a charrage return has been added to the end of the first line and the new version has added the word ‘Notes’ to the next line.\nthe staging area and the last commit\n\ngit add .\ngit diff --cached\n\ndiff --git a/dir1/file2 b/dir1/file2\nindex 143a8bb..f12af0a 100644\n--- a/dir1/file2\n+++ b/dir1/file2\n@@ -1,2 +1,3 @@\n * Notes\n ---\n+Notes\n\n\nOnce we stage the modifications, we see that the same differences are recorded.\nthe index and a tree (in this case, the current tree)\n\ngit diff --cached HEAD^{tree}\n\ndiff --git a/dir1/file2 b/dir1/file2\nindex 143a8bb..f12af0a 100644\n--- a/dir1/file2\n+++ b/dir1/file2\n@@ -1,2 +1,3 @@\n * Notes\n ---\n+Notes\n\n\nthe workspace and the current commit\n\ngit diff HEAD\n\ndiff --git a/dir1/file2 b/dir1/file2\nindex 143a8bb..f12af0a 100644\n--- a/dir1/file2\n+++ b/dir1/file2\n@@ -1,2 +1,3 @@\n * Notes\n ---\n+Notes\n\n\ntwo commits (e.g. previous and current commits)\n\ngit diff HEAD^ HEAD\n\ndiff --git a/.gitignore b/.gitignore\nnew file mode 100644\nindex 0000000..1944fd6\n--- /dev/null\n+++ b/.gitignore\n@@ -0,0 +1 @@\n+*.tmp\ndiff --git a/dir1/file2 b/dir1/file2\nindex 4fcc8f8..143a8bb 100644\n--- a/dir1/file2\n+++ b/dir1/file2\n@@ -1 +1,2 @@\n * Notes\n+---\n\n\ntwo trees (first example, the current and previous commit trees)\n\ngit diff HEAD^{tree} HEAD^^{tree}\n\ndiff --git a/.gitignore b/.gitignore\ndeleted file mode 100644\nindex 1944fd6..0000000\n--- a/.gitignore\n+++ /dev/null\n@@ -1 +0,0 @@\n-*.tmp\ndiff --git a/dir1/file2 b/dir1/file2\nindex 143a8bb..4fcc8f8 100644\n--- a/dir1/file2\n+++ b/dir1/file2\n@@ -1,2 +1 @@\n * Notes\n----\n\n\n\ngit diff 07a94 2b61e\n\ndiff --git a/dir1/file2 b/dir1/file2\nnew file mode 100644\nindex 0000000..4fcc8f8\n--- /dev/null\n+++ b/dir1/file2\n@@ -0,0 +1 @@\n+* Notes\ndiff --git a/file1 b/file1\nindex 50fcd26..28ed245 100644\n--- a/file1\n+++ b/file1\n@@ -1 +1,2 @@\n File 1\n+---------------\n\n\ntwo blobs (indeed any two objects)\n\ngit diff 50fcd 28ed2\n\ndiff --git a/50fcd b/28ed2\nindex 50fcd26..28ed245 100644\n--- a/50fcd\n+++ b/28ed2\n@@ -1 +1,2 @@\n File 1\n+---------------\n\n\n\n\n\n7.1.4 ls-files\nSimilar to the previous section, the following is only really available via the terminal.\nWe can list the files that comprise the repository by:\n\ngit ls-files \n\n.gitignore\ndir1/file2\nfile1\n\n\nThe change to dir1/file2 above was just to illustrate the git diff. In doing so we now have a modified version of this file that has not been committed Before we move on, I am going to remove these changes so that the dir1/file2 is not in a modified state and reflects the state of the current commit. To do so, I will use perform a hard reset (git reset --hard). More will be discussed about the git reset command later in this tutorial - for now all that is important is to know that it restores the workspace to a previous state.\nIn addition to the git reset --hard, I will also clean and prune the repository.\n\ngit reset --hard \ngit clean -qfdx\ngit reflog expire --expire-unreachable=now --all\ngit gc --prune=now\n\nHEAD is now at b165944 Modified file2, added .gitignore",
    "crumbs": [
      "Reproducible research",
      "Introduction to git"
    ]
  },
  {
    "objectID": "10_git.html#tags",
    "href": "10_git.html#tags",
    "title": "Git and version control",
    "section": "7.2 Tags",
    "text": "7.2 Tags\nAlthough it is possible to track the history of a repository via its commit sha1 names, most find it more convenient to apply tags to certain milestone commits. For example, a particular commit might represent a specific point in the history of a project - such as a release version. Git tags allow us to apply more human readable flags.\n\nTerminalRstudio\n\n\n\ngit tag V.1\n\nIn the above, V.1 is the tag we are applying to the most recent commit. For this example, V.1 simply denotes something like “version 1”. The tag must not contain any spaces (just replace space characters with underscores or periods).\n\ngit log --graph --decorate --oneline\n\n* b165944 (HEAD -&gt; main, tag: V.1) Modified file2, added .gitignore\n* aa930b4 Modified file1 and added file2 (in dir1)\n* 3e8fd80 Initial repo and added file1\n\n\n\ngit reflog\n\nb165944 HEAD@{0}: reset: moving to HEAD\nb165944 HEAD@{1}: commit: Modified file2, added .gitignore\naa930b4 HEAD@{2}: commit: Modified file1 and added file2 (in dir1)\n3e8fd80 HEAD@{3}: commit (initial): Initial repo and added file1\n\n\n\n\nThe functionality to add tags to commits is not directly supported in Rstudio. in order to apply a tag, you will need to switch the terminal and enter a command like:\n\n\ngit tag V.1\n\nIn the above, V.1 is the tag we are applying to the most recent commit. For this example, V.1 simply denotes something like “version 1”. The tag must not contain any spaces (just replace space characters with underscores or periods).\nNow if we return to the “History” tab of the “Review Changes” window, we can see the tag represented by a yellow tag in the commit diagram.\n\n\n\n\n\n\n\n\n\n\nAnother visual representation of the repository",
    "crumbs": [
      "Reproducible research",
      "Introduction to git"
    ]
  },
  {
    "objectID": "10_git.html#merge-branches",
    "href": "10_git.html#merge-branches",
    "title": "Git and version control",
    "section": "8.1 Merge branches",
    "text": "8.1 Merge branches\nFinally, (if we are satisfied that Feature is stable and complete), we might like to introduce these changes into the main branch so that they become a part of the main project base. This operation is called a merge and is completed with the git merge &lt;branch&gt; command where &lt;branch&gt; is the name of the branch you want to merge the current branch (that pointed to by HEAD) with. Typically we want to merge the non-main branch with the main branch. Therefore we must be checkout the main branch before merging.\n\nTerminalRstudio\n\n\n\ngit checkout main\n\nSwitched to branch 'main'\n\n\n\ngit merge Feature --no-edit\n\nMerge made by the 'ort' strategy.\n dir1/file3 | 2 ++\n file1      | 1 +\n 2 files changed, 3 insertions(+)\n create mode 100644 dir1/file3\n\n\n\n\n\ncheckout the main branch using the “Switch branch” selector\nmerging is not directly supported in Rstudio, so go to the terminal and enter the git merge command as shown below\n\nif you now review the “History” tab of the “Review Changes” window, you will see the confluence of the two branches reflected in the commit graphic\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMore information following the creation of the branch\n\n\n\n\n\n\ngit log --graph --decorate --oneline --all\n\n*   ff084b7 (HEAD -&gt; main) Merge branch 'Feature'\n|\\  \n| * 1873e38 (Feature) Feature complete\n| * d42c9e2 New feature\n* | 62b3e3d Bug fix in file1\n|/  \n* 3f7b638 (tag: V.1) Modified file2, added .gitignore\n* 3e599df Modified file1 and added file2 (in dir1)\n* ce3fb9c Initial repo and added file1\n\n\n\ntree -ra -L 2 --charset ascii\ngit reflog\n\n.\n|-- file1\n|-- dir1\n|   |-- file3\n|   |-- file2\n|   `-- f.tmp\n|-- .gitignore\n`-- .git\n    |-- refs\n    |-- objects\n    |-- logs\n    |-- info\n    |-- index\n    |-- hooks\n    |-- description\n    |-- config\n    |-- branches\n    |-- ORIG_HEAD\n    |-- HEAD\n    `-- COMMIT_EDITMSG\n\n8 directories, 11 files\nff084b7 HEAD@{0}: merge Feature: Merge made by the 'ort' strategy.\n62b3e3d HEAD@{1}: checkout: moving from Feature to main\n1873e38 HEAD@{2}: commit: Feature complete\nd42c9e2 HEAD@{3}: checkout: moving from main to Feature\n62b3e3d HEAD@{4}: commit: Bug fix in file1\n3f7b638 HEAD@{5}: checkout: moving from Feature to main\nd42c9e2 HEAD@{6}: commit: New feature\n3f7b638 HEAD@{7}: checkout: moving from main to Feature\n3f7b638 HEAD@{8}: commit: Modified file2, added .gitignore\n3e599df HEAD@{9}: commit: Modified file1 and added file2 (in dir1)\nce3fb9c HEAD@{10}: commit (initial): Initial repo and added file1\n\n\n\n\n\n\n\n\n\n\n\nAnother visual representation of the repository\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\n\n\nIf, when issuing a git merge command, you get a conflict message, please refer to the section on resolving conflicts below.",
    "crumbs": [
      "Reproducible research",
      "Introduction to git"
    ]
  },
  {
    "objectID": "10_git.html#delete-a-branch",
    "href": "10_git.html#delete-a-branch",
    "title": "Git and version control",
    "section": "8.2 Delete a branch",
    "text": "8.2 Delete a branch\nOnce the purpose of the branch has been fulfilled (for example to develop a new feature) and the branch has been merged back into the main branch, you might consider deleting the branch so as to simplify the commit history.\nImportantly, this action should only ever be performed after the branch has been successfully merged into the main branch (in fact `git will not allow you to delete an un-merged branch unless you really fight for it).\nNote also, this can only be performed on a local repository.\nThis procedure is only available via the terminal.\n\n\ngit branch -d Feature\n\nDeleted branch Feature (was 1873e38).\n\n\n\n\n\n\n\n\n\n\n\n\nMore information following the creation of the branch\n\n\n\n\n\n\ngit log --graph --decorate --oneline --all\n\n*   ff084b7 (HEAD -&gt; main) Merge branch 'Feature'\n|\\  \n| * 1873e38 Feature complete\n| * d42c9e2 New feature\n* | 62b3e3d Bug fix in file1\n|/  \n* 3f7b638 (tag: V.1) Modified file2, added .gitignore\n* 3e599df Modified file1 and added file2 (in dir1)\n* ce3fb9c Initial repo and added file1\n\n\n\ntree -ra -L 2 --charset ascii\ngit reflog\n\n.\n|-- file1\n|-- dir1\n|   |-- file3\n|   |-- file2\n|   `-- f.tmp\n|-- .gitignore\n`-- .git\n    |-- refs\n    |-- packed-refs\n    |-- objects\n    |-- logs\n    |-- info\n    |-- index\n    |-- hooks\n    |-- description\n    |-- config\n    |-- branches\n    |-- ORIG_HEAD\n    |-- HEAD\n    `-- COMMIT_EDITMSG\n\n8 directories, 12 files\nff084b7 HEAD@{0}: merge Feature: Merge made by the 'ort' strategy.\n62b3e3d HEAD@{1}: checkout: moving from Feature to main\n1873e38 HEAD@{2}: commit: Feature complete\nd42c9e2 HEAD@{3}: checkout: moving from main to Feature\n62b3e3d HEAD@{4}: commit: Bug fix in file1\n3f7b638 HEAD@{5}: checkout: moving from Feature to main\nd42c9e2 HEAD@{6}: commit: New feature\n3f7b638 HEAD@{7}: checkout: moving from main to Feature\n3f7b638 HEAD@{8}: commit: Modified file2, added .gitignore\n3e599df HEAD@{9}: commit: Modified file1 and added file2 (in dir1)\nce3fb9c HEAD@{10}: commit (initial): Initial repo and added file1\n\n\n\n\n\n\n\n\n\n\n\nAnother visual representation of the repository\n\n\n\n\n\n\n\n\n\nIn order to facilitate the rest of the tutorial, I am going to reset the repository to a commit that precedes the merge. The process of resetting will be covered later on in this tutorial.\n\n\n\n\n\ngit reset --hard HEAD~1\ngit clean -qfdx\ngit reflog expire --expire-unreachable=now --all\ngit gc --prune=now\n\nHEAD is now at 62b3e3d Bug fix in file1\n\n\n\n\n\n\n\n\n\n\n\n\nMore information following the creation of the branch\n\n\n\n\n\n\ntree -ra -L 2 --charset ascii\ngit log --graph --decorate --oneline --all\ngit reflog\n\n.\n|-- file1\n|-- dir1\n|   `-- file2\n|-- .gitignore\n`-- .git\n    |-- refs\n    |-- packed-refs\n    |-- objects\n    |-- logs\n    |-- info\n    |-- index\n    |-- hooks\n    |-- description\n    |-- config\n    |-- branches\n    |-- ORIG_HEAD\n    |-- HEAD\n    `-- COMMIT_EDITMSG\n\n8 directories, 10 files\n* 1873e38 (Feature) Feature complete\n* d42c9e2 New feature\n| * 62b3e3d (HEAD -&gt; main) Bug fix in file1\n|/  \n* 3f7b638 (tag: V.1) Modified file2, added .gitignore\n* 3e599df Modified file1 and added file2 (in dir1)\n* ce3fb9c Initial repo and added file1\n62b3e3d HEAD@{0}: checkout: moving from Feature to main\n1873e38 HEAD@{1}: commit: Feature complete\nd42c9e2 HEAD@{2}: checkout: moving from main to Feature\n62b3e3d HEAD@{3}: commit: Bug fix in file1\n3f7b638 HEAD@{4}: checkout: moving from Feature to main\nd42c9e2 HEAD@{5}: commit: New feature\n3f7b638 HEAD@{6}: checkout: moving from main to Feature\n3f7b638 HEAD@{7}: commit: Modified file2, added .gitignore\n3e599df HEAD@{8}: commit: Modified file1 and added file2 (in dir1)\nce3fb9c HEAD@{9}: commit (initial): Initial repo and added file1",
    "crumbs": [
      "Reproducible research",
      "Introduction to git"
    ]
  },
  {
    "objectID": "10_git.html#rebasing",
    "href": "10_git.html#rebasing",
    "title": "Git and version control",
    "section": "8.3 Rebasing",
    "text": "8.3 Rebasing\nBranches are great for working on new features without disturbing a stable codebase. However, the main branch may have changed or progressed since the branch started. As a result, it may be building upon old code that may either no longer work or no longer be appropriate.\nWhilst you could attempt to manually apply the newer main code changes into your branch, this is likely to be tedious and error prone. Rebasing is the process of changing the root (base) of the branch from one commit to another. In this way, the base of the branch can be moved to the current HEAD of the main branch, thereby absorbing all the updates from the main branch into the feature branch.\nThis section builds on the repository created up to this point in the tutorial. To remind you, the repository currently looks like:\n\n\n\n\n\n\nCommands to create the repository\n\n\n\n\n\n\nrm -rf ~/tmp/Repo1\nmkdir ~/tmp/Repo1\ncd ~/tmp/Repo1\ngit init \necho 'File 1' &gt; file1\ngit add file1\ngit commit -m 'Initial repo and added file1'\necho '---------------' &gt;&gt; file1\nmkdir dir1\necho '* Notes' &gt; dir1/file2\ngit add file1 dir1/file2\ngit commit -m 'Modified file1 and added file2 (in dir1)'\necho '---' &gt;&gt; dir1/file2\necho 'temp' &gt; dir1/f.tmp\necho '*.tmp' &gt; .gitignore\ngit add .\ngit commit -m 'Modified file2, added .gitignore'\ngit tag V.1\ngit branch Feature\ngit checkout Feature\necho 'b' &gt;&gt; file1\necho 'File 3' &gt; dir1/file3\ngit add .\ngit commit -m 'New feature'\ngit checkout main\necho ' another bug fix' &gt;&gt; dir1/file2\ngit add .\ngit commit -m 'Bug fix in file1'\ngit checkout Feature\necho ' a modification' &gt;&gt; dir1/file3\ngit add .\ngit commit -m 'Feature complete'\n\nInitialized empty Git repository in /home/runner/tmp/Repo1/.git/\n[main (root-commit) d5d9d1d] Initial repo and added file1\n 1 file changed, 1 insertion(+)\n create mode 100644 file1\n[main 1b394bd] Modified file1 and added file2 (in dir1)\n 2 files changed, 2 insertions(+)\n create mode 100644 dir1/file2\n[main df53a0c] Modified file2, added .gitignore\n 2 files changed, 2 insertions(+)\n create mode 100644 .gitignore\nSwitched to branch 'Feature'\n[Feature 4cd4402] New feature\n 2 files changed, 2 insertions(+)\n create mode 100644 dir1/file3\nSwitched to branch 'main'\n[main 8876db7] Bug fix in file1\n 1 file changed, 1 insertion(+)\nSwitched to branch 'Feature'\n[Feature 360d05f] Feature complete\n 1 file changed, 1 insertion(+)\n\n\n\n\n\n\n\n\n\n\n\n\nMore information about this repository\n\n\n\n\n\n\ntree -ra -L 2 --charset ascii\n\n.\n|-- file1\n|-- dir1\n|   |-- file3\n|   |-- file2\n|   `-- f.tmp\n|-- .gitignore\n`-- .git\n    |-- refs\n    |-- objects\n    |-- logs\n    |-- info\n    |-- index\n    |-- hooks\n    |-- description\n    |-- config\n    |-- branches\n    |-- HEAD\n    `-- COMMIT_EDITMSG\n\n8 directories, 10 files\n\n\n\ngit log --graph --decorate --oneline --all\n\n* 360d05f (HEAD -&gt; Feature) Feature complete\n* 4cd4402 New feature\n| * 8876db7 (main) Bug fix in file1\n|/  \n* df53a0c (tag: V.1) Modified file2, added .gitignore\n* 1b394bd Modified file1 and added file2 (in dir1)\n* d5d9d1d Initial repo and added file1\n\n\n\ngit reflog\n\n360d05f HEAD@{0}: commit: Feature complete\n4cd4402 HEAD@{1}: checkout: moving from main to Feature\n8876db7 HEAD@{2}: commit: Bug fix in file1\ndf53a0c HEAD@{3}: checkout: moving from Feature to main\n4cd4402 HEAD@{4}: commit: New feature\ndf53a0c HEAD@{5}: checkout: moving from main to Feature\ndf53a0c HEAD@{6}: commit: Modified file2, added .gitignore\n1b394bd HEAD@{7}: commit: Modified file1 and added file2 (in dir1)\nd5d9d1d HEAD@{8}: commit (initial): Initial repo and added file1\n\n\n\n\n\n\nTerminalRstudio\n\n\n\ngit checkout Feature\n\nAlready on 'Feature'\n\n\n\ngit rebase main\n\nRebasing (1/2)\nRebasing (2/2)\n\n                                                                                \nSuccessfully rebased and updated refs/heads/Feature.\n\n\n\n\nRebasing is not directly supported by Rstudio.\n\ncheckout the Feature branch using the “Switch branch” selector\nin the terminal, type git rebase main to rebase the Feature branch on the end of the main branch.\nif you now review the “History” tab of the “Review Changes” window, you will now see that the history is linear and the Feature branch stems from the end of the main branch. That is, we have moved the base of the Feature branch.\n\n\n\n\n\n\n\n\n\n\n\nMore information about this repository\n\n\n\n\n\n\ntree -ra -L 2 --charset ascii\n\n.\n|-- file1\n|-- dir1\n|   |-- file3\n|   |-- file2\n|   `-- f.tmp\n|-- .gitignore\n`-- .git\n    |-- refs\n    |-- objects\n    |-- logs\n    |-- info\n    |-- index\n    |-- hooks\n    |-- description\n    |-- config\n    |-- branches\n    |-- ORIG_HEAD\n    |-- HEAD\n    |-- COMMIT_EDITMSG\n    `-- AUTO_MERGE\n\n8 directories, 12 files\n\n\n\ngit log --graph --decorate --oneline --all\n\n* 9926ad7 (HEAD -&gt; Feature) Feature complete\n* 11f132b New feature\n* 8876db7 (main) Bug fix in file1\n* df53a0c (tag: V.1) Modified file2, added .gitignore\n* 1b394bd Modified file1 and added file2 (in dir1)\n* d5d9d1d Initial repo and added file1\n\n\n\ngit reflog\n\n9926ad7 HEAD@{0}: rebase (finish): returning to refs/heads/Feature\n9926ad7 HEAD@{1}: rebase (pick): Feature complete\n11f132b HEAD@{2}: rebase (pick): New feature\n8876db7 HEAD@{3}: rebase (start): checkout main\n360d05f HEAD@{4}: checkout: moving from Feature to Feature\n360d05f HEAD@{5}: commit: Feature complete\n4cd4402 HEAD@{6}: checkout: moving from main to Feature\n8876db7 HEAD@{7}: commit: Bug fix in file1\ndf53a0c HEAD@{8}: checkout: moving from Feature to main\n4cd4402 HEAD@{9}: commit: New feature\ndf53a0c HEAD@{10}: checkout: moving from main to Feature\ndf53a0c HEAD@{11}: commit: Modified file2, added .gitignore\n1b394bd HEAD@{12}: commit: Modified file1 and added file2 (in dir1)\nd5d9d1d HEAD@{13}: commit (initial): Initial repo and added file1\n\n\n\n\n\n\n\n\n\nIf the rebased commits were previously on a remote repository (hopefully you checked to make sure noone was relying on any of the commits that have been squashed), then it will be necessary to force a push on this repository.",
    "crumbs": [
      "Reproducible research",
      "Introduction to git"
    ]
  },
  {
    "objectID": "10_git.html#reset",
    "href": "10_git.html#reset",
    "title": "Git and version control",
    "section": "9.1 Reset",
    "text": "9.1 Reset\nReset is not directly supported by Rstudio - use the terminal for this section.\n\n9.1.1 Soft reset\nWhen we perform a soft reset, we move the head to the nominated commit, but the workspace is unchanged.\n\ngit reset --soft 8666504\n\n\n\n\n\n\n\nMore information about this repository\n\n\n\n\n\n\ntree -ra -L 2 --charset ascii\n\n.\n|-- file1\n|-- dir1\n|   |-- file2\n|   `-- f.tmp\n|-- .gitignore\n`-- .git\n    |-- refs\n    |-- objects\n    |-- logs\n    |-- info\n    |-- index\n    |-- hooks\n    |-- description\n    |-- config\n    |-- branches\n    |-- ORIG_HEAD\n    |-- HEAD\n    `-- COMMIT_EDITMSG\n\n8 directories, 10 files\n\n\n\ngit log --graph --decorate --oneline --all\n\n* 9b90dde (tag: V.1) Modified file2, added .gitignore\n* 8666504 (HEAD -&gt; main) Modified file1 and added file2 (in dir1)\n* 19152cc Initial repo and added file1\n\n\n\n\n8666504 HEAD@{0}: reset: moving to 8666504\n9b90dde HEAD@{1}: commit: Modified file2, added .gitignore\n8666504 HEAD@{2}: commit: Modified file1 and added file2 (in dir1)\n19152cc HEAD@{3}: commit (initial): Initial repo and added file1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnother visual representation of the repository\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMore information about this repository\n\n\n\n\n\n\ntree -ra -L 2 --charset ascii\n\n.\n|-- file1\n|-- dir1\n|   |-- file2\n|   `-- f.tmp\n|-- .gitignore\n`-- .git\n    |-- refs\n    |-- objects\n    |-- logs\n    |-- info\n    |-- index\n    |-- hooks\n    |-- description\n    |-- config\n    |-- branches\n    |-- ORIG_HEAD\n    |-- HEAD\n    `-- COMMIT_EDITMSG\n\n8 directories, 10 files\n\n\n\ngit log --graph --decorate --oneline --all\n\n* 9b90dde (tag: V.1) Modified file2, added .gitignore\n* 8666504 (HEAD -&gt; main) Modified file1 and added file2 (in dir1)\n* 19152cc Initial repo and added file1\n\n\n\n\n8666504 HEAD@{0}: reset: moving to 8666504\n9b90dde HEAD@{1}: commit: Modified file2, added .gitignore\n8666504 HEAD@{2}: commit: Modified file1 and added file2 (in dir1)\n19152cc HEAD@{3}: commit (initial): Initial repo and added file1\n\n\n\n\n\n\n\n9.1.2 Hard reset\nWhen we perform a hard reset, we not only move the head to the nominated commit, but the workspace is altered to reflect the workspace that existed when that commit was originally performed.\nAs I am about to demonstrate this on a repo that I have just performed a soft reset on, I am first going to start by re-establishing the original repository. If you have not just run a soft reset, then ignore the following.\n\n\n\n\n\n\nRe-establish repository\n\n\n\n\n\n\ngit reset --hard V.1\ngit clean -qfdx\ngit reflog expire --expire-unreachable=now --all\ngit gc --prune=now\n\nHEAD is now at 9b90dde Modified file2, added .gitignore\n\n\n\n\n\nNow we are in a position to perform the hard reset.\n\ngit reset --hard 8666504\n\nHEAD is now at 8666504 Modified file1 and added file2 (in dir1)\n\n\n\n\n\n\n\n\nMore information about this repository\n\n\n\n\n\n\ntree -ra -L 2 --charset ascii\n\n.\n|-- file1\n|-- dir1\n|   `-- file2\n`-- .git\n    |-- refs\n    |-- packed-refs\n    |-- objects\n    |-- logs\n    |-- info\n    |-- index\n    |-- hooks\n    |-- description\n    |-- config\n    |-- branches\n    |-- ORIG_HEAD\n    |-- HEAD\n    `-- COMMIT_EDITMSG\n\n8 directories, 9 files\n\n\nNotice that .gitignore is not not present.\n\ngit log --graph --decorate --oneline --all\n\n* 9b90dde (tag: V.1) Modified file2, added .gitignore\n* 8666504 (HEAD -&gt; main) Modified file1 and added file2 (in dir1)\n* 19152cc Initial repo and added file1\n\n\n\ngit reflog\n\n8666504 HEAD@{0}: reset: moving to 8666504\n9b90dde HEAD@{1}: reset: moving to V.1\n8666504 HEAD@{2}: reset: moving to 8666504\n9b90dde HEAD@{3}: commit: Modified file2, added .gitignore\n8666504 HEAD@{4}: commit: Modified file1 and added file2 (in dir1)\n19152cc HEAD@{5}: commit (initial): Initial repo and added file1\n\n\nNote, however, if we looked at the log, it would be as if the previous commit had not occurred. For this reason, care must be exercised when using reset on remote repositories since others may be relying on a specific point in the repo history that you may have just erased.\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnother visual representation of the repository\n\n\n\n\n\n\n\n\n\nIf we now make a change (such as a change to file1 and adding file3) and commit, it would be as if any commits after 8666504 had never occurred.\n\necho 'End' &gt; file1\necho 'File3' &gt;&gt; dir1/file3\ngit add file1 dir1/file3\ngit commit -m 'Modified file1 and added file3'\n\n[main c850ba7] Modified file1 and added file3\n 2 files changed, 2 insertions(+), 2 deletions(-)\n create mode 100644 dir1/file3\n\n\n\n\n\n\n\n\nMore information about this repository\n\n\n\n\n\n\ngit log --graph --decorate --oneline --all\n\n* c850ba7 (HEAD -&gt; main) Modified file1 and added file3\n| * 9b90dde (tag: V.1) Modified file2, added .gitignore\n|/  \n* 8666504 Modified file1 and added file2 (in dir1)\n* 19152cc Initial repo and added file1\n\n\n\ngit reflog\n\nc850ba7 HEAD@{0}: commit: Modified file1 and added file3\n8666504 HEAD@{1}: reset: moving to 8666504\n9b90dde HEAD@{2}: reset: moving to V.1\n8666504 HEAD@{3}: reset: moving to 8666504\n9b90dde HEAD@{4}: commit: Modified file2, added .gitignore\n8666504 HEAD@{5}: commit: Modified file1 and added file2 (in dir1)\n19152cc HEAD@{6}: commit (initial): Initial repo and added file1\n\n\n\ntree -ra -L 2 --charset ascii\n\n.\n|-- file1\n|-- dir1\n|   |-- file3\n|   `-- file2\n`-- .git\n    |-- refs\n    |-- packed-refs\n    |-- objects\n    |-- logs\n    |-- info\n    |-- index\n    |-- hooks\n    |-- description\n    |-- config\n    |-- branches\n    |-- ORIG_HEAD\n    |-- HEAD\n    `-- COMMIT_EDITMSG\n\n8 directories, 10 files\n\n\nNotice the addition of file3 in dir1\n\ngit ls-files\n\ndir1/file2\ndir1/file3\nfile1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnother visual representation of the repository",
    "crumbs": [
      "Reproducible research",
      "Introduction to git"
    ]
  },
  {
    "objectID": "10_git.html#revert",
    "href": "10_git.html#revert",
    "title": "Git and version control",
    "section": "9.2 Revert",
    "text": "9.2 Revert\nAs with git reset, git revert is not directly supported by Rstudio, hence the methods used in this section should be performed in the terminal. There is one exception to this, Rstudio is able to revert an modified file back to its state in the last commit.\n\n\n\n\n\n\nRe-establish repository\n\n\n\n\n\n\ngit reset --hard V.1\ngit clean -qfdx\ngit reflog expire --expire-unreachable=now --all\ngit gc --prune=now\n\nHEAD is now at 9b90dde Modified file2, added .gitignore\n\n\n\n\n\nRevert generates a new commit that removes the changes that were introduced by one or more of the most recent commits. Note, it does not revert to a particular commit, but rather undoes a commit. So, to roll back to 8666504 (the second last commit), we just have to revert the last commit (HEAD).\n\ngit revert HEAD\n\n[main bc4b4be] Revert \"Modified file2, added .gitignore\"\n Date: Thu Mar 28 03:38:18 2024 +0000\n 2 files changed, 2 deletions(-)\n delete mode 100644 .gitignore\n\n\nHowever, if we explore the reflog, we can see the entire history\n\ngit reflog\n\nbc4b4be HEAD@{0}: revert: Revert \"Modified file2, added .gitignore\"\n8666504 HEAD@{1}: reset: moving to 8666504\n9b90dde HEAD@{2}: reset: moving to V.1\n8666504 HEAD@{3}: reset: moving to 8666504\n9b90dde HEAD@{4}: commit: Modified file2, added .gitignore\n8666504 HEAD@{5}: commit: Modified file1 and added file2 (in dir1)\n19152cc HEAD@{6}: commit (initial): Initial repo and added file1\n\n\n\ngit log --graph --decorate --oneline --all\n\n* bc4b4be (HEAD -&gt; main) Revert \"Modified file2, added .gitignore\"\n* 9b90dde (tag: V.1) Modified file2, added .gitignore\n* 8666504 Modified file1 and added file2 (in dir1)\n* 19152cc Initial repo and added file1\n\n\n\n\n\n\n\n\nMore information about this repository\n\n\n\n\n\n\ntree -ra -L 2 --charset ascii\n\n.\n|-- file1\n|-- dir1\n|   `-- file2\n`-- .git\n    |-- refs\n    |-- packed-refs\n    |-- objects\n    |-- logs\n    |-- info\n    |-- index\n    |-- hooks\n    |-- description\n    |-- config\n    |-- branches\n    |-- ORIG_HEAD\n    |-- HEAD\n    |-- COMMIT_EDITMSG\n    `-- AUTO_MERGE\n\n8 directories, 10 files\n\n\nNotice the absence of .gitignore. Notice also that dir1/f.tmp is also present. Although this file was added at the same time as .gitignore, it was never committed and therefore is not altered with repo manipulations.\nIf we list the files that are part of the repo:\n\ngit ls-files\n\ndir1/file2\nfile1\n\n\nwe will see that we are back to the state where only file1 and dir1/file2 are present.\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnother visual representation of the repository\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRe-establish repository\n\n\n\n\n\n\ngit reset --hard V.1\ngit clean -qfdx\ngit reflog expire --expire-unreachable=now --all\ngit gc --prune=now\n\nHEAD is now at 9b90dde Modified file2, added .gitignore\n\n\n\n\n\nIf we had actually wanted to roll back to commit 19152cc, then we could do so by sequentially issuing git revert:\n\ngit revert --no-commit HEAD\ngit revert --no-commit HEAD~1\ngit commit -m 'Rolled back'\n\n[main c9f013a] Rolled back\n 3 files changed, 4 deletions(-)\n delete mode 100644 .gitignore\n delete mode 100644 dir1/file2\n\n\n\n\n\n\n\n\nMore information about this repository\n\n\n\n\n\n\ngit reflog\n\nc9f013a HEAD@{0}: commit: Rolled back\n8666504 HEAD@{1}: reset: moving to 8666504\n9b90dde HEAD@{2}: reset: moving to V.1\n8666504 HEAD@{3}: reset: moving to 8666504\n9b90dde HEAD@{4}: commit: Modified file2, added .gitignore\n8666504 HEAD@{5}: commit: Modified file1 and added file2 (in dir1)\n19152cc HEAD@{6}: commit (initial): Initial repo and added file1\n\n\n\ngit log --graph --decorate --oneline --all\n\n* c9f013a (HEAD -&gt; main) Rolled back\n* 9b90dde (tag: V.1) Modified file2, added .gitignore\n* 8666504 Modified file1 and added file2 (in dir1)\n* 19152cc Initial repo and added file1\n\n\n\ntree -ra -L 2 --charset ascii\n\n.\n|-- file1\n`-- .git\n    |-- refs\n    |-- packed-refs\n    |-- objects\n    |-- logs\n    |-- info\n    |-- index\n    |-- hooks\n    |-- description\n    |-- config\n    |-- branches\n    |-- ORIG_HEAD\n    |-- HEAD\n    `-- COMMIT_EDITMSG\n\n7 directories, 8 files\n\n\nNotice that file2 is now also absent. If we list the files that are part of the repo:\n\ngit ls-files\n\nfile1\n\n\nwe will see that we are back to the state where only file is present\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnother visual representation of the repository",
    "crumbs": [
      "Reproducible research",
      "Introduction to git"
    ]
  },
  {
    "objectID": "10_git.html#checkout-and-branching",
    "href": "10_git.html#checkout-and-branching",
    "title": "Git and version control",
    "section": "9.3 Checkout and branching",
    "text": "9.3 Checkout and branching\nOnce again, the methods outlined in this section are not directly supported by Rstudio. Please use the terminal instead.\n\n\n\n\n\n\nRe-establish repository\n\n\n\n\n\n\ngit reset --hard V.1\ngit clean -qfdx\ngit reflog expire --expire-unreachable=now --all\ngit gc --prune=now\n\nHEAD is now at 9b90dde Modified file2, added .gitignore\n\n\n\n\n\nIf we wanted to review the state of files corresponding to commit 8666504, we could checkout the code from that commit. This provides a way to travel back in time through your commits and explore the (tracked) files exactly as they were.\n\ngit checkout 86665\n\nNote: switching to '86665'.\n\nYou are in 'detached HEAD' state. You can look around, make experimental\nchanges and commit them, and you can discard any commits you make in this\nstate without impacting any branches by switching back to a branch.\n\nIf you want to create a new branch to retain commits you create, you may\ndo so (now or later) by using -c with the switch command. Example:\n\n  git switch -c &lt;new-branch-name&gt;\n\nOr undo this operation with:\n\n  git switch -\n\nTurn off this advice by setting config variable advice.detachedHead to false\n\nHEAD is now at 8666504 Modified file1 and added file2 (in dir1)\n\n\n\n\n\n\n\n\nMore information about this repository\n\n\n\n\n\n\ngit reflog\n\n8666504 HEAD@{0}: checkout: moving from main to 86665\n8666504 HEAD@{1}: reset: moving to 8666504\n9b90dde HEAD@{2}: reset: moving to V.1\n8666504 HEAD@{3}: reset: moving to 8666504\n9b90dde HEAD@{4}: commit: Modified file2, added .gitignore\n8666504 HEAD@{5}: commit: Modified file1 and added file2 (in dir1)\n19152cc HEAD@{6}: commit (initial): Initial repo and added file1\n\n\n\ngit log --graph --decorate --oneline --all\n\n* 9b90dde (tag: V.1, main) Modified file2, added .gitignore\n* 8666504 (HEAD) Modified file1 and added file2 (in dir1)\n* 19152cc Initial repo and added file1\n\n\n\ntree -ra -L 2 --charset ascii\n\n.\n|-- file1\n|-- dir1\n|   `-- file2\n`-- .git\n    |-- refs\n    |-- packed-refs\n    |-- objects\n    |-- logs\n    |-- info\n    |-- index\n    |-- hooks\n    |-- description\n    |-- config\n    |-- branches\n    |-- ORIG_HEAD\n    |-- HEAD\n    `-- COMMIT_EDITMSG\n\n8 directories, 9 files\n\n\nNotice that file2 is now also absent. If we list the files that are part of the repo:\n\ngit ls-files\n\ndir1/file2\nfile1\n\n\nwe will see that we are back to the state where only file is present\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnother visual representation of the repository\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRstudio git history representation\n\n\n\n\n\nIf we go to the “History” tab of the “Review Changes” window, you will notice that the commit history has been truncated to reflect that we have gone back in commit history.\n\nNevertheless, if we select “All branches” from the dropdown menu, we can see the full commit history.\n\n\n\n\nThe output advises us that we are in a detached HEAD state. This occurs when a commit is checked out rather than a branch. Normally, when changes are committed, the new commit is added to the HEAD of the current branch. However, in a detached HEAD state, any commits that are made are not associated with any branch and will effectively be lost next time you checkout.\nSo if for example, we then added another file (file3)..\n\necho 'END' &gt; file3\ngit add file3\ngit commit -m 'END added to file3'\n\n[detached HEAD 41420f4] END added to file3\n 1 file changed, 1 insertion(+)\n create mode 100644 file3\n\n\n\n\n\n\n\n\nMore information about this repository\n\n\n\n\n\n\ngit reflog\n\n41420f4 HEAD@{0}: commit: END added to file3\n8666504 HEAD@{1}: checkout: moving from main to 86665\n8666504 HEAD@{2}: reset: moving to 8666504\n9b90dde HEAD@{3}: reset: moving to V.1\n8666504 HEAD@{4}: reset: moving to 8666504\n9b90dde HEAD@{5}: commit: Modified file2, added .gitignore\n8666504 HEAD@{6}: commit: Modified file1 and added file2 (in dir1)\n19152cc HEAD@{7}: commit (initial): Initial repo and added file1\n\n\n\ngit log --graph --decorate --oneline --all\n\n* 41420f4 (HEAD) END added to file3\n| * 9b90dde (tag: V.1, main) Modified file2, added .gitignore\n|/  \n* 8666504 Modified file1 and added file2 (in dir1)\n* 19152cc Initial repo and added file1\n\n\n\n\n\nNow if we checked out main, the commit we made whilst in detached head mode would be lost.\n\ngit checkout main\n\nWarning: you are leaving 1 commit behind, not connected to\nany of your branches:\n\n  41420f4 END added to file3\n\nIf you want to keep it by creating a new branch, this may be a good time\nto do so with:\n\n git branch &lt;new-branch-name&gt; 41420f4\n\nSwitched to branch 'main'\n\n\n\n\n\n\n\n\nMore information about this repository\n\n\n\n\n\n\ngit reflog\n\n9b90dde HEAD@{0}: checkout: moving from 41420f43a8fecf508cc1f7888121867005dea9d1 to main\n41420f4 HEAD@{1}: commit: END added to file3\n8666504 HEAD@{2}: checkout: moving from main to 86665\n8666504 HEAD@{3}: reset: moving to 8666504\n9b90dde HEAD@{4}: reset: moving to V.1\n8666504 HEAD@{5}: reset: moving to 8666504\n9b90dde HEAD@{6}: commit: Modified file2, added .gitignore\n8666504 HEAD@{7}: commit: Modified file1 and added file2 (in dir1)\n19152cc HEAD@{8}: commit (initial): Initial repo and added file1\n\n\n\ngit log --graph --decorate --oneline --all\n\n* 9b90dde (HEAD -&gt; main, tag: V.1) Modified file2, added .gitignore\n* 8666504 Modified file1 and added file2 (in dir1)\n* 19152cc Initial repo and added file1\n\n\n\n\n\nIf, having reviewed the state of a commit (by checking it out), we decided that we wanted to roll back to this state and develop further (make additional commits), we are effectively deciding to start a new branch that splits off at that commit. See the section on Branching for more details on how to do that.",
    "crumbs": [
      "Reproducible research",
      "Introduction to git"
    ]
  },
  {
    "objectID": "10_git.html#simulate-a-remote-repository-locally",
    "href": "10_git.html#simulate-a-remote-repository-locally",
    "title": "Git and version control",
    "section": "10.1 Simulate a remote repository locally",
    "text": "10.1 Simulate a remote repository locally\nFor the purpose of this tutorial, we will create a remote repository that is on the same computer as the above repository that we have been working on. Whilst not the typical situation, it does mean that an external location and account is not necessary to follow along with the tutorial. As previously mentioned, the actual location of the remote repository is almost irrelevant to how you interact with it. Therefore, whether the remote repository is on the same computer or elsewhere in the world makes little difference (other than permissions and connections).\nThis step is not supported directly by Rstudio - please use the terminal.\n\ncd ~/tmp/RemoteRepo1\ngit init --bare\n\nInitialized empty Git repository in /home/runner/tmp/RemoteRepo1/\n\n\nNow that we have a remote repository - albeit empty at this stage - we return to our local repository and declare (add) the location of the remote repository using the git remote add &lt;name&gt; &lt;url&gt; command. In this command, an optional name can be supplied to refer to the remote repository (&lt;name&gt;). The compulsory &lt;url&gt; argument is the address (location) of the remote repository.\nThis step is not supported directly by Rstudio - please use the terminal.\n\ngit remote add origin ~/tmp/RemoteRepo1\n\nTo see what this has achieved, we can have a quick look at the .git/config\n\ncat .git/config\n\n[core]\n    repositoryformatversion = 0\n    filemode = true\n    bare = false\n    logallrefupdates = true\n[remote \"origin\"]\n    url = /home/runner/tmp/RemoteRepo1\n    fetch = +refs/heads/*:refs/remotes/origin/*\n\n\nYou should notice that there is now a ‘remote’ section with the name of ‘origin’ and the ‘url’ points to the location we nominated.\n\n10.1.1 Pushing\nCurrently the remote repository is empty. We will now push our local commit history to the remote repository. This is achieved via the git push -u &lt;name&gt; &lt;ref&gt; command. Here, &lt;name&gt; is the name of the remote repository (‘origin’) and &lt;ref&gt; is a reference the head of the commit chain we want to sync.\n\nTerminalRstudio\n\n\n\ngit push -u origin main\n\nTo /home/runner/tmp/RemoteRepo1\n * [new branch]      main -&gt; main\nbranch 'main' set up to track 'origin/main'.\n\n\n\n\nRstudio does not have a direct means by which we can define the remote repository. Thus, we must start by entering the following into the terminal.\n\ngit push -u origin main\n\nThereafter, you might notice that some up and down (push and pull respectively) buttons become active within the “git” panel.\n\nNow, after each subsequent commit, you can “push” your code to the remote repository simply by pushing the up (push) arrow.\n\n\n\n\ngit reflog\ngit log --graph --decorate --oneline --all\n\ndb8857f HEAD@{0}: checkout: moving from Feature to main\n7cf8d04 HEAD@{1}: commit: Feature complete\n874b199 HEAD@{2}: checkout: moving from main to Feature\ndb8857f HEAD@{3}: commit: Bug fix in file1\n9e38af6 HEAD@{4}: checkout: moving from Feature to main\n874b199 HEAD@{5}: commit: New feature\n9e38af6 HEAD@{6}: checkout: moving from main to Feature\n9e38af6 HEAD@{7}: commit: Modified file2, added .gitignore\ncb35bf5 HEAD@{8}: commit: Modified file1 and added file2 (in dir1)\n2b2a239 HEAD@{9}: commit (initial): Initial repo and added file1\n* 7cf8d04 (Feature) Feature complete\n* 874b199 New feature\n| * db8857f (HEAD -&gt; main, origin/main) Bug fix in file1\n|/  \n* 9e38af6 Modified file2, added .gitignore\n* cb35bf5 Modified file1 and added file2 (in dir1)\n* 2b2a239 Initial repo and added file1\n\n\n\n\n\n\n\n\n\n\n\n\nAnother visual representation of the repository\n\n\n\n\n\n\n\n\n\nNote that when we pushed the commits to the remote repository, we only pushed the main branch. Consequently, the remote repository only has a single branch.",
    "crumbs": [
      "Reproducible research",
      "Introduction to git"
    ]
  },
  {
    "objectID": "10_git.html#cloning",
    "href": "10_git.html#cloning",
    "title": "Git and version control",
    "section": "10.2 Cloning",
    "text": "10.2 Cloning\nTo collaborate with others on a repository, we start by cloning the repository you wish to collaborate on. So at the moment, we have the original repository (~/tmp/Repo1) created by user 1. We also have a remote repository (~/tmp/RemoteRepo1).\nTo demonstrate cloning (and collaborating), we will also assume the personal of user 2 and we will clone the remote repository to yet another local path (~/tmp/MyRepo1). Of course, this would not normally be on the same machine as the original repository, we are just doing it this way to simulate multiple users on the same machine.\n\nTerminalRstudio\n\n\n\ngit clone ~/tmp/RemoteRepo1 ~/tmp/MyRepo1\n\nCloning into '/home/runner/tmp/MyRepo1'...\ndone.\n\n\n\n\n\nclick on the Project selector in the top right of the Rstudio window (as highlighted by the red ellipse in the image below.\n\nselect New Project from the dropdown menu\nselect Version Control form the Create Project panel\nselect Git from the Create Project from Version Control panel\nprovide a path to a remote repository. Normally this URL would be for a location on a server such as Github, Gitlab, Bitbucket etc.\nHowever, for this demostration we will point to the remote repository that we set up in the previous section (~/tmp/RemoteRepo1)\nprovide a directory name in which to store this new cloned repository. Normally this field is populated based on the name give in the URL. However, in this case, it would suggest a name of RemoteRepo1 which already exists (for the repository we are trying to clone), and we don’t wish to overwrite that one. I will instead offer an alternative name (MyRepo1).\nwe also need to supply a path to where this cloned repository will be stored.\n\nclick the “Create Project” button.\n\n\n\n\n\nThe contents (and state) of ~/tmp/MyRepo1 should match that of ~/tmp/Repo1 (other than any files excluded due to a .gitignore or files not yet committed).\n\ntree -ra -L 2 --charset ascii\n\n.\n|-- file1\n|-- dir1\n|   `-- file2\n|-- .gitignore\n`-- .git\n    |-- refs\n    |-- packed-refs\n    |-- objects\n    |-- logs\n    |-- info\n    |-- index\n    |-- hooks\n    |-- description\n    |-- config\n    |-- branches\n    `-- HEAD\n\n8 directories, 8 files\n\n\n\n\n\n\nNote that when cloning repository, all branches in the remote repository are cloned. However, since the remote repository only had one branch (main), so too the clone only has one branch.\nNow as the collaborator (user 2), lets make a modification and push this change up to the remote repository.\n\n\n\n\n\n\nImportant info about pushing to a remote repository\n\n\n\n\n\nBefore pushing any changes, it is absolutely vital that you adhere to the following steps:\n\ncommit your changes - so that you have something new to push and they are safe before the next step.\npull (and if necessary reconcile - see the next section below) the latest from the remote repository. This is critical as it ensures that the changes you are pushing are against the latest stage of the repository. Without this step, you might be pushing changes that are based on a stage that is not longer current.\npush your changes\n\n\n\n\n\nTerminalRstudio\n\n\n\ngit pull\n\nAlready up to date.\n\n\n\necho 'Something else' &gt; file4\ngit add file4\ngit commit -m 'Added file4'\n\n[main 27171c8] Added file4\n 1 file changed, 1 insertion(+)\n create mode 100644 file4\n\n\n\ngit push -u origin main\n\nTo /home/runner/tmp/RemoteRepo1\n   db8857f..27171c8  main -&gt; main\nbranch 'main' set up to track 'origin/main'.\n\n\n\n\n\nstart by pulling the latest from the remote repository just incase there has been an change\nclick on the “Create new blank file in the current directory” button and select “Text file” - name it file4\nedit this file by adding the contents Something else\nsave the file\nstage (add) the file\ncommit the change with a message of “Added file4”\npush this commit either by clicking on the green up (push) arrow in the “Review Changes” window or the same arrow in the git tab of the main Rstudio window.\n\n\n\n\n\n\n\n\nNotice how the second (cloned. MyRepo1) repository and the remote repository (RemoteRepo1) are one commit ahead of the original local repository (Repo1). For Repo1 to be in sync with MyRepo1, the original user will have to pull the remote repository changes manually.",
    "crumbs": [
      "Reproducible research",
      "Introduction to git"
    ]
  },
  {
    "objectID": "10_git.html#pulling",
    "href": "10_git.html#pulling",
    "title": "Git and version control",
    "section": "10.3 Pulling",
    "text": "10.3 Pulling\nRetrieving a commit chain (pulling) from a remote repository is superficially the opposite of pushing. Actually, technically it is two actions:\n\na fetch that retrieves the remote information and uses it to create a branch off your local repository (the name of this branch is made from the name of the remote and the branch that was fetched - e.g. origin/master).\na merge that merges this branch into the main repository.\n\nThese actions can be performed individually, however, they are more typically performed together via the git pull command.\nTo illustrate, lets return to being user 1 and we will pull the changes contributed by user 2 in the section above.\n\ngit pull\n\nFrom /home/runner/tmp/RemoteRepo1\n   db8857f..27171c8  main       -&gt; origin/main\nUpdating db8857f..27171c8\nFast-forward\n file4 | 1 +\n 1 file changed, 1 insertion(+)\n create mode 100644 file4\n\n\nThe associated message informs us that upon pulling, a file (file4) has been added. Any conflicts arising from the merging stage of the pull can be resolved in the usual manner of opening the conflicted file(s) making manual edits and then committing the changes.\n\ngit reflog\n\n27171c8 HEAD@{0}: pull: Fast-forward\ndb8857f HEAD@{1}: checkout: moving from Feature to main\n7cf8d04 HEAD@{2}: commit: Feature complete\n874b199 HEAD@{3}: checkout: moving from main to Feature\ndb8857f HEAD@{4}: commit: Bug fix in file1\n9e38af6 HEAD@{5}: checkout: moving from Feature to main\n874b199 HEAD@{6}: commit: New feature\n9e38af6 HEAD@{7}: checkout: moving from main to Feature\n9e38af6 HEAD@{8}: commit: Modified file2, added .gitignore\ncb35bf5 HEAD@{9}: commit: Modified file1 and added file2 (in dir1)\n2b2a239 HEAD@{10}: commit (initial): Initial repo and added file1\n\n\n\ngit log --graph --decorate --oneline --all\n\n* 27171c8 (HEAD -&gt; main, origin/main) Added file4\n* db8857f Bug fix in file1\n| * 7cf8d04 (Feature) Feature complete\n| * 874b199 New feature\n|/  \n* 9e38af6 Modified file2, added .gitignore\n* cb35bf5 Modified file1 and added file2 (in dir1)\n* 2b2a239 Initial repo and added file1\n\n\n\ngit reflog\n\n27171c8 HEAD@{0}: commit: Added file4\ndb8857f HEAD@{1}: clone: from /home/runner/tmp/RemoteRepo1\n\n\n\ngit log --graph --decorate --oneline --all\n\n* 27171c8 (HEAD -&gt; main, origin/main, origin/HEAD) Added file4\n* db8857f Bug fix in file1\n* 9e38af6 Modified file2, added .gitignore\n* cb35bf5 Modified file1 and added file2 (in dir1)\n* 2b2a239 Initial repo and added file1\n\n\n\ngit reflog\ngit log --graph --decorate --oneline --all\n\n* 27171c8 (HEAD -&gt; main) Added file4\n* db8857f Bug fix in file1\n* 9e38af6 Modified file2, added .gitignore\n* cb35bf5 Modified file1 and added file2 (in dir1)\n* 2b2a239 Initial repo and added file1",
    "crumbs": [
      "Reproducible research",
      "Introduction to git"
    ]
  },
  {
    "objectID": "10_git.html#github-as-a-remote-repository",
    "href": "10_git.html#github-as-a-remote-repository",
    "title": "Git and version control",
    "section": "10.4 Github as a remote repository",
    "text": "10.4 Github as a remote repository\nGitHub provides the world’s leading centralized platform for version control, collaboration, and project management, facilitating seamless teamwork, tracking changes, and ensuring the integrity and accessibility of code repositories throughout the software development lifecycle.\nAlthough anyone can explore (read) public repositories on github, only those with github accounts can contribute and collaborate.\n\n10.4.1 Setup Github account\nTo create a free github account:\n\nvisit https://github.com and click “Sign up for github”\nregister by providing your prefered email address, a username and a password when prompted\nto complete the account activation, you will need to verify your details via an email sent to your nominated email address\n\nAs of the start of 2024, github now requires Two-Factor Authentication (2FA) for enhanced security. Whenever you login to github (or are prompted for a password, you will also need to use 2FA. To setup 2FA:\n\nclick on your profile picture in the top right corner.\nselect “Settings” from the dropdown menu.\nselect “Password and authentication” in the left sidebar.\nunder “Two-factor authentication” section, click “Enable”.\nchoose your preferred method (authenticator app or SMS) and follow the prompts to set it up.\n\nPasswords and Two-Factor Authentication (2FA) are used when you (as a human) securely login and interact directly with the GitHub website. However, it is also possible to have other tools (such as git) interact with Github on your behalf via an Application Programming Interfacet (API). Passwords/2FA are not appropriate to authenticate these machine to machine communications. Instead, Github requires the use of a Personal Access Token (PAT). PATs offer a more secure and granular approach, allowing users to control access without exposing their account password.\nTo generate a Personal Access Token (PAT):\n\nclick on your profile picture in the top right corner.\nselect “Settings” from the dropdown menu.\nselect “Developer settings” from the bottom of the left sidebar.\nselect “Personal access tokens” from the left sidebar.\nselect “Tokens (classic)” from the dropdown menu\nclick “Generate new token”\nselect “Generate new token (classic)” from the dropdown menu\nat this point you will likely be prompted for your password\nprovide a “note” - this is more of a short description of what the token is to be used for (in the example below, I have entered “git push/pull” to remind me that this is a simple token for regular push/pull interaction between my local and remote repositories).\n\nYou also need to provide an expiration. Although not secure or recommended, I have selected “No expiration” as I don’t want to have to re-do my PAT across multiple machines too regularly.\nFinally, you also need to indicate scope (what activities you are granting permission for the tools to be able to perform). In this case, I have ticked the “repo” box. This grants general rea/write access to my repositories. I have not granted permission for more administration like activities such as managing teams, deleting repositories, etc - these activities I am happy to perform myself via the website.\nclick “Generate token” and securely copy the generated token. Until this is stored safely (see below) do not close the page, because Github will never show you this PAT again.\n\n\n\n\n\n\n\n\nImportant\n\n\n\n\n\nImportant: Store your PAT safely as you won’t be able to see it again! Ideally, you should store this PAT in a digital wallet. Digital wallets vary according to operating systems. R users might like to use the r function from the asdf package (which you will need to install prior) as follows in order to store the PAT.\nIn an R console, enter:\n\ngitcreds::gitcreds_set()\n\nWhen propted for a password, paste in the copied PAT that hopefully is still in your clipboard - else you might need to re-copy it.\nTo confirm that you have successfully stored your PAT in your wallet, you can:\n\ngitcreds::gitcreds_get()\n\nand confirm that it indicates that there is a hidden password.\n\n\n\n\n\n\n10.4.2 Create remote Github repository\n\nlogin to your Github account\neither:\n\nclick on the “Create new..” button (with the plus sign) to the right of your profile picture in the top right corner and select “New repository” from the dropdown menu\nclick on “Repositories” from the top horizontal menu followed by the big green “New” button\n\n\nfill out the details of the Create a new repository for similar to the following\n\nIn particular:\n\ngive the repository a name. Typically use the same name as you used for the local repository to avoid confusion\nprovide a description. Along with the name, this field is searchable so the more detailed it is, the more likely your repository will be discoverable by others as well as yourself in the future\nindicate the privacy level. This affects whether your repository is discoverable and readable by anyone (public) or just those you invite (private)\nideally, you also want to include a README file and license in your repository. However, if you enable either of these options in the form, Github will bypass providing a screen with some additional instructions that many find useful for linking your local and remote repository. So on this occasion, we will leave these options as they are\n\nclick the “Create repository” button at the bottom of the page\nGithub will present you with the following page:\n\nThis page presents three alternative sets of instructions that you run locally (on your machine) in order to establish a link between the local and remote repository. You need to run the appropriate set of commands in your local terminal\n\nif no local repository exists, follow the first set of instructions\nif you already have a local repository (as is the case with this demonstration), follow the second set of instructions\nif you intend to import a repository from a different versioning system, follow the last set of instructions\n\nonce you have run the above commands locally, you can refresh the Github page and you will be presented with your remote repository. From here you can navigate through your code, manage privileges etc.\n\nIf you would like to allow others to collaborate with you on your repository, then regardless of whether the repository is public or private, you will need to invite them as a collaborator. To do so:\n\nclick on “Settings” from the horizontal menu bar\nclick on “Collaborators” from the left sidebar (you may then be asked to submit your password)\nclick on the green “Add people” button\nin the popup, enter either the username, full name or email address of the person you want to invite to collaborate with you. Once you click the “Select a collaborator above” and select the appropriate candidate, this person will be sent an invite via email.\nnominate the role that this collaborator can assume (e.g. what capacity does the collaborator have to edit, invite others, alter settings, delete the repository etc)\nrepeat steps 3-4 for each additional collaborator you wish to invite",
    "crumbs": [
      "Reproducible research",
      "Introduction to git"
    ]
  },
  {
    "objectID": "20_basic_principles.html",
    "href": "20_basic_principles.html",
    "title": "Basic Principles",
    "section": "",
    "text": "Statistics is a branch of mathematical sciences that relates to the collection, analysis, presentation and interpretation of data and is therefore central to most scientific pursuits. Fundamental to statistics is the concept that samples are collected and statistics are calculated to estimate populations and their parameters.\nStatistical populations can represent natural biological populations (such as the Victorian koala population), although more typically they reflect somewhat artificial constructs (e.g. Victorian male koalas). A statistical population strictly refers to all the possible observations from which a sample (a subset) can be drawn and is the entity about which you wish to make conclusions.\nThe population parameters are the characteristics (such as population mean, variability etc) of the population that we are interested in drawing conclusions about. Since it is usually not possible to observe an entire population, the population parameters must be estimated from corresponding statistics calculated from a subset of the population known as a sample (e.g sample mean, variability etc). Provided the sample adequately represents the population (is sufficiently large and unbiased), the sample statistics should be reliable estimates of the population parameters of interest.\nIt is primarily for this reason that most statistical procedures impose certain sampling and distributional assumptions on the collected data. For example, most statistical tests assume that the observations have been drawn randomly from populations (to maximize the likelihood that the sample will truly represent the population). Additional terminology fundamental to the study of ecological statistics are listed in the following table (in which the examples pertain to a hypothetical research investigation into estimating the protein content of koala milk).\n\n\n\n\n\n\n\n\nTerm\nDefinition\nExample\n\n\n\n\nMeasurement\nA single piece of recorded information reflecting a characteristic of interest (e.g. length of a leaf, pH of a water aliquot mass of an individual, number of individuals per quadrat etc)\nProtein content of the milk of a single female koala\n\n\nObservation\nA single measured sampling or experimental unit (such as an individual, a quadrat, a site etc)\nA small quantity of milk from a single koala\n\n\nPopulation\nAll the possible observations that could be measured and the unit of which wish to draw conclusions about (note a statistical population need not be a viable biological population)\nThe milk of all female koalas\n\n\nSample\nThe (representative) subset of the population that are observed\nA small quantity of milk collected from 15 captive female koalas.Note that such a sample may not actually reflect the defined population. Rather, it could be argued that such a sample reflects captive populations. Nevertheless, such extrapolations are common when field samples are difficult to obtain.\n\n\nVariable\nA set of measurements of the same type that comprise the sample. The characteristic that differs (varies) from observation to observation\nThe protein content of koala milk.\n\n\n\nIn addition to estimating population parameters, various statistical functions (or statistics) are often calculated to express the relative magnitude of trends within and between populations. For example, the degree of difference between two populations is usually described in classic frequentist statistics by a t-statistic.\nAnother important concept in statistics is the idea of probability. The frequentist view of the probability of an event or outcome is the proportion of times that the event or outcome is expected to occur in the long-run (after a large number of repeated sampling events). For many statistical analyses, probabilities of occurrence are used as the basis for conclusions, inferences and predictions.\nConsider the vague research question “How much do Victorian male koalas weigh?”. This could be interpreted as:\n\nHow much do each of the Victorian male koalas weigh individually?\nWhat is the total mass of all Victorian male koalas added together?\nWhat is the mass of the typical Victorian male koala?\n\nArguably, it is the last of these questions that is of most interest. We might also be interested in the degree to which these weights differ from individual to individual and the frequency of individuals in different weight classes.",
    "crumbs": [
      "Statistical principles",
      "Basic principles"
    ]
  },
  {
    "objectID": "20_basic_principles.html#continuous-distributions",
    "href": "20_basic_principles.html#continuous-distributions",
    "title": "Basic Principles",
    "section": "3.1 Continuous distributions",
    "text": "3.1 Continuous distributions\n\n3.1.1 The normal (Gaussian) distribution\nIt has been a long observed mathematical phenomenon that the accumulation of a very large set of independent random influences tend to converge upon a central value (central limit theorem) and that the distribution of such accumulated values follow a specific “bell shaped” curve called a normal or Gaussian distribution. The normal distribution is a symmetrical distribution in which values close to the center of the distribution are more likely and that progressively larger and smaller values are less commonly encountered.\n\nf(x;\\mu,\\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\left(\\frac{x-\\mu}{2\\sigma}\\right)^2}\nAt first, this might appear to be a very daunting formula. It essentially defines the density (frequency) of any value of x. The exact shape of the distribution is determined by just two parameters:\n\n\\mu - the mean. This defines the center of the distribution, the location of the peak.\n\\sigma^2 - the variance (or \\sigma, the standard deviation) which defines the variability or spread of values around the mean.\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant properties of the Gaussian distribution:\n\nThere is no relationship between the distributions mean (location) and variance - they are independent of one another.\nIt is symmetric and unbounded and thus defined for all real numbers in the range of (-\\infty, \\infty).\nGoverned by central limits theorem\n\naverages tend to converge to a central limit\n\n\nAs many biological measurements (such as weights, lengths etc) are influenced by an almost infinite number of factors (many of which can be considered independent and random), many biological variables also follow a Gaussian distribution. The Gaussian distribution is particularly well suited for representing the distribution variables whose values are either\n\nconsiderably larger (or smaller) than zero (e.g. koalas mass) or\nhave no theoretical limits (e.g. difference in masses between sibling fledglings)\n\nEven discrete responses (such as counts that can only logically be positive integers) can occasionally be approximately described by a Gaussian distribution, particularly if either the samples are very large and the values free from boundary conditions (such as being close to a lower limit of 0), or else we are dealing with average counts.\nSince many scientific variables behave according to the central limit theorem, many of the common statistical procedures have been specifically derived for (and thus assume) that the underlying distribution from which the data are drawn is Gaussian. Specifically, parameter estimation, inference and hypothesis tests from simple parametric tests (regression, ANOVA etc) assume that the residuals (stochastic, unexplained components of data) are normally distributed around a mean of zero. The reliability of such tests is dependent on the degree of conformity to this assumption of normality. Likewise, many other statistical elements rely on normal distributions, and thus the normal distribution (or variants thereof) is one of the most important mathematical distributions.\n\n\n3.1.2 Log-normal distribution\nMany biological variables have a lower limit of zero (at least in theory). For example, a koala cannot weigh less than 0kg or there cannot be less than 0mm of rain in a month. Such circumstances can result in asymmetrical distributions that are highly truncated towards the left with a long right tail.\nIn such cases, the mean and median present different values (the latter arguably more reflective of the ‘typical’ value). These distributions can often be described by a log-normal distribution. Furthermore, some variables do not naturally vary on a linear scale. For example, growth rates or chemical concentrations might naturally operate on logarithmic or exponential scales. Consequently, when such data are collected on a linear scale, they might be expected to follow a non-normal (perhaps log-normal) distribution.\n\nf(x;\\mu,\\sigma) = \\frac{1}{x\\sigma\\sqrt{2\\pi}} e^{-\\left(\\frac{ln x-\\mu}{2\\sigma^2}\\right)^2}\nAs with the Gaussian distribution, the exact shape of the log-normal distribution is determined by just two parameters:\n\n\\mu - the mean. This defines the center of the distribution, the location of the peak.\n\\sigma^2 - the variance (or \\sigma, the standard deviation) which defines the variability or spread of values around the mean.\n\nHowever, \\mu and \\sigma^2 are the mean and variance of ln(x) rather than x.\n\n\n\n\n\n\n\n\n\n\n\n\nImportant properties of the log-normal distribution:\n\nThe variance is related (proportional) to the mean (\\sigma^2 \\sim\n\\mu^2)\nThe log-normal distribution is skewed to the right as a result of being bounded at 0, yet unbounded to the right (0, \\infty)\nAlso governed by central limits theorem except that it describes the distribution of values that are the product (rather than sum) of a large number of independent random factors.\n\n\n\n3.1.3 t-distribution\nThe t-distribution, also known as the Student’s t-distribution, is a probability distribution that is similar to the standardised normal distribution (mean of 0, standard deviation of 1) however it is better suited for smaller sample sizes. It is characterized by its bell-shaped curve and heavier tails, making it suitable for modeling data that deviates from normality. The t-distribution is often employed in hypothesis testing and confidence interval estimation when dealing with small sample sizes or when the population standard deviation is unknown. It provides a robust alternative to the normal distribution in situations where the underlying data exhibit skewness or constraints.\n\n\nf(t; \\mu, \\nu) = \\frac{\\Gamma(\\frac{\\nu + 1}{2})}{\\sqrt{\\pi \\nu} \\Gamma(\\frac{\\nu}{2})} \\left( 1 + \\frac{(t - \\mu)^2}{\\nu} \\right)^{-\\frac{\\nu + 1}{2}}\n\nWhere:\n\nf(t; \\mu, \\nu) represents the probability density function of the t-distribution.\n\\mu is the location parameter (mean).\n\\nu is the degrees of freedom parameter, which controls the shape of the distribution (fatter tails with lower \\nu).\n\\Gamma(⋅) is the gamma function.\n\nThis formula resembles the Gaussian distribution but includes an additional term involving the degrees of freedom and a different power in the exponent. This difference reflects the heavier tails of the t-distribution compared to the bell-shaped normal distribution. This formula describes the shape of the t-distribution, which converges to the standard normal distribution as the degrees of freedom increase.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.1.4 Gamma distribution\nThe Gamma distribution describes the distribution of waiting times until a specific number of independent events (typically deaths) have occurred. For example, if the average mortality rate is one individual per five days (rate=1/5 or scale=5), then a Gamma distribution could be used to describe the distribution of expected waiting time before 10 individuals were dead.\n\nThere are two parameterizations of the Gamma distribution\n\nin terms of shape (k) and scale (\\theta)\n\n\nf(x;k,\\theta) = \\frac{1}{\\theta^k}\\frac{1}{\\gamma(k)}x^{k-1}e^{-\\frac{x}{\\theta}}\\\\\n\\text{for}~x\\gt 0~\\text{and}~k,\\theta\\gt 0\n\n\nin terms of shape (\\alpha) and rate (\\beta)\n\n  f(x;\\alpha,\\beta) = \\beta^\\alpha\\frac{1}{\\gamma(\\alpha)}x^{\\alpha-1}e^{-\\beta x}\\\\\n  \\text{for}~x\\gt 0~\\text{and}~\\alpha,\\beta\\gt 0\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nIn addition to being used to describe the distribution of waiting times, the gamma distribution can also be used as an alternative to the normal distribution when data (residuals) are skewed with a long right tail, such as when there is a relationship between mean and variance. When such data are modeled with a normal distribution, illogical negative predicted values can occur. Such values are not possible from a Gamma distribution.\nThe Gamma distribution is also an important conjugate prior for the precision (variance) of a normal distribution in Bayesian modeling. Important properties of the Gamma distribution:\n\nThe shape parameter defines the number of events (for example, 10 deaths) and can technically be any positive number.\n\nshape values less than 1, the gamma distribution has a mode of 0\nshape values equal to 1, the gamma distribution is equivalent to the exponential distribution\nshape values greater than 1, the distribution becomes increasingly more symmetrical and approaches a normal distribution when the shape parameter is large.\n\nThe scale or rate (rate=1/scale) parameter defines how often (scale) or the rate at which events are expected to occur\nThe variance is proportional to the mean (variance=\\frac{scale}{mean}, variance=\\frac{mean^2}{shape})\n\n\n\n3.1.5 Uniform distribution\nThe uniform distribution describes a square distribution within a specific range.\n\nf(x;a,b) = \\begin{cases}\n  \\frac{1}{b-a} & \\text{for } a \\leq x \\geq b,\\\\[1em]\n    0       & \\text{for } x \\lt a \\text{ or } x \\gt b\n\\end{cases}\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant properties of the uniform distribution:\n\nHas a constant probability density within the range a\\le x\\ge b of \\frac{1}{b-a} and zero outside of this range\nWhilst this distribution is rarely employed in frequentist statistics, it is occasionally used as an improper prior distribution in Bayesian modeling.\n\n\n\n3.1.6 Exponential distribution\nThe exponential distribution describes the distribution of waiting times for the occurrence a single discrete event (such as an individual death) given a constant rate (probability of occurrence per unit of time) - for example, describing longevity or the time elapsed between events (such as whale sightings). It is also useful for describing the distribution of measurements that naturally attenuate (exponentially) such as light levels penetrating to increasing water depths.\n\nf(x;\\lambda) = \\lambda e^{-\\lambda x} The uniform distribution is defined by a single parameter:\n\n\\lambda - the rate. The rate at which the event is expected to occur. The larger the rate, the steeper the curve.\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant properties of the uniform distribution:\n\nIt is bounded by 0 on the left and limitless on the right (0, \\infty).\nThe mean and variance are both related to the rate (variance=\\frac{1}{\\lambda^2}, mean=\\frac{1}{\\lambda})\n\n\n\n3.1.7 Beta distribution\nThe beta distribution describes the probability of success in a binomial trial is the only continuous distribution defined within the range that is bound at both ends (0-1). As it operates in the range of 0-1, it is ideal for modeling proportions and percentages. However, it is also useful for modeling other continuous quantities on a finite scale. The values are transformed (see Transformations) from the arbitrary finite scale to the 0-1 scale, modeling with a beta distribution and finally the parameters are back-transformed into the original scale.\n\nf(x;a,b) = \\frac{\\Gamma(a+b)}{\\Gamma(a)\\Gamma(b)}x^{a-1}(1-x)^{b-1}\nThe beta distribution is defined by two shape parameters:\n\na - shape parameter 1. Number of successes in binomial trial (a-1)\nb - shape parameter 1. Number of successes in binomial trial (b-1)\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe beta distribution is also a conjugate prior for binomial, Bernoulli and geometric distributions. Important properties of the beta distribution:\n\nIt is bounded by 0 on the left and limitless on the right (0, \\infty).\nWhen a=b, the distribution is symmetric about x=0.5\nWhen a=b=1, the distribution is a uniform distribution with a=0 and b=1.\nThe location of the peak shifts towards 0 as a&lt;b and shifts towards 1 as a&gt;b.\nThe variance of the distribution is inversely proportional to the total of a+b (the number of trials).",
    "crumbs": [
      "Statistical principles",
      "Basic principles"
    ]
  },
  {
    "objectID": "20_basic_principles.html#discrete-distributions",
    "href": "20_basic_principles.html#discrete-distributions",
    "title": "Basic Principles",
    "section": "3.2 Discrete distributions",
    "text": "3.2 Discrete distributions\n\n3.2.1 Binomial distribution\nThe binomial distribution describes the number of ‘successes’ out of a total of n independent trials each with a set probability. On any given trial, only two possible outcomes (binary) are possible (0 and 1) - that is it is a Bernoulli trial. Importantly, the binomial distribution is bounded at both ends - zero to the left and the trial size on the right. Typical binomial include:\n\nthe number of surviving individuals from a pool of individuals\nthe number of infected individuals from a pool of individuals\nthe number of items of a particular class (e.g. males) from a pool of items\n\n\nf(x;n,p) = \\left(\\begin{array}{c}\nn\\\\x\n\\end{array}\\right)p^{x}(1-p)^{n-x}\nThe binomial distribution is defined by two shape parameters:\n\nn - the total number of trials\np - the probability of success on any given trial. Defined as any real number between 0 and 1.\n\nThe \\left(\\begin{array}{c} n\\\\x \\end{array}\\right) component is a normalizing constant that defines the number of ways of drawing x items out of n trials and also ensures that all probabilities add up to 1.\n\n\n\n\n\n\n\n\n\n\n\n\nImportant properties of the binomial distribution:\n\nIt is bounded by 0 on the left and by n (the number of trials/individuals/quadrats etc) on the right (0, n).\nVariance is proportional to n and related to the mean in that the larger the sample size, the larger the variance.\nVariance is greatest when p=0.5 and decreases as p approaches 0 or 1.\nWhen n is large and p is away from 0 or 1, the binomial distribution approaches a normal distribution\nWhen n is large and p is small, the binomial distribution approaches a Poisson distribution/li&gt;\n\n\n\n3.2.2 Poisson distribution\nThe poisson distribution describes the number (counts) of independent discrete items or events (individuals, times, deaths) recorded for a given effort. The poisson distribution is defined by a single parameter (\\lambda) that describes the expected count (mean) as well as the variance in count. The poisson distribution is bounded at the lower end by zero, yet theoretically unbounded at the upper end (0,\\infty).\nThe poisson distribution is particularly appropriate for modeling count data as they are always truncated at zero, have no upper limit and tend to get more variable with increasing mean.\n\nf(x;\\lambda) = \\frac{e^{-\\lambda}\\lambda^x}{x!}\nThe poisson distribution is defined by a single parameter:\n\n\\lambda - the expected value\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant properties of the binomial distribution:\n\nIt is bounded by 0 on the left and unbounded on the right (0, \\infty).\nMean and variance are both equal to \\lambda.\nAssumes that the ratio of variance to mean (Dispersion) is 1 (D=\\frac{var}{mean}=1)\nWhen \\lambda is large, the binomial distribution approaches a normal distribution",
    "crumbs": [
      "Statistical principles",
      "Basic principles"
    ]
  },
  {
    "objectID": "20_basic_principles.html#negative-binomial-distribution",
    "href": "20_basic_principles.html#negative-binomial-distribution",
    "title": "Basic Principles",
    "section": "3.3 Negative binomial distribution",
    "text": "3.3 Negative binomial distribution\nThe negative binomial distribution describes the expected number of failures out of a sequence of n independent trials before a success is obtained each with a set probability (typically 0.5). The negative binomial is a useful alternative to the poisson distribution for modeling count data for which the variance is greater than the mean (e.g. overdispersed, particularly when caused by a heterogeneous/patchy/clumped response). The negative binomial distribution is bounded at the lower end by zero, yet theoretically unbounded at the upper end (0,\\infty).\n\nThere are two parameterizations of the Gamma distribution\n\nin terms of the size (n) and probability (p)\nf(x;n,p) = \\frac{(n+x-1)!}{(n-1!)x!}p^{n}(1-p)^x\n\nn - the number of successes to occur before stopping the count of failures. n acts as a stopping point in that the number of failures are counted until n successes are encountered.\np - the probability of success of any single trial.\nin terms of mean \\mu=n(1-p)/p) and overdispersion parameter or scaling factor (\\omega). This parameterization is more meaningful in ecology.\nf(x;\\mu,\\omega) =\n\\frac{\\Gamma(\\omega+x)}{\\Gamma(\\omega)x!}\\frac{(\\mu^x\\omega^\\omega}{(\\mu+\\omega)^{\\mu+\\omega}}\n\n\\mu - the mean (expected number of failures).\n\\omega - the dispersion or scaling factor.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant properties of the negative binomial distribution:\n\nIt is bounded by 0 on the left and unbounded on the right (0, \\infty).\nThe variance is related to the mean (\\sigma^2=\\mu+\\mu^2/\\omega) - variance increases with increasing mean.",
    "crumbs": [
      "Statistical principles",
      "Basic principles"
    ]
  },
  {
    "objectID": "20_basic_principles.html#measures-of-location",
    "href": "20_basic_principles.html#measures-of-location",
    "title": "Basic Principles",
    "section": "5.1 Measures of location",
    "text": "5.1 Measures of location\nMeasures of location describe the center of a distribution and thus characterize the typical value of a population. There are many different measures of location (see Table below), all of which yield identical values (in the center of the distribution) when the population (and sample) follows an exactly symmetrical distribution. Whilst the mean is highly influenced by unusually large or small values (outliers) and skewed distributions, the median is more robust. The greater the degree of asymmetry and outliers, the more disparate the different measures of location.\n\n\n\n\n\n\n\n\nParameter\nDescription\nR syntax\n\n\n\n\nEstimates of location\n\n\n\n\nArithmetic mean (\\mu)\nthe sum of the values divided by the number of values (n)\nmean(x)\n\n\nTrimmed mean\nthe arithmetic mean calculated after a fraction (typically 0.05 or 5\\%) of the lower and upper values have been discarded\nmean(x, trim=0.05)\n\n\nWinsorized mean\nthe arithmetic mean is calculated after the trimmed values are replaced by the upper and lower trimmed quantiles\nlibrary(psych)&lt;br&gt;winsor(x, trim=0.05)\n\n\nMedian\nthe middle value\nmedian(x)\n\n\nMinimum, maximum\nthe smallest and largest values\nmin(x), max(x)\n\n\nEstimates of spread\n\n\n\n\nVariance (\\sigma^2)\nthe average deviation (difference) of observations from the mean\nvar(x)\n\n\nStandard deviation (\\sigma)\nsquare-root of the variance\nsd(x)\n\n\nMedian average deviation\nthe median difference of observations from the median value\nmad(x)\n\n\nInter-quartile range\nthe difference between the 75% and 25% ranked observations\nIQR(x)\n\n\nPrecision and confidence\n\n\n\n\nStandard error \\bar{y}(s_{\\bar{y}})\nthe precision of an estimate \\bar{y}\nsd(x)/sqrt(length(x))\n\n\n95% confidence interval of \\mu\nthe interval with a 95% probability of containing the true mean\nlibrary(gmodels)&lt;br&gt;ci(x)",
    "crumbs": [
      "Statistical principles",
      "Basic principles"
    ]
  },
  {
    "objectID": "20_basic_principles.html#measures-of-dispersion-and-variability",
    "href": "20_basic_principles.html#measures-of-dispersion-and-variability",
    "title": "Basic Principles",
    "section": "5.2 Measures of dispersion and variability",
    "text": "5.2 Measures of dispersion and variability\nIn addition to having an estimate of the typical value (center of a distribution), it is often desirable to have an estimate of the spread of the values in the population. That is, do all Victorian male koalas weigh the same or do the weights differ substantially?\nIn its simplest form, the variability, or spread, of a population can be characterized by its range (difference between maximum and minimum values). However, as ranges can only increase with increasing sample size, sample ranges are likely to be a poor estimate of population spread.\nVariance (s^2) describes the typical deviation of values from the typical (mean) value: s^2=\\sum{\\frac{(y_i-\\bar{y})^2}{n-1}} Note that by definition, the mean value must be in the center of all the values, and thus the sum of the positive and negative deviations will always be zero. Consequently, the deviances are squared prior to summing. Unfortunately, this results in the units of the spread estimates being different to the units of location. Standard deviation (the square-root of the variance) rectifies this issue.\nNote also, that population variance (and standard deviation) estimates are calculated with a denominator of n-1 rather than n. The reason for this is that since the sample values are likely to be more similar to the sample mean (which is of course derived from these values) than to the fixed, yet unknown population mean, the sample variance will always underestimate the population variance. That is, the sample variance and standard deviations are biased estimates of the population parameters. Ideally, the mean and variance should be estimated from two different independent samples. However, this is not practical in most situations. Division by n-1 rather than n is an attempt to partly offset these biases.\nThere are more robust (less sensitive to outliers) measures of spread including the inter-quartile range (difference between 75% and 25% ranked observations) and the median absolute deviation (MAD: the median difference of observations from the median value).",
    "crumbs": [
      "Statistical principles",
      "Basic principles"
    ]
  },
  {
    "objectID": "20_basic_principles.html#measures-of-the-precision-of-estimates---standard-errors-and-confidence-intervals",
    "href": "20_basic_principles.html#measures-of-the-precision-of-estimates---standard-errors-and-confidence-intervals",
    "title": "Basic Principles",
    "section": "5.3 Measures of the precision of estimates - standard errors and confidence intervals",
    "text": "5.3 Measures of the precision of estimates - standard errors and confidence intervals\nSince sample statistics are used to estimate population parameters, it is also desirable to have a measure of how good the estimates are likely to be. For example, how well the sample mean is likely to represent the true population mean. The proximity of an estimated value to the true population value is its accuracy.\nClearly, as the true value of the population parameter is never known (hence the need for statistics), it is not possible to determine the accuracy of an estimate. Instead, we measure the precision (repeatability, consistency) of the estimate. Provided an estimate is repeatable (likely to be obtained from repeated samples) and that the sample is a good, unbiased representative of the population, a precise estimate should also be accurate.\nStrictly, precision is measured as the degree of spread (standard deviation) in a set of sample statistics (e.g. means) calculated from multiple samples and is called the standard error. The standard error can be estimated from a single sample by dividing the sample standard deviation by the square-root of the sample size (\\frac{\\sigma}{\\sqrt{n}}). The smaller the standard error of an estimate, the more precise the estimate is and thus the closer it is likely to approximate the true population parameter.\nThe central limit theorem (which predicates that any set of averaged values drawn from an identical population will always converge towards being normally distributed) suggests that the distribution of repeated sample means should follow a normal distribution and thus can be described by its overall mean and standard deviation (=standard error). In fact, since the standard error of the mean is estimated from the same single sample as the mean, its distribution follows a special type of normal distribution called a t-distribution.\nIn accordance to the properties of a normal distribution (and thus a t-distribution with infinite degrees of freedom), 68.27% of the repeated means fall between the true mean and \\pm one sample standard error (see Figure bellow). Put differently, we are 68.27% percent confident that the interval bound by the sample mean plus and minus one standard error will contain the true population mean. Of course, the smaller the sample size (lower the degrees of freedom), the flatter the t-distribution and thus the smaller the level of confidence for a given span of values (interval).\nThis concept can be easily extended to produce intervals associated with other degrees of confidence (such as 95%) by determining the percentiles (and thus number of standard errors away from the mean) between which the nominated percentage (e.g. 95%) of the values lie. The 95% confidence interval is thus defined as:\nP\\{\\bar{y}-t_{0.05(n-1)}s_{\\bar{y}}\\le\\mu\\le\\bar{y}+t_{0.05(n-1)}s_{\\bar{y}}\\}\nwhere \\bar{y} is the sample mean, s_{\\bar{y}} is the standard error, t_{0.05(n-1)} is the value of the 95% percentile of a distribution with n-1 degrees of freedom, and \\mu is the unknown population mean.\nFor a 95% confidence interval, there is a 95% probability that the interval will contain the true mean. Note, this interpretation is about the interval, not the true population value, which remains fixed (albeit unknown). The smaller the interval, the more confidence is placed in inferences about the estimated parameter.\n\n\n\n\n\n\n\n\n\nThe left hand figure above illustrates a Normal distribution displaying percentage quantiles (grey) and probabilities (areas under the curve) associated with a range of standard deviations beyond the mean. The right hand figure displays 20 possible 95% confidence intervals from 20 samples (n=30) drawn from the one population. Bold intervals are those that do not include the true population mean. In the long run, 5% of such intervals will not include the population mean (\\mu).",
    "crumbs": [
      "Statistical principles",
      "Basic principles"
    ]
  },
  {
    "objectID": "21_statistical_philosophies.html",
    "href": "21_statistical_philosophies.html",
    "title": "Different statistical philosophies",
    "section": "",
    "text": "The two major opposing philosophies (frequentist and Bayesian) differ in how they consider population parameters and thus how interpretations are framed.\n\n1 Frequentist (classical)\nVariation in observed data allows the long-run frequency of different outcomes to be approximated (we can directly measure the likelihood of obtaining the data given a certain null hypothesis).\nIn the frequentist framework, population parameters (characteristics of the population) are considered fixed quantities (albeit unknown). That is, there is one true state (e.g. mean) per population. The observed data (and its characteristics - sample mean) however, represents just one possible outcome that could be collected from this true state. Multiple simultaneous data collection experiments would yield different outcomes. This notion that there is a single true population parameter and a distribution of possible sample outcomes is the basis for a framework that conveniently allows a logical and relatively simple algebraic link between data, hypotheses and probability.\nTraditional (frequentist) statistics focus on determining the probability of obtaining the collected data, given a set of parameters (hypothesis). In probability notation, this can be written as: P(D|H) where D represents the collected data and H represents the set of population parameters or hypothese(s).\nThe process of inference (hypothesis testing) is based on approximating long-run frequencies (probability) of repeated sampling from the stationary (non-varying) population(s). This approach to statistics provides relatively simple analytical methods to objectively evaluate research questions (albeit with rigid underlying assumptions) and thus gained enormous popularity across many disciplines.\nThe price for mathematical convenience is that under this philosophy, the associated statistical interpretations and conclusions are somewhat counter-intuitive and not quite aligned with the desired manner in which researchers would like to phrase research conclusions. Importantly, conclusions are strictly about the data, not the parameters or hypotheses.\nInference (probability and confidence intervals) is based on comparing the one observed outcome (parameter estimate) to all other outcomes that might be expected if the null hypothesis really was true. Moreover, as probabilities of point events always equal zero (e.g. the probability of obtaining a mean of 13.5646 is infinitely small), probability (the p-value) is calculated as the probability of obtaining data that is at least as extreme as that observed.\nAs a result of the somewhat obtuse association between hypotheses and the collected data, frequentist statistical outcomes (particularly p-values and confidence intervals) are often misused and misinterpreted. Of particular note;\n\nA frequentist p-value is the probability of rejecting the null hypothesis, it is not a measure of the magnitude of an effect or the probability of a hypothesis being true.\nGiven that a statistical null hypothesis can never actually be true (e.g. a population slope is never going to be exactly zero), a p-value is really just a measure of whether the sample size is big enough to detect a non-zero effect.\nA 95% confidence interval defines the proportion of repeated samples (95/100) of a particular spread of values that are likely to contain the true mean. It is not a range of values for which you are 95% confident the true mean lies between.\n\n\n\n2 Bayesian\nBy contrast, the Bayesian framework considers the observed data to be fixed and a truth (a real property of a population) whilst the population parameters (and therefore also the hypotheses) are considered to have a distribution of possible values. Consequently, inferential statements can be made directly about the probability of hypotheses and parameters. Furthermore, outcomes depend on the observed data and not other more extreme (unobserved) data.\nBound up in this framework is the manner in which Bayesian philosophy treats knowledge and probability. Rather than being a long-run frequency of repeated outcomes (which never actually occur) as it is in frequentist statistics, probability is considered a degree of belief in an outcome. It naturally acknowledges that belief is an iterative process by which knowledge is continually refined on the basis of new information. In the purest sense, our updated (posterior) belief about a parameter or hypothesis is the weighted product of our prior belief in the parameter or hypothesis and the likelihood of obtaining the data given the parameter or hypothesis, such that our prior beliefs are reevaluated in light of the collected data.\nThe fundamental distinction between Bayesian and frequentist statistics is the opposing perspectives and interpretations of probability. Whereas, frequestists focus on the probability of the data given the (null) hypothesis (P(D|H)), Bayesians focus on the probability of the hypothesis given the data (P(H|D)).\nWhilst the characteristics of a single sample (in particular, its variability) permit direct numerical insights (likelihood) into the characteristics of the data given a specific hypothesis (and hence frequentist probability), this is not the case for Bayesian hypotheses.\nNevertheless, further manipulations of conditional probability rules reveal a potential pathway to yield insights about a hypothesis given a collected sample (and thus Bayesian probability).\nRecall the conditional probability law:\nP(A|B) = \\frac{P(AB)}{P(B)}~ \\text{and equivalently, } P(B|A) = \\frac{P(BA)}{PAB)}\nThese can be transformed to express in terms of P(AB):\nP(AB) = P(A|B)\\times P(B), \\text{ and } P(BA) = P(B|A)\\times P(A)\nSince P(AB) is the same as P(BA),\n\\begin{align*} P(A|B)\\times P(B) &= P(B|A)\\times\nP(A)\\\\ P(A|B) &= \\frac{P(B|A)\\times P(A)}{P(B)} \\end{align*}\nThis probability statement (the general Bayes’ rule) relates the conditional probability of outcome A given B to the conditional probability of B given A. If we now substitute outcome A for H (hypothesis) and outcome B for D (data), it becomes clearer how Bayes’ rule can be used to examine the probability of a parameter or hypothesis given a single sample of data.\nP(H|D) =\n\\frac{P(D|H)\\times P(H)}{P(D)}\nwhere P(H|D) is the posterior probability of the hypothesis (our beliefs about the hypothesis after inspiration from the newly collected data), P(D|H) is the likelihood of the data, P(H) is the prior probability of the hypothesis (our prior beliefs about the hypotheses before collecting the data) and P(D) is the normalizing constant.\nBayesian statistics offer the following advantages;\n\nInterpretive simplicity:\n\nSince probability statements pertain to population parameters (or hypotheses), drawn conclusions are directly compatible with research or management questions.\n\nComputationally robustness:\n\nDesign balance (unequal sample sizes) is not relevant\nMulticollinearity is not relevant\nThere is no need to have expected counts greater than 5 in contingency analyses\n\nInferential flexibility:\n\nThe stationary joint posterior distribution reflects the relative credibility of all combinations of parameter values and from which we can explore any number and combination of inferences. For example, because this stationary joint posterior distribution never changes no matter what perspective (number and type of questions) it is examined from, we can explore all combinations of pairwise comparisons without requiring adjustments designed to protect against inflating type II errors. We simply derive samples of each new parameter (e.g. difference between two groups) from the existing parameter samples thereby obtaining the posterior distribution of these new parameters.\nAs we have the posterior density distributions for each parameter, we have inherent credible intervals for any parameter. That is we can state the probability that a parameter values lies within a particular range. We can also state the probability that two groups are different.\nWe get the covariances between parameters and therefore we can assess interactions in multiple regression\n\n\nDespite all the merits of the Bayesian philosophy (and that its roots pre-date the frequentist movement), widespread adoption has been hindered by two substantial factors;\n\nProponents have argued that the incorporation (indeed necessity) of prior beliefs introduces subjectivity into otherwise objective pursuits. Bayesians however, claim that considering so many other aspects of experimental design (choice of subjects and treatment levels etc) and indeed the research questions asked are discretionary, objectivity is really a fallacy. Rather than being a negative, true Bayesians consider the incorporation of priors to be a strength. Nevertheless, it is possible to define vague or non-informative priors\nExpressing probability as a function of the parameters and hypotheses rather than as a function of the data relies on applying an extension of conditional probability (called Bayes’ rule) that is only tractable (solved with simple algebra) for the most trivial examples. Hence it is only through advances in modern computing power that Bayesian analyses have become feasible. For example, when only a discrete set of hypotheses are possible, then P(D) essentially becomes the sum of all these possible scenarios (each P(DH_i)):\nP(D) = \\sum{P(D|H)P(H)}\nP(H|D) = \\frac{P(D|H)\\times P(H)}{\\sum{P(D|H_i)P(H)}}\nHowever, if there are potentially an infinite number of possible hypotheses (the typical case, at least in theory) this sum is replaced with an integral:\nP(H|D) = \\frac{P(D|H)\\times P(H)}{\\int P(D|H_i)P(H)dH}\n\nWhilst it is not always possible to integrate over all the possible scenarios, modern computing now permits more brute force solutions in which very large numbers of samples can be drawn from the parameter space which in turn can be used to recreate the exact posterior probability distribution.",
    "crumbs": [
      "Statistical principles",
      "Statistical philosophies"
    ]
  },
  {
    "objectID": "22_estimation.html",
    "href": "22_estimation.html",
    "title": "Estimation and inference",
    "section": "",
    "text": "Least squares (LS) parameter estimation is achieved by simply minimizing the (sum) overall (square) differences between the observed sample values and those values calculated (predicted) from an equation containing those estimated parameter(s). For example, the least squares estimate of the population mean (\\mu) is a value that minimizes the differences between the sample values and this estimated mean.\n\n\n\n\n\n\n\n\n\nIt is clear in the figure above that the sum of the (squared) length of the grey lines that join the observed data to the candidate parameter value of 7 is greater than the sum of the (squared) length of the black lines that join the observed data to the candidate parameter value of 10.\nAs the differences between observed and predicted are squared (otherwise they would sum to 0), it is assumed that these differences (residuals) are symmetrically distributed around 0. For this reason, they are almost exclusively aligned to the Normal (Gaussian) distribution (as nearly every other useful sampling distribution has the potential to be asymmetric and/or invalid for negative values).\nLeast squares estimation has no inherent basis for testing hypotheses or constructing confidence. Instead, inference is achieved via manipulating the properties of the residual variance.",
    "crumbs": [
      "Statistical principles",
      "Estimation and inference"
    ]
  },
  {
    "objectID": "22_estimation.html#markov-chain-monte-carlo-sampling",
    "href": "22_estimation.html#markov-chain-monte-carlo-sampling",
    "title": "Estimation and inference",
    "section": "3.1 Markov Chain Monte Carlo sampling",
    "text": "3.1 Markov Chain Monte Carlo sampling\nMarkov Chain Monte Carlo (MCMC) algorithms provide a powerful, yet relatively simple means of generating samples from high-dimensional distributions. Whilst there are numerous specific MCMC algorithms, they all essentially combine a randomization routine (Monte Carlo component) with a routine that determines whether or not to accept or reject randomizations (Markov Chain component) in a way that ensures that the samples are drawn from multidimensional space proportional to their likelihoods such that the distribution of these samples (posterior probability distribution) reflects their likelihood in high-dimensional space. Given a sufficiently large number of simulations, the resulting samples should exactly describe the target probability distribution and thereby allow the full range of parameter characteristics to be derived.\nTo illustrate the mechanics of MCMC sampling, I will contrive a very simple scenario in which we have two parameters (\\alpha and \\beta). I will assume infinitely vague priors on both parameters and thus the density function from which to draw samples will simply be the likelihood density function for these parameters. For simplicity sake, this likelihood function will be a multivariate normal distribution for the two parameters with values of \\alpha=0 (var=1) and \\beta=5 (var=2). The following two graphics provide two alternative views (perspective view and contour view) of this likelihood density function.\n\n\n\n\n\n\n\n\n\nThe specific MCMC algorithm that I will describe is the Metropolis-Hastings algorithm and I will do so as much as possible via visual aids. Initially a location (coordinates) in multi-dimensional space (in this case 2-D) corresponding to a particular set of parameter values is chosen as the starting location. We will start with the coordinates (α=0.5, β=-0.5), yet make the point that if we collect enough samples, the importance of the initial starting configuration will diminish.\n\n\n\n\n\n\n\n\n\nThe values of the parameters sampled from this joint distribution are then stored.\nNow lets move to another random sampling location. To do so, we will move a random multivariate distance and direction from the current location based on the coordinates of a single random location drawn from a multivariate normal distribution centered around zero. The ratio of probabilities (heights on the perspective diagram) corresponding to the new and previous locations is then calculated and this ratio effectively becomes the probability of accepting the new location. For example, the probability associated with the initial location was approximately 0.037 and the probability associated with the new location is 0.019.\nThe probability of moving is therefore 0.019/0.037=0.513. There is just over 50% chance of moving. A random number between 0 and 1 is then drawn from a uniform distribution. If this number is greater than the ratio described above, the new location is rejected and the previous location is resampled. However, if the number is less than the ratio, then the new location is accepted and parameter values are stored from this new location.\n\n\n\n\n\n\n\n\n\nThe sampling chain continues until a predefined number of iterations have occurred.\n\n\n\n\n\n\n\n\n\nWith 1000 samples collected\n\n\n\n\n\n\n\n\n\nThe collected samples, in this case 1000 samples for \\alpha and 1000 for \\beta, can then be used to calculate a range of characteristics.\n\ncolnames(params)&lt;-c(\"a\",\"b\")\napply(params,2,mean)\n\n          a           b \n-0.03090054  5.22433258 \n\napply(params,2,sd)\n\n        a         b \n0.8792583 1.4364185 \n\n\nRecall that the true parameter means and standard deviations were (\\alpha=0, var=1 \\Rightarrow sd=1; \\beta=5, var=2 \\Rightarrow\nsd=1.414). Whilst the means and standard deviations from the simulated samples are similar to these values, they are in no way identical. There are numerous potential reasons for this. We will now explore these. It is worth noting at this point that we are in the unique position of being able to identify that the parameter estimates do not yet match closely the true parameter values. Normally this would not be case. However, there are various diagnostics that we can employ to help us evaluate whether the chain has yielded a stationary posterior distribution or not.\n\nPerhaps the posterior distribution (represented by the red dots) had not yet stabilized. That is, perhaps the shape of the posterior distribution was still changing slightly with each additional sample collected. The chain had not yet completely sampled the entire surface in approximate proportion to the densities. If so, then collecting substantially more samples could address this. Lets try 10,000.\n\n\n\n\n\n\n\n\n\n\n\n\ncolnames(params)&lt;-c(\"a\",\"b\")\napply(params,2,mean)\n\n          a           b \n-0.02836663  4.99912614 \n\napply(params,2,sd)\n\n       a        b \n1.010878 1.422975 \n\n\nThe resulting estimates are now very accurate. Given sufficient independent MCMC samples, exact solutions can be obtained (c.f. frequentist approximates that become increasingly more dubious with increasing model complexity and diminishing levels of replication).\nThe “sharper” the features (and more isolated islands) in the multidimensional profile, the more samples will be required in order to ensure that the Markov chain traverses (mixes across) all of the features. One of the best ways of evaluating whether or not sufficient samples have been collected is with a trace plot. Trace plots essentially display the iterative history of MCMC sequences. Ideally, the trace plot should show no trends, just consistent random noise around a stable baseline. Following are three trace plots that depict an adequate number samples, inadequate number of samples and insufficient mixing across all features.\n\n\n\n\n\n\n\n\n\n\n\nThe drawback of a Markov Chain is that the samples are not independent of one another. In a sense, its strength (the properties that allow it to simulate in proportion to the density) can also be a weakness in that consecutive samples are likely to be very similar (non-independent) to one another. Summary statistics based on non-independent samples tend to be biased and can be unrepresentative.\n\n\nIt turns out that there is a very simple remedy for this however. Rather than store every single sample, we can thin the sample set by skipping over (not storing) some of the samples. For example, we could nominate to store every second, or fifth or tenth etc sample. The resulting samples will thence be more independent of one another. Note however, that the sample will be smaller by a factor of the thinning value and thus it may be necessary to perform more iterations.\nTo illustrate autocorrelation and thinning, we will run the MCMC simulations twice, the first time with no thinning and a second time with a thinning factor of 10. Non-thinned (stored) samples are depicted as circles with black borders. Associated with each chain of simulations, is an autocorrelation function (ACF) plot which represents the degree of correlation between all pairs of samples separated by progressively larger lags (number of samples).\nThinning factor = 1\n\n\n\n\n\n\n\n\n\nThinning factor = 10\n\n\n\n\n\n\n\n\n\nFor the un-thinned samples, the autocorrelation at lag 1 (adjoining pairs of samples) is over 0.8 (80%) indicating a very high degree of non-independence. Ideally, we would like to aim for a lag 1 autocorrelation value no greater than 0.1. According to the ACF plot associated with the un-thinned simulation chain, to achieve a lag 1 autocorrelation value of no greater than 0.1 would require a sample separation (thinning) of greater than 10. Indeed, the second simulation chain had a thinning factor of 10 (only retained every 10th sample) and still has a lag 1 autocorrelation value greater than 0.1 (although this is may now be because it is based on a much smaller sample size). To be sure, we could re-simulate the chain again with a 15-fold thinning but with 10 times more iterations (a total of 10,000).\n\n\n\n\n\n\n\n\n\n\n\nAlthough the importance of the starting configuration (initial coordinates) should eventually diminish as the stochastic walk traverses more and more of the profile, for shorter chains, the initial the initial path traversed is likely to exert a pull on the sample values. Of course, substantially increasing the number of iterations of the chain will substantially increase the time it takes to run the chain.\n\n\nHence, it is common to define a burnin interval at the start of the chain. This defines the number of initial iterations that are ignored (samples not retained). Generally, ignoring the first 10% of iterations should be sufficient. In the current artificial example, this is unlikely to be an issue as the density distribution has very blunt, connected features.\nEventually (given enough iterations), the MCMC chain should sample the entire density profile and the resulting posterior distribution should converge (produce relatively invariant outcomes) on a stationary distribution (the target distribution).\nUnlike models fitted via maximum likelihood (which stop iterating when the improvement in log-likelihood does not exceed a pre-defined value - at which point they have usually converged), MCMC chains do not have a stopping criterion. Rather, the analyst must indicate the number of iterations (simulations) to perform. The number of iterations is always a compromise between convergence and time. The more iterations, the better the mixing and coverage, the greater the likelihood of convergence, yet the greater the time it takes to complete the analysis.\n\nTrace plots - as described above, trace plots for each parameter offer visual indicators of issues with the mixing and coverage of the samples collected by the chain as well as the presence of a starting configuration impact. In addition to the lack of drift, the residuals (sometimes called units) should be centered around 0. Positive displacement of the residuals can be an indication of zero-inflation (response having more zeros that the modelling distribution should expect).\nPlots of the posterior distribution (density or histograms) of each parameter. The basic shape of these plots should follow the approximate expected joint probability distribution (between family modeled in determining likelihood and prior). Conjugate priors are typically used to simplify the expected posterior distribution.\n\n\nIn our contrived example, this should be a normal distribution. Hence, obviously non-normal (particularly bi-modal) distributions would suggest that the chain has not yet fully mixed and converged on a stationary distribution. Specifically, it would imply that the sampling chain had focused heavily on one region of the parameter space before moving and focusing on another region. In the case of expected non-normal distributions, the density plots can also serve to remind us that certain measures of location and spread (such as mean and variance) might be inadequate characterizations of the population parameters.\n\n\n\n\n\n\n\n\n\n\n\nAutocorrelation - correlation between successive samples (as a result of way the Markov Chain spreads) are useful for determining the appropriate amount of thinning required to generate a well mixed series of samples. Autocorrelation can be explored visually (in a plot such as that illustrated earlier) or numerically by simply exploring the autocorrelation values at a range of lags.\nThe Geweke diagnostic (and Geweke-Brooks plot) compares the means (by a standard Z-score) of two non-overlapping sections of the chain (by default the first 10% and the last 50%) to explore the likelihood that the samples from the two sections come from the same stationary distribution. In the plot, successively larger numbers of iterations are dropped from the start of the samples (up to half of the chain) and the resulting Z-scores are plotted against the first iteration in the starting segment.\n\n\nSubstantial deviations of the Z-score from zero (those outside the horizontal critical confidence limits) imply poor mixing and a lack of convergence. The Geweke-Brooks plot is also particularly useful at diagnosing a lack of mixing due o the initial configuration and how long the burnin phase should be.\n\n\n\n\n\n\n\n\n\n\n\nThe Gelman and Rubin diagnostic essentially calculates the ratio of the total variation within and between multiple chains to the within chain variability (this ratio is called the Potential Scale Reduction Factor). As the chain progresses (and samples move towards convergence) the variability between chains should diminish such that the scale reduction factor essentially measures the ratio of the within chain variability to itself. At this point (when the potential scale reduction factor is 1), it is likely that any one chain should have converged. When the potential scale reduction factor is greater than 1.1 (or more conservatively 1.2), we should run the chain(s) for longer to improve convergence.\n\n\nClearly, in order to calculate the Gelman and Rubin diagnostic and plot, it is necessary to run multiple chains. When doing so, it is advisable that different starting configurations be used for each chain.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA Raftery and Lewis diagnostic estimates the number of iterations and the burnin length required to have a given probability (95%) of posterior quantile (0.025) accuracy (0.005). In the following, I will show the Raftery and Lewis diagnostic from a sample length of 1000 (which it considers below the minimum requirements for the quantile, probability and accuracy you have nominated) and this will be followed by diagnostics from a longer chain (40,000 iterations with a thinning factor of 10). In the latter case, for each parameter it lists the burnin length (M), number of MCMC iterations (N), minimum number of (uncorrelated) samples to construct this plot and I (the proportional increase in necessary iterations due to autocorrelation). Values of I greater than 5 indicate substantial autocorrelation.\n\n\n\nset.seed(9)  \nraftery.diag(params)\n\n\nQuantile (q) = 0.025\nAccuracy (r) = +/- 0.005\nProbability (s) = 0.95 \n\nYou need a sample size of at least 3746 with these values of q, r and s\n\nparams1&lt;-mcmcsim1(n=40000, start.x=0.5, start.y=3,tex=FALSE,lns=FALSE,fls=FALSE,thin=10)\n\n\n\n\n\n\n\nraftery.diag(params1)\n\n\nQuantile (q) = 0.025\nAccuracy (r) = +/- 0.005\nProbability (s) = 0.95 \n                                       \n Burn-in  Total Lower bound  Dependence\n (M)      (N)   (Nmin)       factor (I)\n 4        4782  3746         1.28      \n 5        5894  3746         1.57      \n\n\n\n\nThe Heidelberg and Welch diagnostic tests a null hypothesis that the Markov chain is from a stationary distribution. The test evaluates the accuracy of the estimated means after incrementally discarding more and more (10%, 20%, 30% up to 50%) of the chain. The point at which the null hypothesis is accepted marks the number of iterations required to keep. If the null hypothesis is not accepted by the time 50% of the chain has been discarded, it suggests that more MCMC iterations are required to reach a stationary distribution and to estimate the mean with sufficient accuracy. For the portion of the chain that passed (null hypothesis accepted), the sample mean and 95% confidence interval are calculated.\n\n\nIf the ratio of the mean to 95% interval is greater than 0.1, then the mean is considered accurate otherwise a longer overall chain length is recommended. Note, this diagnostic is misleading for residuals or any other parameters whose means are close to zero. Whilst this diagnostic can help trim chains to gain speed, in practice it is more useful for just verifying that the collected chain is long enough.\n\nset.seed(9) \npar(mfrow=c(1,3),mar=c(4,4,1,1))\nheidel.diag(params)\n\n                                \n  Stationarity start     p-value\n  test         iteration        \na passed       1         0.326  \nb passed       1         0.713  \n                            \n  Halfwidth Mean   Halfwidth\n  test                      \na failed    0.0262 0.0902   \nb passed    5.0083 0.1438   \n\nheidel.diag(params1)\n\n                                   \n     Stationarity start     p-value\n     test         iteration        \n[,1] passed       1         0.245  \n[,2] passed       1         0.912  \n                               \n     Halfwidth Mean   Halfwidth\n     test                      \n[,1] failed    -0.017 0.0395   \n[,2] passed     5.003 0.0683   \n\n\n\n\nAlthough MCMC algorithms have their roots in the Bayesian paradigm, they are not exclusively bound to Bayesian analyses. Philosophical arguments aside, MCMC sampling algorithms provide very powerful ways to tackling otherwise intractable statistical models. Indeed, it is becoming increasingly more common for MCMC sampling to employ vague (non-informative) priors to fit models that are otherwise not analytically compliant or convergent.",
    "crumbs": [
      "Statistical principles",
      "Estimation and inference"
    ]
  },
  {
    "objectID": "22_estimation.html#priors",
    "href": "22_estimation.html#priors",
    "title": "Estimation and inference",
    "section": "3.2 Priors",
    "text": "3.2 Priors\nBayesian analyses incorporate prior expectations about the parameters being estimated (fixed effects, random effects and residuals). As with the likelihoods and posteriors, the priors are distributions (rather than single values) thereby reflecting the typical prior expectations as the variability (spread).\nWhilst proponents consider the ability to evaluate information within a broader and more substantive context is one of the great strengths of the Bayesian framework, opponents claim that the use or reliance of prior information has the potential to bias the outcomes. To an extent, both camps have valid points (which is perhaps why the philosophical debates continue). However, there is a practical compromise.\nIt is possible, and indeed advisable to employ a range of priors including non-informative (or null, vague, weakly informative) priors. Strickly, non-informative priors (such as a uniform distribution which is purely flat over its entire range) have no impact on the outcomes as they simply multiply the likelihood by a constant. Hence, non-informative flat priors result in parameter estimates that are almost identical to those of equivalent traditional analyses.\nWeakly informative priors (which are typically more preferable) impose a broad structure (e.g. the prior of a parameter is normally distributed with a certain mean and typically very large standard deviation). Whilst in the absence of any other prior knowledge, it is common to use a normal distribution with a mean of zero and very large standard deviation. Note however, that such a prior distribution will have the effect of ‘shrinking’ estimates towards zero. Such priors are very useful when no other priors are sensible and justifiable. By applying a range of priors, it is possible to evaluate the impact of different priors and this in itself can be a valuable form of analysis.\nEven when no prior information is available, priors must still be defined to make use of MCMC sampling techniques. In such cases, non-informative priors are typically defined. The other retort to the subjectivity accusation is that science is an inherently subjective activity in the first place. To deny there is no subjectivity in other aspects of sampling design and data collection is purely fanciful. At least in the case of selecting priors, we can (by using a range of priors) at least evaluate this source of bias.\n\n3.2.1 Conjugate priors\nAlthough priors can be specified from any distribution, historically the choice of distribution was such that the posterior distribution could be calculated via relatively simple hand calculations. Recall that the posterior distribution is equal to the product of the likelihood function (P(D|H)) and the prior (P(H)) normalized (divided by) the probability of the data (P(D)) and that P(D) is determined by integrating across all possible parameter (hypothesis) values.\nP(H|D) = \\frac{P(D|H)\\times P(H)}{\\int P(D|H)P(H)dH}\nCalculating the integral (\\int P(D|H)P(H)dH) is generally more manageable when the posterior distribution (P(H|D) is the same distribution family (same distribution type, yet possibly different values of the parameters) as the prior distribution (P(H)). The posterior and prior distributions are thence called conjugate distributions and the prior is considered the conjugate prior to the model likelihood.\n\n\n\n\n\n\n\n\n\n\nObservations\nLikelihood distribution\nParameter\nConjugate prior\nPrior hyperparameters\n\n\n\n\nNormal (measurements)\nNormal (Gaussian)\n\\mu\nnormal\n\\mu, \\tau\n\n\n\n\n\\tau\ngamma\nr, \\mu\n\n\nSkewed normal (measurements)\nLog-normal\n\\mu\nnormal\n\\mu, \\tau\n\n\n\n\n\\tau\ngamma\nr, \\mu\n\n\nBinary (presence/absence, dead/alive)\nBinomial\nlogit(p)\nbeta\na, b\n\n\n\n\nprobit(p)\ngamma\na, b\n\n\n\n\ncloglog(p)\ngamma\na, b\n\n\nPoisson\nPoisson\nlog(\\lambda)\ngamma\nr, \\mu\n\n\n\nNegative binomial\nlogit(p)\nbeta\na, b\n\n\nPercentages\nBeta\na\nbeta\na, b\n\n\n\n\nb\nbeta\na, b\n\n\n\nNote, vague gamma priors has a is relatively flat density over most of its range, yet tend to have a sharp spike at the origin. This can cause problems, particularly in multilevel models. Consequently, uniform priors are recommended as an alternative to gamma priors.\nThe need for the conjugacy as a mathematical convenience is now unnecessary thanks to MCMC sampling techniques. Nevertheless, conjugacy does make it easier to anticipate the distribution of the posterior(s) and thus help identify and diagnose issues of mixing and convergence (via a lack of conformity to the anticipated distribution).\n\n\n3.2.2 Priors on variance\nNumerous non-informative priors for variance have been suggested including:\n\nflat uniform prior distribution U(0, A), where A\\rightarrow\\infty\nnon-informative inverse-gamma Gamma(\\epsilon, \\epsilon), where \\epsilon is set to a low value such as 0.001, 1.0E-06 etc)\nhalf-cauchy distribution HC(\\alpha) where the scale parameter, \\alpha is typically 5 or 25.\n\nGelman (2006) warns that the use of the inverse-gamma family of non-informative priors are very sensitive to \\epsilon particularly when variance is close to zero and this may lead to unintentionally informative priors. When the number of groups (treatments or varying/random effects) is large (more than 5), Gelman (2006) advocated the use of either uniform or half-cauchy priors. Yet when the number of groups is low, Gelman (2006) indicates that uniform priors have a tendency to result in inflated variance estimates. Consequently, half-cauchy priors are generally recommended for variances.",
    "crumbs": [
      "Statistical principles",
      "Estimation and inference"
    ]
  },
  {
    "objectID": "30_glm.html",
    "href": "30_glm.html",
    "title": "Generalised linear models",
    "section": "",
    "text": "Load the necessary libraries\n\nlibrary(tidyverse)   #for data wrangling and plotting\nlibrary(gganimate)   #for animations\nlibrary(gridExtra)   #for additional plotting routines\nlibrary(ggfortify)   #for regression diagnostics (autoplot)\nlibrary(DHARMa)      #for simulated residuals\nlibrary(performance) #for model diagnostics\nlibrary(see)         #for model diagnostics\nlibrary(glmmTMB)     #for GLM(M)'s\nlibrary(gmodels)     #for additional statistics\nlibrary(MASS)        #for neg binom glm",
    "crumbs": [
      "Linear modelling",
      "Generalised linear models"
    ]
  },
  {
    "objectID": "30_glm.html#categorical-predictors",
    "href": "30_glm.html#categorical-predictors",
    "title": "Generalised linear models",
    "section": "4.1 Categorical predictors",
    "text": "4.1 Categorical predictors\nThe model structure for linear models containing a single categorical predictor variable (known as a factor) with two or more treatment levels (groups) is similar in form to the multiple linear regression model (listed immediately above) with the overall mean (\\mu) replacing the y-intercept (\\beta_0). The factor levels (groups) are represented in the model by binary (contain only of 0s and 1s, see Table below) indicator (or `dummy’) variables and their associated estimable parameters (\\beta_1,~\\beta_2,~...).\nFor a data set comprising of p groups and n replicates within each group, the linear model is: y_{ij} = \\mu + \\beta_1(dummy_1)_{ij} +\n\\beta_2(dummy_2)_{ij} + .... + \\varepsilon_{ij} where j represents the treatment levels (from 1 to p) and i represents the set of replicates (from 1 to n) within the j^{th} group. Hence, y_{ij} represents the i^{th} observation of the response variable within the j^{th} group and (dummy_1)_{ij} represents the dummy code for the i^{th} replicate within the j^{th} group of the first dummy variable (first treatment level).\n\n\nRaw data\n\n\n\n\n\n\n\nY\nGroup\n\n\n\n\n2\nG1\n\n\n3\nG1\n\n\n4\nG1\n\n\n6\nG2\n\n\n7\nG2\n\n\n8\nG2\n\n\n10\nG3\n\n\n11\nG3\n\n\n12\nG3\n\n\n\n\nDummy coded data\n\n\n\n\n\n\n\n\n\ny\ndummy_1\ndummy_2\ndummy_3\n\n\n\n\n2\n1\n0\n0\n\n\n3\n1\n0\n0\n\n\n4\n1\n0\n0\n\n\n6\n0\n1\n0\n\n\n7\n0\n1\n0\n\n\n8\n0\n1\n0\n\n\n10\n0\n0\n1\n\n\n11\n0\n0\n1\n\n\n12\n0\n0\n1\n\n\n\n\n\nThe dummy variable for a particular treatment level contains all 0s except in the rows that correspond to observations that received that treatment level. The table above illustrates the dummy coding for a single factor with three levels (G1',G2’, `G3’) each with three replicates. Note that a linear model of the form:\ny_{ij}=\\beta_0+\\beta_1(dummy_1)_{ij} + \\beta_2(dummy_2)_{ij}\n+\\beta_3(dummy_3)_{ij} + \\varepsilon_{ij}\nis over-parameterized (as we are attempting to estimate four parameters from three populations, see the tutorial on estimation.\nAn effects model for a factor with p groups, will have p+1 parameters (the overall mean \\mu plus the p \\beta parameters), and thus the linear effects model is considered to be over-parameterized. Given that \\beta_j=\\mu_j - \\mu, it is only possible to estimate p-1 orthogonal (independent) parameters. For example, once \\mu and p-1 of the effects parameters have been estimated, the final effects parameter is no longer free to vary and therefore cannot be independently estimated. Likewise, if the full linear model contains as many dummy variables as there are treatment groups, then it too is over-parameterized.\nIn order to obtain parameter estimates, the model must be reduced to a total of p parameters. Over-parameterization can be resolved by one of two alternative parameterizations:\n\nmeans parameterization - removing one of the parameters from the effects model (either the overall mean (\\mu) or one of the treatment effects (\\beta_j) parameters - a procedure rarely used in a frequentist framework in biology). When it is the overall mean that is removed, then each of the regression parameters represent the mean of a group.\ny_{ij} = \\beta_j+\\varepsilon_{ij}, \\hspace{2cm}\\text{where}~j= 1:\\text{number of levels of the factor} \\begin{pmatrix}\n  2\\\\3\\\\4\\\\6\\\\7\\\\8\\\\10\\\\11\\\\12\n\\end{pmatrix} =\n\\begin{pmatrix}\n  1&0&0\\\\1&0&0\\\\1&0&0\\\\0&1&0\\\\0&1&0\\\\0&1&0\\\\0&0&1\\\\0&0&1\\\\0&0&1\n\\end{pmatrix} \\times\n\\begin{pmatrix}\n  \\beta_1\\\\\\beta_2\\\\\\beta_3\n\\end{pmatrix} +\n\\begin{pmatrix}   \\varepsilon_1\\\\\\varepsilon_2\\\\\\varepsilon_3\\\\\\varepsilon_4&\\\\\\varepsilon_5\\\\\\varepsilon_6\\\\\\varepsilon_7&\\\\\\varepsilon_8\\\\\\varepsilon_9\n\\end{pmatrix}\n\neffects parameterization - generating a new set (p-1) of effects parameters (\\beta_{j}, where j represents the set of orthogonal parameters from 1 to p-1) each of which represent a linear combination of groups rather than a single group effect. That is, each \\beta can include varying contributions from any number of the groups and are not restricted to a single contrast of (=\\beta_j-\\beta). For example, one of the parameters might represent the difference in means between two groups or the difference in means between one group and the average of two other groups. y_{ij} = \\beta_0+\\beta_j+\\varepsilon_{ij},\n\\hspace{2cm}\\text{where}~j= 1:(p-1) In the effects parameterization, \\mu typically represents the mean of one of the groups (a reference group) and each of the \\beta effects represent the difference between subsequent group means and the reference group.\n\\begin{pmatrix}\n  2\\\\3\\\\4\\\\6\\\\7\\\\8\\\\10\\\\11\\\\12\n\\end{pmatrix} =\n\\begin{pmatrix}\n  1&0&0\\\\1&0&0\\\\1&0&0\\\\1&1&0\\\\1&1&0\\\\1&1&0\\\\1&0&1\\\\1&0&1\\\\1&0&1\n\\end{pmatrix} \\times\n\\begin{pmatrix}\n  \\beta_0\\\\\\beta_1\\\\\\beta_2\\\\\\beta_3\n\\end{pmatrix} +\n\\begin{pmatrix}   \\varepsilon_1\\\\\\varepsilon_2\\\\\\varepsilon_3\\\\\\varepsilon_4&\\\\\\varepsilon_5\\\\\\varepsilon_6\\\\\\varepsilon_7&\\\\\\varepsilon_8\\\\\\varepsilon_9\n\\end{pmatrix}\n\n\nThe reduced number of effects parameters are defined through the use of a matrix of contrast coefficients. Note, the new set of effects parameters should incorporate the overall relational effects of each of the groups equally such that each group maintains an equal contribution to the overall model fit. The contrast coefficients are responsible for determining how the model is re-parameterized into an orthogonal model matrix.\nA number of pre-fabricated, contrast matrices exist (to yield commonly used model matrices), each of which estimate a different set of specific comparisons between treatment combinations. We will explore each of these with Motivating example 3 in subsequent sections on fitting linear models.",
    "crumbs": [
      "Linear modelling",
      "Generalised linear models"
    ]
  },
  {
    "objectID": "30_glm.html#maximum-likelihood",
    "href": "30_glm.html#maximum-likelihood",
    "title": "Generalised linear models",
    "section": "9.2 Maximum likelihood",
    "text": "9.2 Maximum likelihood\nOrdinary Least Squares provides an elegant solution for when the data satisfy certain assumptions (normality, homogeneity, independence, etc), yet for many other situations, it is not appropriate.\nFor the following demonstrations, we will use the data from Motivating example 1.\nAs with OLS, lets start with motivating example 1 data\n\ndat$y\n\n [1]  9.64  3.79 11.00 27.88 32.84 32.56 37.84 29.86 45.05 47.65\n\n\nA more general alternative to OLS is Maximum Likelihood (ML).\nLikelihood is a measure of how probable (likely) a set of observations are at following (or being drawn from) a specific distribution. For example, we could evaluate the likelihood that the observations could have come from a normal (Gaussian) distribution with a specific mean and standard deviation.\nBefore we can understand likelihood, we must first remind ourselves of a couple of things about probability.\n\nfor any continuous distribution, the probability of obtaining a specific values (that is, that a specific value (X) is equal to a particular reference values (x)) is infinitely small (Pr(X=x)=0). We can only directly estimate probabilities of obtaining values less than (or greater than) nominated reference points (quantiles).\nit is possible to calculate the probability that a specific value (X) is between two reference points (Q1 and Q2). This is just the probability of X being less than Q1 minus the probability of X being less than Q2 (Pr(Q1 &lt; X &gt; Q2)).\n\nSo although we cant estimate the probability that Pr(X=x) directly from the distributions’ function (f(x)), we can approximate this by calculating the probability that X is in the interval [x,\nx+\\Delta]:\n\n\\frac{Pr(x &lt; X \\le x + \\Delta)}{\\Delta}\n\nThe smaller \\Delta (so long as it is larger than 0), the more accurate the estimate. This becomes a simple calculus problem. The derivative (f'(x)) of the distributions’ function is a probability density function (PDF). The PDF allows us to approximate the probability of obtaining a single value from a distribution.\nProvided the data are all iid (individually and identically distributed), and thus from the same distribution, the probability (likelihood) that a set of values (X) comes from a specific distribution (described by its parameters, \\theta) can be calculated as the product of their individual probabilities.\n\nL(X|\\theta) = \\prod^n_{i=1} f'(x_i|\\theta)\n\nThe products of probability densities can soon become very small and this can lead to computation and rounding issues. Hence it is more usual to work with the logarithms of likelihood. The log laws indicate that the log of a product is the same as the sum of the individual logs (log(A\\times B) = log(A) +\nlog(B)).\n\nLL(X|\\theta) = \\sum^n_{i=1} log(f'(x_i|\\theta))\n\nThe PDF of a Gaussian distribution is:\n\nP(x|\\mu, \\sigma) = \\frac{1}{\\sqrt{\\sigma^2 2\\pi}} exp^{-\\frac{1}{2}((x-\\mu)/\\sigma)^2}\n\nSo in order to estimate the optimum values for the parameters (\\mu and \\sigma), given our data (x), we would maximize the following:\nReturning to our example,\n\n\\begin{align}\nLL(x_1, x_2, x_3, ..., x_n|\\mu, \\sigma^2) =& \\sum^n_{i=1}ln(\\frac{1}{\\sqrt{\\sigma^2 2\\pi}} exp^{-\\frac{1}{2}((x_i-\\mu)/\\sigma)^2})\\\\\n=& -\\frac{n}{2}ln(2\\pi\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum^n_{i=1}(x_i-\\mu)^2\n\\end{align}\n\nNote optimization routines usually attempt to minimize rather than maximize. Nevertheless, finding the maximum of a log-likelihood (what we are after) is the same as finding the minimum of the negative log-likelihood.\nFor the examples and routines that follow, we will write our own code from first principles as well as use existing built in R functions. Although the code that we write could be written to find maximums, the inbuilt functions typically optimise on minimums. As a result, to enable us to compare routines, we will work with minimizing negative log-likelihood.\nDo, lets now write a function for calculating the negative log-likelihood for our data given a Gaussian distribution based on the formula above.\n\nLL.gaus = function(theta, x) {\n    m=theta[1]\n    s=theta[2]\n    ll = -(length(x)/2)*(log(2*pi*s^2)) + (-1/(2*s^2)) * sum((x-m)^2)\n    ##OR\n    ## ll = sum(dnorm(x, mean=m, sd=s, log=TRUE))\n    return(-ll)\n}\n\nIn a similar manner to the brute force approach we used to approximate OLS earlier, lets use a brute force approach to explore the partial negative log-likelihood profile for the mean and then approximate the mean (we will fix the standard deviation at 1). We refer to it as a partial profile, because it is holding the other parameter constant.\n\nmu = seq(15,40,len=1000)\ntheta=cbind(mu=mu,sigma=1)\nB=apply(theta, 1, LL.gaus, dat$y)\ntheta[which(B==min(B)),]\n\n      mu    sigma \n27.81281  1.00000 \n\nggplot(data=NULL) + geom_line(aes(y=B, x=theta[,'mu']))\n\n\n\n\n\n\n\n\nAgain, this estimation is very close to the ‘true’ value.",
    "crumbs": [
      "Linear modelling",
      "Generalised linear models"
    ]
  },
  {
    "objectID": "30_glm.html#optimization",
    "href": "30_glm.html#optimization",
    "title": "Generalised linear models",
    "section": "9.3 Optimization",
    "text": "9.3 Optimization\nOptimization algorithms are essentially search algorithms. They are attempting to find a single point in multidimensional space. There are numerous types of algorithms, each of which offers different advantages and disadvantages under different circumstances. For example, some assume that the underlying likelihood is very smooth (changes very gradually) and gain efficiencies out of being able to located minimums via differentiation. Others are less efficient, yet more accurate when the likelihood is not smooth or there are multiple local minima.\nLets use an analogy to gain a better appreciation of the problem and solutions. Imagine we had a big block of land and we wanted to install a well from which to draw water from an underground aquifer. Although there are no physical restrictions on where the well can be positioned, we are keen on it being as shallow as possible (perhaps because it is cheaper to drill a shallow well).\nThe depth from the land surface down to the aquifer is not constant over space and we want to be able to put our well in the shallowest point. Although we do not know the true underground topography, we can drill narrow pilot holes and accurately measure the depth down to the aquifer at any point in space.\nTo put this another way, although we do not know what the underground profile looks like throughout the entire spatial domain (all possible latitide/longitude values), we can estimate its value (depth) at any point (single latitude/longitude).\n\n\nTo find the optimum location for our well, we need a search algorithm. One that is able to find the latitude/longitude associated with the minimum depth. We will showcase a few different options and try to describe the advantages and disadvantages of each. For example, in our well analogy, if the depth profile was very smooth (undulated very slowly), we might be able to use approximations to the curvature of the undulations to find where any minimums are. On the other hand, if the profile is not smooth (perhaps there are underground caves or other abrupt underground geological features), such approximations may be very inaccurate and more exhaustive searching (such as a grid of pilot holes) may be required.\nJust like with the underground aquifer, although a (negative) log-likelihood function has an unknown profile in multidimensional space (one dimension for each parameter to estimate), we can evaluate it for any combination of parameters.\n\nBrute forceSimplex methodsDerivative methodsStochastic global optimisation\n\n\nOne conceptually simple way of searching for the minimum of a function is to evaluate the function for a large number of parameter combinations (perhaps in a grid). For example, to drill a pilot hole every 100m in a grid. If the grid is fine enough, it will located the minimum (maximum) no matter what the functions profile is. However, the finer the grid, the more effort is required - lower efficiency.\nThe following code chunk evaluates and plots the negative log-likelihood for a full (100\\times 100) grid of parameter combinations. Negative log-likelihood is represented as a colour along the green to white spectrum. The blue lines represent major contours in the profile. The optimum parameters (those associated with the minimum negative log-likelihood and thus maximum log-likelihood) are indicated by the black solid point.\n\nmu = seq(15,40,len=100)\nsigma=seq(10,20,len=100)\ntheta = expand.grid(mu=mu, sigma=sigma)\ntheta$LL=apply(theta, 1, LL.gaus, x=dat$y)\nggplot(data=theta,aes(y=mu, x=sigma, fill=LL)) +\n    geom_tile(show.legend=FALSE) + geom_contour(aes(z=LL)) +\n    geom_point(data=theta[which(theta$LL==min(theta$LL)),], aes(y=mu, x=sigma), fill='black') +\n    scale_fill_gradientn(colors=terrain.colors(12)) +\n    scale_x_continuous(expand=c(0,0)) +\n    scale_y_continuous(expand=c(0,0))\n\nWarning: The following aesthetics were dropped during statistical transformation: fill\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\n\n\n\n\n\n\ntheta[which(theta$LL==min(theta$LL)),]\n\n           mu    sigma       LL\n4252 27.87879 14.24242 40.76342\n\n\nThe optimum solution yielded an estimated mean of 27.8787879 and sigma of 14.2424242. These values are very similar to the empirical calculations.\nThe brute force approach is a useful illustration, but it is practically limited to just a one or two parameters and since the entire search space within the domain needs to be evaluated. This is essentially an exhaustive search algorithm. Furthermore, the accuracy is dependent on the resolution of the grid. The finer the grid, the more accurate, however it does require n^p function evaluations (where n is the number of grid increments per p parameters.\nOf course, we can always use a relatively course search grid and having identified the ‘optimum’ parameter configuration within this grid, we could apply a finer resolution grid over a narrower search domain. This is analogous to starting with a 100x100m drilling grid and then centring a 1x1m drilling grid around the ‘best’ location.\n\n\nA simplex is a multidimensional space that has one more vertex than there are parameter dimensions (p+1). In this case, we are estimating two parameters, therefore the simplex is a triangle (three vertices). The most common form of simplex optimisation is the Nelder-Mead algorithm. As with most optimisation algorithms, the Nelder-Mead algorithm is a search algorithm that aims to find a point in multidimensional space as efficiently as possible.\nOptimisation routines work on a wide variety of function (not just likelihood functions). To keep the terminology about what is being optimised general, the function to be optimised is often referred to as a loss or objective function. A loss function is any function that evaluates the performance (or fit) of a set of events (i.e. data). The further the loss is from zero, the worse the fit (hence the desire to find the minimum).\nThe Nelder-Mead algorithm can be described as (keep in mind that it is a minimisation rather than maximisation):\n\nStart with some initial estimate values - e.g. a set of p+1 vertices for each parameter.\nIdentify the vertex with the highest (ie worst) loss (negative log-likelihood).\nReflect the simplex around the centroid of the other vertices 3a. If the reflected point is better (lower negative log-likelihood) than the second worst point, but not better than the best point, replace the worst point with the reflected point. 3b. If instead, the reflected point is the best vertex, then expand this point by doubling the reflection distance\n 3b.1.  If this expanded point is better than the reflected point,\n        replace the worst point with the expanded point\n 3b.2   Otherwise replace the worst point with the reflected point.\n3c. If instead neither 3a or 3b (the reflected point is not better than the second worst point, then contract the point by halving the reflection distance.\n 3c.1.  If this contracted point is better than the worst point, replace the worst point with the contracted point\n 3c.2.  Otherwise, shrink the entire simplex (contract all vertices towards the centroid)\nRepeat Steps 2-3 until either the maximum number of iterations have occurred or the change in loss between two successive iterations is below a certain threshold.\n\nClearly, the more iterations are performed, the more accurate the estimates, and yet the longer the search will take.\nIn the well analogy, the simplex represents three points of the search triangle. Reflecting the triangle allows us to move away from the direction we know to be deeper. We expand the triangle in order to explore a new direction and contract the triangle to narrow our search area.\nCode for illustrating the process is listed as details below. This code is a modification of the code presented in https://github.com/nicolaivicol/nelder-mead-R/blob/master/optimNM.R\n\n\n\n\n\n\nSee the function\n\n\n\n\n\n\nget.optim.NM &lt;- function(X.vert, params.init, objective.fn, iter.max=250, abs.tol=0.0001, x, control=list(fnscale=1,refl=1,expan=2,contr=-0.5,shrink=0.5))\n{\n  # input dimension\n  X.len &lt;- length(params.init)\n  # initialize controls before iterations of searching\n  iter &lt;- 0; not.converged &lt;- 1; not.max.iter &lt;- 1\n  X.optim &lt;- params.init; f_X.optim &lt;- control$fnscale*objective.fn(X.optim, x=x)\n  # while loop, iterations\n  while (not.converged & not.max.iter)\n  {\n    # get values at vertices\n    f_X.vert &lt;- control$fnscale*apply(X = X.vert, MARGIN = 1, FUN = objective.fn, x) \n    # order ascending X.vert and f(X.vert), by f(X.vert)\n    X.order &lt;- sort(f_X.vert, index.return = TRUE)$ix\n    X.vert &lt;- X.vert[X.order, ]\n    f_X.vert &lt;- f_X.vert[X.order]\n    # get centroid (mean on each dimension) of all points except the worst\n    X.centr &lt;- apply(X = X.vert[1:X.len, ], MARGIN = 2, FUN = mean)\n    # get reflected point\n    X.refl &lt;- X.centr + control$refl*(X.centr - X.vert[X.len+1, ])\n    f_X.refl &lt;- control$fnscale*objective.fn(X.refl,x)\n    if ((f_X.vert[1] &lt;= f_X.refl) & (f_X.refl &lt; f_X.vert[X.len]))\n    { \n      # if the reflected point is better than the second worst, but not better than the best...\n      # ... then obtain a new simplex by replacing the worst point with the reflected point\n      X.vert[X.len+1, ] &lt;- X.refl \n    } else if (f_X.refl &lt; f_X.vert[1]) {\n      # if the reflected point is the best point so far\n      # ... then compute the expanded point\n      X.expan &lt;- X.centr + control$expan*(X.centr - X.vert[X.len+1, ])\n      f_X.expan &lt;- control$fnscale*objective.fn(X.expan,x)\n      # ... if the expanded point is better than the reflected point\n      if (f_X.expan &lt; f_X.refl)\n      {\n        # ... then obtain a new simplex by replacing the worst point with the expanded point\n        X.vert[X.len+1, ] &lt;- X.expan   \n      } else {\n        # ... else obtain a new simplex by replacing the worst point with the reflected point\n        X.vert[X.len+1, ] &lt;- X.refl\n      }\n    } else {\n      # ... reflected point is not better than second worst\n      # ... then compute the contracted point\n      X.contr &lt;- X.centr + control$contr*(X.centr - X.vert[X.len+1, ])\n      f_X.contr &lt;- control$fnscale*objective.fn(X.contr,x)\n      # ... if the contracted point is better than the worst point\n      if (f_X.contr &lt; f_X.vert[X.len+1])\n      {\n        # ... then obtain a new simplex by replacing the worst point with the contracted point\n        X.vert[X.len+1, ] &lt;- X.contr\n      } else {\n        # ... shrink the simplex: X = X1 + coef.shrink(X-X1)\n        X.vert &lt;- sweep(control$shrink*sweep(X.vert, 2, X.vert[1, ], FUN = \"-\"), 2, X.vert[1, ], FUN=\"+\")\n      }    \n    }\n    # get values at vertices\n    f_X.vert &lt;- control$fnscale*apply(X = X.vert, MARGIN = 1, FUN = objective.fn, x) \n    # order asc X.vert and f(X.vert)\n    X.order &lt;- sort(f_X.vert, index.return = TRUE)$ix\n    X.vert &lt;- X.vert[X.order, ]\n    f_X.vert &lt;- f_X.vert[X.order]   \n    # update controls\n    iter &lt;- iter + 1 \n    not.max.iter &lt;- (iter &lt; iter.max)*1\n    not.converged &lt;- (abs(control$fnscale*objective.fn(X.vert[X.len, ],x)- control$fnscale*objective.fn(X.vert[1, ],x)) &gt; abs.tol)*1\n    \n    X.optim &lt;- X.vert[1, ]; f_X.optim &lt;- control$fnscale*objective.fn(X.optim,x)\n  }\n  return(list(X.optim=X.optim, f_X.optim=f_X.optim, X.vert=X.vert, iter=iter))   \n}\n\n\n\n\nWe can illustrate the iterative process by plotting the outcome of a limited number of iterations - in this case 10 iterations. In this illustration, the filled in triangle represents the current optimum simplex. Previous simplexes remain as open triangle.\n\n## Starting values at the center of the vectices\nparams.init &lt;- c(mu = 20, sigma = 12)\nd.mu &lt;- 0.5\nd.sigma &lt;- 0.3\nsimplex &lt;- rbind(Vertex1 = params.init + c(d.mu,d.sigma),\n  Vertex2 = params.init + c(-d.mu, d.sigma),\n  Vertex3 = params.init + c(-d.mu, -d.sigma)) |&gt;\n  data.frame()\nbase.plot &lt;- ggplot(data = theta, aes(y = mu, x = sigma)) +\n  geom_tile(aes(fill = LL), show.legend = FALSE) + \n  geom_contour(aes(z = LL)) +\n  scale_fill_gradientn(colors = terrain.colors(12)) +\n  scale_x_continuous(expand = c(0,0)) +\n  scale_y_continuous(expand = c(0,0))\n\nnewdata &lt;- vector('list', 15)\na &lt;- get.optim.NM(X.vert = as.matrix(simplex), params.init, objective.fn = LL.gaus, x = dat$y, iter.max = 1, control = list(fnscale = 1, refl = 1, expan = 2, contr = -0.5, shrink = 0.5))\nnewdata[[1]] &lt;- data.frame(a$X.vert)\nfor (i in 2:15) {\n  a &lt;- get.optim.NM(X.vert = a$X.vert, params.init = a$X.optim, objective.fn = LL.gaus, x = dat$y, iter.max = 1, control = list(fnscale = 1, refl = 1, expan = 2, contr = -0.5, shrink = 0.5))\n  newdata[[i]] &lt;- data.frame(a$X.vert)\n}\nnewdata &lt;- bind_rows(newdata, .id = 'Iter') |&gt; \n  mutate(Iter=factor(Iter, levels = unique(Iter)))\ng &lt;- base.plot + \n  geom_polygon(data = newdata, aes(y = mu, x = sigma, group = Iter), \n    color = 'black', fill = 'grey40') +\n  transition_manual(Iter) + \n  shadow_trail(distance = 0.1, alpha = 0.4, color = 'grey40', fill = NA) + \n  labs(title = 'Iter: {current_frame}')\nga &lt;- animate(g, fps = 20, duration = 15)\n#ga=animate(g, renderer=av_renderer())\nanim_save('simplexAnim.gif', animation = ga, path = '../tut/resources/',  renderer =av_renderer()) \n\n\nHaving seen this illustration, we could allow the Nelder-Mead simplex optimization to iterate more thoroughly. We will now instruct the routine to iterate a maximum of 250 times. Along with setting a maximum number of iterations, most optimizations also have a stopping trigger based around the extent of improvement between iterations. This convergence tolerance defines a threshold difference below which two successive iteration outcomes are considered the same. The lower the value, the more accuracy is demanded.\nIt is also a good idea to repeat the iterations again from multiple starting configurations to ensure that any single optimization has not just settled n a local minimum (maximum).\n\nget.optim.NM(X.vert = as.matrix(simplex), params.init, objective.fn = LL.gaus, \n  x = dat$y, iter.max = 250, abs.tol = 1e-08, \n  control = list(fnscale = 1, refl = 1, expan = 2, contr = -0.5, shrink = 0.5))\n\n$X.optim\n      mu    sigma \n27.81238 14.25925 \n\n$f_X.optim\n   sigma \n40.76329 \n\n$X.vert\n              mu    sigma\nVertex3 27.81238 14.25925\nVertex2 27.80955 14.25881\nVertex1 27.81327 14.25858\n\n$iter\n[1] 32\n\nget.optim.NM(X.vert = as.matrix(simplex - c(-0.5, 0.2)), params.init, objective.fn = LL.gaus, \n  x = dat$y, iter.max = 250, abs.tol = 1e-08, \n  control = list(fnscale = 1, refl = 1, expan = 2, contr = -0.5, shrink = 0.5))\n\n$X.optim\n      mu    sigma \n27.81122 14.25812 \n\n$f_X.optim\n   sigma \n40.76329 \n\n$X.vert\n              mu    sigma\nVertex1 27.81122 14.25812\nVertex2 27.80970 14.25939\nVertex3 27.81149 14.26005\n\n$iter\n[1] 38\n\n\nThe two sets of estimated parameters (listed as X.optim) are very similar (converged) and are very similar to those calculated empirically.\nR has an inbuilt function (optim()) that is an interface to numerous optimization algorithms, and the default algorithm is Nelder-Mead. As with other optimizations, optim() defaults to minimizing. To force it to maximize (if our likelihood function returned log-likelihood rather than negative log-likelihood), we can indicate that the fnscale is -1. Other important optimization control parameters include:\n\nmaxit - the maximum number of iterations to perform (100 for Nelder-Mead).\nabstol - the absolute convergence tolerance. The lower the tolerance, the smaller the change in optimized value (log-likelihood) required before convergence is reached.\n\n\noptim(par = c(20, 12), LL.gaus, x = dat$y, control = list(fnscale = 1))\n\n$par\n[1] 27.81143 14.25771\n\n$value\n[1] 40.76329\n\n$counts\nfunction gradient \n      53       NA \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n\nWe can see that the negative log-likelihood calculation was performed 51 times before convergence. Whilst this is not the same as the number of iterations, it does provide an estimate of the total computation load.\nIf we compare this to the brute force approach (which required 100\\times 100=10,000 evaluations), the Nelder-Mead simplex approach is a substantial improvement.\n\n\nIf the profile of a function is smooth enough, and the function itself is doubly differentiable (can be differentiated into first and second order derivatives), then we can make use of a group of algorithms based on a root-finding algorithm devised by Isaac Newton. In mathematical contexts, a root is the value of the parameters (x) when the function equals zero (f(x)=0).\nA simplified version of Newton’s method, the Newton-Raphson method shows that root (x_{n+1}) of a function (f(x_n)) can be approximated by iteratively solving:\n\nx_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}\n\nwhere x_n is the initial parameter estimate, x_{n+1} is the improved parameter estimate, f(x_n) is the value of the function at x_ n and f'(x_n) is the first order derivative (the slope) of the function at x_n).\nBefore we use this approach to estimate maximum likelihood estimates, lets see how it works with a polynomial function (e.g. y=0.1x^4 +\nx^3 - x^2 + 30).\nIn the following animation, we will use the Newton-Raphson method to estimate the value of x when y=0. The animation will go through five iterations. For each iteration, the red line represents the slope associated with the initial estimate of x. The point where this line intersects with the dashed line (y=0) is the updated estimate for x. We can see that by the fifth iteration, the estimated x is has began to stabalise (converge) on a value of approximately 1.98.\n\n\n\n\n\n\nCode used to implement Newton-Raphson method for simple functions and generate animation\n\n\n\n\n\n\nNR &lt;- function(f, x, return.grid=FALSE) {\n    if (is.function(f)) f=body(f)[[2]]    \n    f_x = eval(f)\n    f._x = eval(D(f,'x'))\n    if (return.grid) {\n        list(x1=x - f_x/f._x, f_x=f_x, f._x=f._x)\n    }else {\n        list(x1=x - f_x/f._x)\n    }\n}\noptim.NR &lt;- function(f, x0, abs.tol=1.0E-6, iter.max=250, return.grid=FALSE) {\n    iter &lt;- 0; not.converged &lt;- 1; not.max.iter &lt;- 1\n    fgrid &lt;- list()\n    while (not.converged & not.max.iter) {\n        nr &lt;- NR(f, x = x0, return.grid)\n        x1 &lt;- nr$x1\n        iter &lt;- iter + 1 \n        not.max.iter &lt;- (iter &lt; iter.max)*1\n        not.converged &lt;- (abs(x0-x1) &gt; abs.tol)*1\n        if (return.grid) fgrid[[iter]] &lt;- c(x0 = x0, x1 = x1, f_x = nr$f_x, f._x = nr$f._x)\n        x0 &lt;- x1\n    }\n    list(X.optim = x0, iter = iter, grid = do.call('rbind', fgrid))\n}\n\n\nf &lt;- function(x) {0.1*x^4 + x^3 - x^2 + 10*x + 30} \na &lt;- optim.NR(f, x0 = 3, abs.tol = 0.001, return.grid = TRUE)\n\nx &lt;- seq(-4, 4, len = 100)\ndat1 &lt;- data.frame(x = x, y = f(x))\ngdat &lt;- a$grid %&gt;% as.data.frame |&gt;\n    mutate(Iter = 1:n(),\n           x = ifelse(x0&gt;0, x0+1, x0-1),\n           y = f_x + (x-x0)*f._x)\n\ng &lt;- ggplot() + geom_line(data = dat1, aes(y = y, x = x)) +\n    geom_hline(yintercept = 0, linetype = 'dashed') +\n    geom_segment(data = gdat, aes(x = x, y = y, xend = x1, yend = 0), color = 'red') +\n    geom_segment(data = gdat, aes(x = x0, xend = x0, y = 0, yend = f_x), linetype = 'dashed')+\n    geom_text(data = gdat |&gt; filter(Iter&lt;4), aes(x = x0, y = -5, label = \"x[0]\"), parse = TRUE) +\n    geom_text(data = gdat %&gt;% filter(Iter&lt;4), aes(x = x1, y = -5, label = \"x[1]\"), parse = TRUE) +\n    geom_point(data = gdat, aes(x = x0, y = f_x)) +\n    geom_text(data = gdat |&gt; filter(Iter&lt;4), aes(x = x0, y = f_x, label = \"f(x)\"), \n      parse = TRUE, nudge_x = 0.1, nudge_y = -5, hjust = 0, vjust = 0) +\n    geom_text(data = gdat |&gt; filter(Iter&lt;4), aes(x = x0+0.3+(x1-x0)/2, y = (f_x/2)-5, label = \"f*minute*(x)\"), \n      parse = TRUE, color = 'red') +\n    geom_text(data = gdat, aes(x = -4, y = 140, label = paste0(\"Iter == \", Iter)), parse = TRUE, hjust = 0) +\n    geom_text(data = gdat, aes(x = -4, y = 120, label = paste0(\"x[1] == \", round(x1, 3))), parse = TRUE, hjust = 0) +\n    transition_manual(Iter)\nga &lt;- animate(g, fps = 20, duration = 5)\nanim_save('NMAnim1.gif', animation = ga, path = '../tut/resources')\n\n\n\n\n\nIf we allow our implementation of the Newton-Raphson method to run from an initial guess of 3 until it converges:\n\noptim.NR(f, x0=3)\n\n$X.optim\n[1] -1.982394\n\n$iter\n[1] 6\n\n$grid\nNULL\n\n\nWe see that it takes just 6 to converge.\nIn the above, we estimated the root (value of x when f(x)=0) of a function. If instead, we want to find the value of x when the function is at its minimum (e.g. optimisation), then we want to find the root of the first derivative (f'(x)) of the function - that is, we want to find the value of x where the slope of f(x) is 0.\nAgain, in order to illustrate the principles of what we are trying to achieve, we must digress from our actual example. Rather than try to find the root of a function, we will now try to find the root of the derivative of a function (so as to find the minimum of a function). So we are now shifting our focus away from the profile of the function and onto the profile of the derivative of the function (since the derivative of a function is a slope profile).\nThe left hand side of the following figure represents the profile of the function (y = 0.001x^3 - 0.001x^2 - 0.3x + 5). The right hand side represents the derivative (0.001(3x^2) - 0.301(2x)) of that function.\n\nf &lt;- function(x) {0.001*x^3 - 0.001*x^2 -0.3*x + 5}\nf1 &lt;- D(body(f)[[2]], 'x')\nx &lt;- seq(0, 20, len = 100)\ndat1 &lt;- data.frame(x = x, y = f(x))\ng1 &lt;- ggplot() + \n  geom_line(data = dat1, aes(y = y, x = x)) \n\ng2 &lt;- ggplot() + \n  geom_line(data = data.frame(x = x, y = eval(f1, envi = list(x = x))), aes(y = y, x = x)) +\n  geom_hline(yintercept = 0, linetype = 'dashed')\ngrid.arrange(g1, g2, nrow = 1)\n\n\n\n\n\n\n\n(a &lt;- optim.NR(f1, x0 = 3, abs.tol = 0.001, return.grid = TRUE))\n\n$X.optim\n[1] 10.33889\n\n$iter\n[1] 6\n\n$grid\n           x0       x1           f_x       f._x\n[1,]  3.00000 20.43750 -2.790000e-01 0.01600000\n[2,] 20.43750 12.87523  9.121992e-01 0.12062500\n[3,] 12.87523 10.59535  1.715639e-01 0.07525136\n[4,] 10.59535 10.34209  1.559353e-02 0.06157209\n[5,] 10.34209 10.33889  1.924166e-04 0.06005255\n[6,] 10.33889 10.33889  3.079948e-08 0.06003333\n\n\nIf we animate the process:\n\n\n\n\n\n\nAnimation code\n\n\n\n\n\n\ndat1 &lt;- data.frame(x, y = eval(f1, env = list(x = x)))\ngdat &lt;- a$grid |&gt;\n  as.data.frame() |&gt;\n  mutate(Iter = 1:n(),\n           x = ifelse(x0&gt;0,x0+1, x0-1),\n           y = f_x + (x-x0)*f._x)\n\ng2 &lt;- ggplot() + \n  geom_line(data = dat1, aes(y = y, x = x)) +\n  geom_hline(yintercept = 0, linetype = 'dashed') +\n  geom_segment(data = gdat, aes(x = x, y = y, xend = x1, yend = 0), color = 'red') +\n  geom_segment(data = gdat, aes(x = x0, xend = x0, y = 0, yend = f_x), linetype = 'dashed') +\n  geom_text(data = gdat |&gt; filter(Iter&lt;4), aes(x = x0, y = -0.05, label = \"x[0]\"), parse = TRUE) +\n  geom_text(data = gdat |&gt; filter(Iter&lt;4), aes(x = x1, y = -0.05, label = \"x[1]\"), parse = TRUE) +\n  geom_point(data = gdat, aes(x = x0, y = f_x)) +\n  geom_text(data = gdat |&gt; filter(Iter&lt;4), aes(x = x0, y = f_x, label = \"f(x)\"), \n    parse = TRUE, nudge_x = 0.1, nudge_y = -0.05, hjust = 0, vjust = 0) +\n  geom_text(data = gdat |&gt; filter(Iter&lt;4), aes(x = x0+0.3+(x1-x0)/2, y = (f_x/2)-0.05, label = \"f*minute*(x)\"), \n    parse = TRUE, color = 'red') +\n  geom_text(data = gdat, aes(x = 0, y = 0.8, label = paste0(\"Iter == \", Iter)), \n    parse = TRUE, hjust = 0) +\n  geom_text(data = gdat, aes(x = -0, y = 1, label = paste0(\"x[1] == \", round(x1, 3))), parse = TRUE, hjust = 0) +\n  transition_manual(Iter)\nga &lt;- animate(g2, fps = 20, duration = 6)\nanim_save('NMAnim2.gif', animation = ga, path='../tut/resources')\n\n\n\n\n\nNote that we are now looking for were the slope of the profile is equal to zero. This makes no distinction between a peak (maximum) and a valley (minimum) as both have a slope of zero. Provided the (negative)log-likelihood profile is monotonic (has either a single peak or valley), this should be OK. It can however be problematic if the profile has local minima and maxima. To minimize issues, it is best to select starting values (inital parameter values) that are likely to be reasonably close to the optimum values.\nOk, great. Now how do we use this approach to optimize for multiple parameters.\nRecall that the Newton-Raphson method for optimisation estimates the root by subtracting the ratio of the first order derivative of the (loss) function by the second order derivative of the function.\n x_{n+1} = x_n - \\frac{f'(x_n)}{f''(x_n)} \nWhen there are multiple parameters to estimate, then there are multiple partial gradients (slopes).\nNow that we are back to estimating the mean and variance of our example data, we have two parameters to estimate. Therefore we need:\n\nthe partial derivative of the negative log-likelihood with respect to the mean parameter\nthe partial derivative of the negative log-likelihood with respect to the sigma parameter\nthe second order partial derivative with respect to mean of the first order derivative with respect to the mean\nthe second order partial derivative with respect to sigma of the first order derivative with respect to the mean\nthe second order partial derivative with respect to mean of the first order derivative with respect to the sigma\nthe second order partial derivative with respect to sigma of the first order derivative with respect to the sigma\n\nThe second order partial derivatives form a square matrix called a Hessian matrix.\nWhen there are multiple parameters to estimate, the above formula becomes:\n\n\\boldsymbol{x_{n+1}} = \\boldsymbol{x_n} - \\frac{\\boldsymbol{g_n}}{\\boldsymbol{H_n}}\n\nwhere \\boldsymbol{g_n} is a vector of partial gradients (first order derivatives), \\boldsymbol{H_n} is a matrix of second order partial derivatives and \\boldsymbol{x_n} and \\boldsymbol{x_{n+1}} are the previous and updated parameter estimates respectively.\nRecall that matrices cannot be divided. Rather we must multiply by the matrix inverse. Hence the above equation becomes:\n\n\\boldsymbol{x_{n+1}} = \\boldsymbol{x_n} - (\\boldsymbol{H_n})^{-1}\\boldsymbol{g_n}\n\nRecall also to invert a matrix, it must be decomposed into an identity matrix and the inverse. In R, this can be achieved via either solve() or qr.solve.\nOn top of this, there is the need to calculate the first and second order derivatives for a function for which there is no equation. Hence, we need to approximate the derivatives using finite differences. That is,we can estimate a derivative (gradient at a specific point on a profile), by calculating the rise over run for a very small run (\\Delta)\n\n\\frac{df(x)}{dx} \\approx \\frac{(f(x) + \\Delta x/2) - (f(x - \\Delta x/2))}{\\Delta x}\n\nNow that all the pieces are in place, we can demonstrate this by iterating through a number of Newton-Raphson cycles to estimate the mean and sigma of our example 1 data.\n\n\n\n\n\n\nNewton-Raphson and animation code\n\n\n\n\n\n\nparams.init &lt;- c(mu = 20, sigma = 12)\n\n## The following function calculates the difference-quotient approximation of gradient\napprox.grad &lt;- function(theta, fn, eps = 1e-05) {\n    p &lt;- length(theta)\n    nf &lt;- length(fn(theta))\n    eps.mat &lt;- diag(p) * (eps/2)\n    Gmat &lt;- array(0, dim = c(nf, p))\n    for (i in 1:p) {\n        Gmat[,i] &lt;- (fn(theta + eps.mat[,i]) - fn(theta - eps.mat[,i]))/eps\n    }\n    if (nf&gt;1) Gmat else c(Gmat)\n}\n\n\noptim.NR &lt;- function(params.init, fn, x, iter.max = 250, abs.tol = 1e-05, control = list(eps = 1e-06)) {\n    fnc &lt;- function(bb) t(approx.grad(bb, fn))\n    eps &lt;- control$eps\n    gradfn &lt;- function(x) approx.grad(x, fnc, eps)\n    iter &lt;- 0; not.converged &lt;- 1; not.max.iter &lt;- 1\n    newpar &lt;- params.init\n    oldpar &lt;- params.init - 1\n    while (not.converged & not.max.iter) {\n        oldpar &lt;- newpar\n        newpar &lt;- oldpar - solve(gradfn(oldpar), t(fnc(oldpar)))\n        iter &lt;- iter + 1\n        not.max.iter &lt;- (iter &lt; iter.max)*1\n        not.converged &lt;- (abs(fn(oldpar) - fn(newpar))&gt;abs.tol)*1\n    }\n    list(iter=iter, final = as.vector(newpar), LL = fn(as.vector(newpar)), \n      gradient = fnc(newpar), hessian = gradfn(newpar))\n}\n\n\n#optim.NR(params.init, fn=function(t) LL.gaus(t, x=y), iter.max=1)\n\nmu &lt;- seq(15, 040, len = 100)\nsigma &lt;- seq(10, 20, len = 100)\ntheta &lt;- expand.grid(mu = mu, sigma = sigma)\ntheta$LL &lt;- apply(theta, 1, LL.gaus, x = dat$y)\n\nbase.plot &lt;- ggplot(data = theta, aes(y = mu, x = sigma)) +\n    geom_tile(aes(fill = LL), show.legend = FALSE) + \n    geom_contour(aes(z = LL)) +\n    scale_fill_gradientn(colors = terrain.colors(12)) +\n    scale_x_continuous(expand = c(0,0)) +\n    scale_y_continuous(expand = c(0,0))\n\nnewdata &lt;- vector('list', 10)\nnewdata[[1]] &lt;- data.frame(t(setNames(params.init, c('mu', 'sigma'))))\np.i &lt;- params.init\nfor (i in 2:10) {\n    a &lt;- optim.NR(params.init = p.i, fn = function(t) LL.gaus(t, x = dat$y), iter.max = 1)\n    newdata[[i]] &lt;- data.frame(t(setNames(a$final,c('mu','sigma'))))\n    p.i &lt;- as.vector(a$final)\n}\nnewdata &lt;- bind_rows(newdata, .id='Iter') |&gt;\n  mutate(Iter = factor(Iter, levels = unique(Iter)), nIter = as.numeric(Iter))\ng &lt;- base.plot + \n    geom_point(data = newdata, aes(y = mu, x = sigma, group = Iter), color = 'black') +\n    geom_path(data = newdata, aes(y = mu, x = sigma), color = 'black') +\n    transition_reveal(nIter) +\n    labs(title = \"Iteration: {frame_along}\")\nga=animate(g, nframes = 10, fps = 1)\n\nanim_save('NMAnim3.gif', animation = ga, path = '../tut/resources')\n\n\n\n\n\nNow lets allow the routine (optim.NR() defined in the concealed code above) to run to convergence.\n\noptim.NR(params.init, fn = function(t) LL.gaus(t, x = dat$y))\n\n$iter\n[1] 5\n\n$final\n[1] 27.81098 14.25903\n\n$LL\n[1] 40.76329\n\n$gradient\n             [,1]          [,2]\n[1,] -1.04734e-06 -5.478284e-07\n\n$hessian\n             [,1]      [,2]\n[1,] 0.0490274488 0.0000000\n[2,] 0.0007105427 0.0980549\n\n\nAgain, we see that these estimates (final in the output) are very similar to the empirical calculations. Furthermore, notice that convergence took only 5 iterations.\nThe inverse of the hessian matrix is the variance-covariance matrix. Therefore, we can also generate estimates of the standard error of the estimates:\n\nSE = \\sqrt{diag(\\boldsymbol{H})}\n\n\nH=optim.NR(params.init, fn=function(t) LL.gaus(t, x=dat$y))$hessian\nsqrt(diag(solve(H)))\n\n[1] 4.516275 3.193488\n\n\nR has an inbuilt routine (nlm()) that performs the Newton-like method for optimisation.\n\nnlm(LL.gaus, params.init, x = dat$y, hessian = TRUE, gradtol = 1e-03)\n\n$minimum\n[1] 40.76329\n\n$estimate\n[1] 27.81205 14.25769\n\n$gradient\n[1]  5.249938e-05 -1.312170e-04\n\n$hessian\n              [,1]          [,2]\n[1,]  4.919280e-02 -1.686155e-05\n[2,] -1.686155e-05  9.836416e-02\n\n$code\n[1] 1\n\n$iterations\n[1] 9\n\nH &lt;- nlm(LL.gaus, params.init, x = dat$y, hessian = TRUE, gradtol = 1e-06*2)$hessian\nsqrt(diag(solve(H)))\n\n[1] 4.509099 3.189208\n\n\n\n\nWhen the profile of a function is not smooth, derivative methods can struggle to locate a minimum efficiently (if at all) and when the profile has multiple local minima, both derivative and simplex methods can fail to converge on the “true” parameter values. Stochastic global methods add a random (stochastic) component to increase the potential of finding the global minimum when there are multiple local minima.\nIf we return briefly to our well analogy, we might appreciate that there could be multiple underground caves and depending on where our initial pilot hole is drilled, the more efficient search algorithms might quickly hone in on a point that is locally shallow yet not the shallowest location in the entire landscape. Of course repeating the search from multiple random locations could help alleviate this issue, yet it might still be difficult to discover the shallowest point if this is associated with a very abrupt (rather than gradual) underground geological feature.\nThe classic stochastic global method is called simulated annealing or the Metropolis algorithm and it works as follows:\n\nStart with some initial estimate values (one for each parameter) and calculate the loss function (e.i. the negative log-likelihood)\nPick a new set of parameter estimates close to the previous - e.g. jump a small distance in multidimensional space - and again calculate the loss function.\nIf the value of the loss function is better (lower), accept the new parameters and return to Step 2.\nIf the value of the loss function is not better,\n\nCalculate the difference (\\Delta L = L_{new} - L_{old}) in loss between the new and old parameter estimates\nPick a random number between 0 and 1\nAccept the new parameter estimates if the random number is less than e^{-\\Delta L/k} (where k is a constant that regulates the acceptance propensity), otherwise retain the previous parameter estimates.\nPeriodically (e.g. every 100 iterations), decrease k so as to reduce the acceptance propensity.\nReturn to Step 2.\n\n\nRather than have a stopping rule based on convergence, simulated annealing continues until the maximum number of iterations have been performed. Along with the capacity and encouragement to occasionally move ‘uphill’ (towards higher loss values), a large number of iterations increases the chances that the global minima will be discovered even in the presence of multiple minima. The iterations, keep track of the parameters associated with the ‘best’ configuration.\nThere are variants of this algorithm that control the jump distance used to select the next point. By adaptively increasing and reducing the jump length following acceptance and non-acceptance respectively, these variances are able to further encourage wider exploration of the parameter space.\nThe following animation illustrates 4096 iterations (stepping up in log_2 increments). The red point indicates the current ‘best’ parameter estimates. Note that whilst the algorithm ‘discovered’ the ‘best’ solution after approximately 100 iterations, it continued to explore the profile thoroughly. If there had been other minima, it is likely to have discovered them as well.\n\n\n\n\n\n\nSimulated Annealing code\n\n\n\n\n\n\noptim.SANN &lt;- function(params.init, fn, iter.max = 2500, jump = 0.05, k = 100) {\n    bestpar &lt;- newpar &lt;- oldpar &lt;- params.init\n    iter &lt;- 0; not.max.iter &lt;- 1\n    inck &lt;- k/10\n    while(not.max.iter) {\n        ## jump to new parameter set\n        newpar &lt;- oldpar + replicate(length(params.init), runif(1, -jump, jump))\n        deltaL &lt;- fn(newpar)-fn(oldpar)\n        if (deltaL&lt;0) { #improvement\n            oldpar &lt;- newpar\n        } else {\n            rnd &lt;- runif(1)\n            if (rnd &lt;= exp(-deltaL/k)) oldpar &lt;- newpar\n        }\n        if (fn(newpar)&lt;fn(bestpar)) bestpar &lt;- newpar\n        iter &lt;- iter+1\n        if ((iter %% inck)==0) k &lt;- k/10\n        not.max.iter &lt;- (iter &lt; iter.max)*1\n    }\n    list(iter = iter, final = as.vector(bestpar), LL = fn(bestpar), last = as.vector(oldpar), LL = fn(bestpar))\n}\n#optim.SANN(params.init=c(-0.25,1), fn=function(p) LL.gaus(p, x=y), iter.max=2500)\n\n\n\n\n\n\n\n\n\n\nSimulated Annealing animation code\n\n\n\n\n\n\np.i &lt;- params.init\niter.cnt &lt;- 1\niters &lt;- 2500\nnewdata &lt;- vector('list', iters)\nbestpar &lt;- p.i\nfor (i in 1:iters) {\n    a &lt;- optim.SANN(params.init = p.i, fn = function(t) LL.gaus(t, x = dat$y), iter.max = 1, jump = 0.5, k = 0.1)\n    if (LL.gaus(a$last, x = dat$y)&lt;LL.gaus(bestpar, x = dat$y)) bestpar = a$last\n    newdata[[i]] &lt;- data.frame(t(setNames(a$last, c('mu', 'sigma'))), iter = floor(log2(i))+1, t(setNames(bestpar, c('bestmu', 'bestsigma'))))\n    p.i &lt;- as.vector(a$last)\n}\nnewdata &lt;- bind_rows(newdata, .id = 'Iter') |&gt; \n  mutate(iter = factor(iter, levels = unique(iter)), nIter = as.numeric(as.character(iter)))\ng &lt;- base.plot + \n    geom_point(data = newdata, aes(y = mu, x = sigma, group = iter), color = 'black') +\n    geom_path(data = newdata, aes(y = mu, x = sigma), color = 'black') +\n    geom_point(data = newdata, aes(y = bestmu, x = bestsigma), color = 'red') + \n    transition_reveal(nIter) +\n    labs(title = \"Iteration: {2^frame_along}\")\nga &lt;- animate(g, nframes = 12, fps = 1)\n#ga\nanim_save('SANNAnim1.gif', animation = ga, path = '../tut/resources')\n\n\n\n\nAllowing the simulated annealing to iterate 2500 times (with updating k):\n\noptim.SANN(params.init = c(20, 12), fn = function(t) LL.gaus(t, x = dat$y), iter.max = 2500, jump = 0.5, k = 0.1)\n\n$iter\n[1] 2500\n\n$final\n[1] 27.79769 14.20603\n\n$LL\n[1] 40.76343\n\n$last\n[1] 28.81665 13.30448\n\n$LL\n[1] 40.76343\n\n\nWhich is again very similar the empirical estimates.",
    "crumbs": [
      "Linear modelling",
      "Generalised linear models"
    ]
  },
  {
    "objectID": "30_glm.html#fitting-glms",
    "href": "30_glm.html#fitting-glms",
    "title": "Generalised linear models",
    "section": "9.4 Fitting GLM’s",
    "text": "9.4 Fitting GLM’s\n\nExample 1 (Gaussian data)Example 3 (Poisson data)Example 4 (NB data)Example 5 (Binary data)Example 6 (Binomial data)\n\n\n\nRaw predictorCentered predictorStandardised predictor\n\n\n\nFit the modelPerformance model checkingDHARMa (simulated) residualsSummarise model\n\n\n\n\\begin{align}\ny_i \\sim{}& N(\\mu_i, \\sigma^2)\\\\\n\\mu_i =& \\beta_0 + \\beta_1 x_i\\\\\n\\end{align}\n\n\ndat.glm &lt;- glmmTMB(y~x, data = dat)\ndat.glm\n\nFormula:          y ~ x\nData: dat\n      AIC       BIC    logLik  df.resid \n 68.61231  69.52006 -31.30615         7 \n\nNumber of obs: 10\n\nDispersion estimate for gaussian family (sigma^2): 30.7 \n\nFixed Effects:\n\nConditional model:\n(Intercept)            x  \n      2.651        4.575  \n\n\nAnd to explore the diagnostics\n\n\n\ndat.glm |&gt; performance::check_model()\n\n\n\n\n\n\n\ndat.glm |&gt; performance::check_outliers()\n\nNULL\n\n\nConclusions:\n\nthese diagnostics are broadly acceptable given the small nature of the data\n\n\n\n\ndat.resid &lt;- dat.glm |&gt; \n  simulateResiduals(plot = TRUE)\n\n\n\n\n\n\n\n\nConclusions\n\nno obvious issues with these diagnostics\n\n\n\n\ndat.glm |&gt; summary()\n\n Family: gaussian  ( identity )\nFormula:          y ~ x\nData: dat\n\n     AIC      BIC   logLik deviance df.resid \n    68.6     69.5    -31.3     62.6        7 \n\n\nDispersion estimate for gaussian family (sigma^2): 30.7 \n\nConditional model:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   2.6507     3.7833   0.701    0.484    \nx             4.5746     0.6097   7.503 6.26e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ndat.glm |&gt; confint()\n\n                2.5 %    97.5 % Estimate\n(Intercept) -4.764543 10.065872 2.650664\nx            3.379537  5.769675 4.574606\n\ndat.glm |&gt; performance::r2()\n\nRandom effect variances not available. Returned R2 does not account for random effects.\n\n\n# R2 for Mixed Models\n\n  Conditional R2: NA\n     Marginal R2: 0.995\n\n\nInterpretation:\n\nwhen x=0 (e.g. the average x value since centered), y is expected to be 2.651 (the y-intercept).\nfor every one unit change in x, the expected value of y increases by 4.575 (the slope)\nthe slope could be as low as 3.38 or as high as 5.77 (95% confidence interval of the slope)\nas the above interval does not include the value of 0, the linear relationship (slope) can be considered significant\nwe would reject the null hypothesis of no relationship (p-value for slope is less than 0.05)\n99.481% of the variance in y is explained by its linear relationship with x (R squared value)\n\n\n\n\n\n\n\nFit the modelPerformance model checkingDHARMa (simulated) residualsSummarise model\n\n\n\n\\begin{align}\ny_i \\sim{}& N(\\mu_i, \\sigma^2)\\\\\n\\mu_i =& \\beta_0 + \\beta_1 (x_i - \\bar{x})\\\\\n\\end{align}\n\n\ndat1b.glm &lt;- glmmTMB(y ~ scale(x, scale = FALSE), data = dat)\ndat1b.glm\n\nFormula:          y ~ scale(x, scale = FALSE)\nData: dat\n      AIC       BIC    logLik  df.resid \n 68.61231  69.52006 -31.30615         7 \n\nNumber of obs: 10\n\nDispersion estimate for gaussian family (sigma^2): 30.7 \n\nFixed Effects:\n\nConditional model:\n            (Intercept)  scale(x, scale = FALSE)  \n                 27.811                    4.575  \n\n\nAnd to explore the diagnostics\n\n\n\ndat1b.glm |&gt; performance::check_model()\n\n\n\n\n\n\n\ndat1b.glm |&gt; performance::check_outliers()\n\nNULL\n\n\nConclusions:\n\nthese diagnostics are broadly acceptable given the small nature of the data\n\n\n\n\ndat.resid &lt;- dat1b.glm |&gt; \n  simulateResiduals(plot = TRUE)\n\n\n\n\n\n\n\n\nConclusions\n\nno obvious issues with these diagnostics\n\n\n\n\ndat1b.glm |&gt; summary()\n\n Family: gaussian  ( identity )\nFormula:          y ~ scale(x, scale = FALSE)\nData: dat\n\n     AIC      BIC   logLik deviance df.resid \n    68.6     69.5    -31.3     62.6        7 \n\n\nDispersion estimate for gaussian family (sigma^2): 30.7 \n\nConditional model:\n                        Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)              27.8110     1.7513  15.880  &lt; 2e-16 ***\nscale(x, scale = FALSE)   4.5746     0.6097   7.503 6.26e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ndat1b.glm |&gt; confint()\n\n                            2.5 %    97.5 %  Estimate\n(Intercept)             24.378427 31.243577 27.811002\nscale(x, scale = FALSE)  3.379537  5.769675  4.574606\n\ndat1b.glm |&gt; performance::r2()\n\nRandom effect variances not available. Returned R2 does not account for random effects.\n\n\n# R2 for Mixed Models\n\n  Conditional R2: NA\n     Marginal R2: 0.995\n\n\nInterpretation:\n\nwhen x=0 (e.g. the average x value since centered), y is expected to be 27.811 (the y-intercept).\nfor every one unit change in x (which represents a span of approximately 34% of the range of x), the expected value of y increases by 4.575 (the slope)\nthe slope could be as low as 3.38 or as high as 5.77 (95% confidence interval of the slope)\nas the above interval does not include the value of 0, the linear relationship (slope) can be considered significant\nwe would reject the null hypothesis of no relationship (p-value for slope is less than 0.05)\n99.481% of the variance in y is explained by its linear relationship with x (R squared value)\n\n\n\n\n\n\n\nFit the modelPerformance model checkingDHARMa (simulated) residualsSummarise model\n\n\n\n\\begin{align}\ny_i \\sim{}& N(\\mu_i, \\sigma^2)\\\\\n\\mu_i =& \\beta_0 + \\beta_1 (x_i - \\bar{x})/\\sigma_{x}\\\\\n\\end{align}\n\n\ndat1c.glm &lt;- glmmTMB(y ~ scale(x), data = dat)\ndat1c.glm\n\nFormula:          y ~ scale(x)\nData: dat\n      AIC       BIC    logLik  df.resid \n 68.61231  69.52006 -31.30615         7 \n\nNumber of obs: 10\n\nDispersion estimate for gaussian family (sigma^2): 30.7 \n\nFixed Effects:\n\nConditional model:\n(Intercept)     scale(x)  \n      27.81        13.85  \n\n\nAnd to explore the diagnostics\n\n\n\ndat1c.glm |&gt; performance::check_model()\n\n\n\n\n\n\n\ndat1c.glm |&gt; performance::check_outliers()\n\nNULL\n\n\nConclusions:\n\nthese diagnostics are broadly acceptable given the small nature of the data\n\n\n\n\ndat.resid &lt;- dat1c.glm |&gt; \n  simulateResiduals(plot = TRUE)\n\n\n\n\n\n\n\n\nConclusions\n\nno obvious issues with these diagnostics\n\n\n\n\ndat1c.glm |&gt; summary()\n\n Family: gaussian  ( identity )\nFormula:          y ~ scale(x)\nData: dat\n\n     AIC      BIC   logLik deviance df.resid \n    68.6     69.5    -31.3     62.6        7 \n\n\nDispersion estimate for gaussian family (sigma^2): 30.7 \n\nConditional model:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   27.811      1.751  15.880  &lt; 2e-16 ***\nscale(x)      13.850      1.846   7.503 6.26e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ndat1c.glm |&gt; confint()\n\n               2.5 %   97.5 % Estimate\n(Intercept) 24.37842 31.24357 27.81100\nscale(x)    10.23206 17.46856 13.85031\n\ndat1c.glm |&gt; performance::r2()\n\nRandom effect variances not available. Returned R2 does not account for random effects.\n\n\n# R2 for Mixed Models\n\n  Conditional R2: NA\n     Marginal R2: 0.995\n\n\nInterpretation:\n\nwhen x=0 (e.g. the average x value since centered), y is expected to be 27.811 (the y-intercept).\nfor every one unit change in x (which represents a span of approximately 34% of the range of x since standardised), the expected value of y increases by r   round(fixef(dat1c.glm)[[1]][2], 3) (the slope)\nthe slope could be as low as 10.232 or as high as 17.469 (95% confidence interval of the slope)\nas the above interval does not include the value of 0, the linear relationship (slope) can be considered significant\nwe would reject the null hypothesis of no relationship (p-value for slope is less than 0.05)\n99.481% of the variance in y is explained by its linear relationship with x (R squared value)\n\n\n\n\n\n\n\n\n\n\nRaw predictorCentered predictorStandardised predictor\n\n\n\nFit the modelPerformance model checkingDHARMa (simulated) residualsSummarise model\n\n\nGiven that exploratory data analysis suggested that the assumptions were not likely to be met for a simple gaussian model and that this was subsequently confirmed when we explored the diagnostics resulting from the OLS model of these data, we will instead entertain a poisson regression model. The model will be of the form:\n\n\\begin{align}\ny_i \\sim{}& Pois(\\lambda_i)\\\\\nlog(\\lambda_i) =& \\beta_0 + \\beta_1 x_i\\\\\n\\end{align}\n\n\ndat3.glm &lt;- glmmTMB(y~x, data = dat3, family = poisson(link = \"log\"))\ndat3.glm\n\nFormula:          y ~ x\nData: dat3\n      AIC       BIC    logLik  df.resid \n 50.35456  50.95973 -23.17728         8 \n\nNumber of obs: 10\n\nFixed Effects:\n\nConditional model:\n(Intercept)            x  \n    0.05136      0.34263  \n\n\nAnd to explore the diagnostics\n\n\n\ndat3.glm |&gt; performance::check_model()\n\n\n\n\n\n\n\ndat3.glm |&gt; performance::check_outliers()\n\nNULL\n\n\nConclusions:\n\nthese diagnostics are broadly acceptable given the small nature of the data\n\n\n\n\ndat.resid &lt;- dat3.glm |&gt; \n  simulateResiduals(plot = TRUE)\n\n\n\n\n\n\n\n\nConclusions\n\nno obvious issues with these diagnostics\n\n\n\n\ndat3.glm |&gt; summary()\n\n Family: poisson  ( log )\nFormula:          y ~ x\nData: dat3\n\n     AIC      BIC   logLik deviance df.resid \n    50.4     51.0    -23.2     46.4        8 \n\n\nConditional model:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  0.05136    0.35408   0.145    0.885    \nx            0.34263    0.04319   7.932 2.15e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ndat3.glm |&gt; confint()\n\n                 2.5 %    97.5 %  Estimate\n(Intercept) -0.6426377 0.7453489 0.0513556\nx            0.2579690 0.4272885 0.3426288\n\ndat3.glm |&gt; performance::r2()\n\nRandom effect variances not available. Returned R2 does not account for random effects.\n\n\n# R2 for Mixed Models\n\n  Conditional R2: NA\n     Marginal R2: 0.924\n\n\n\n\n\n\n\n\nTip\n\n\n\nSince this model used a log link, in order to return the estimates to the scale of the response, we need to backtransform by exponentiation.\nImportantly, if using confidence intervals for the purpose of inference testsing:\n\nif using estimates on the link (log) scale, intervals that do not include 0 are “significant”\nif using estimates on the response scale, intervals that do not include 1 are “significant”\n\n\n\nInterpretation:\n\nwhen x=0, y is expected to be exp(0.051) = 1.053 (the y-intercept).\nfor every one unit change in x, the expected value of y increases by exp(0.343) =\n1.409 (the slope)\nthis is equivalent to a 40.865% increase in the response per one unit change in x\nthe slope (on the link scale) could be as low as 0.258 or as high as 0.427 (95% confidence interval of the slope)\nas the above interval does not include the value of 0, the linear relationship (slope) can be considered significant\nthe slope (on the response scale) could be as low as 1.294 or as high as 1.533 (95% confidence interval of the slope)\nas the above interval does not include the value of 1, the linear relationship (slope) can be considered significant\nwe would reject the null hypothesis of no relationship (p-value for slope is less than 0.05)\n92.397% of the variance in y is explained by its linear relationship with x (R squared value)\n\n\n\n\n\n\n\nFit the modelPerformance model checkingDHARMa (simulated) residualsSummarise model\n\n\nGiven that exploratory data analysis suggested that the assumptions were not likely to be met for a simple gaussian model and that this was subsequently confirmed when we explored the diagnostics resulting from the OLS model of these data, we will instead entertain a poisson regression model. The model will be of the form:\n\n\\begin{align}\ny_i \\sim{}& Pois(\\lambda_i)\\\\\nlog(\\lambda_i) =& \\beta_0 + \\beta_1 (x_i - \\bar{x})\\\\\n\\end{align}\n\n\ndat3b.glm &lt;- glmmTMB(y ~ scale(x, scale = FALSE), data = dat3, family = poisson(link = \"log\"))\ndat3b.glm\n\nFormula:          y ~ scale(x, scale = FALSE)\nData: dat3\n      AIC       BIC    logLik  df.resid \n 50.35456  50.95973 -23.17728         8 \n\nNumber of obs: 10\n\nFixed Effects:\n\nConditional model:\n            (Intercept)  scale(x, scale = FALSE)  \n                 1.9358                   0.3426  \n\n\nAnd to explore the diagnostics\n\n\n\ndat3b.glm |&gt; performance::check_model()\n\n\n\n\n\n\n\ndat3b.glm |&gt; performance::check_outliers()\n\nNULL\n\n\nConclusions:\n\nthese diagnostics are broadly acceptable given the small nature of the data\n\n\n\n\ndat.resid &lt;- dat3b.glm |&gt; \n  simulateResiduals(plot = TRUE)\n\n\n\n\n\n\n\n\nConclusions\n\nno obvious issues with these diagnostics\n\n\n\n\ndat3b.glm |&gt; summary()\n\n Family: poisson  ( log )\nFormula:          y ~ scale(x, scale = FALSE)\nData: dat3\n\n     AIC      BIC   logLik deviance df.resid \n    50.4     51.0    -23.2     46.4        8 \n\n\nConditional model:\n                        Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)               1.9358     0.1411  13.720  &lt; 2e-16 ***\nscale(x, scale = FALSE)   0.3426     0.0432   7.932 2.15e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ndat3b.glm |&gt; confint()\n\n                            2.5 %    97.5 %  Estimate\n(Intercept)             1.6592778 2.2123498 1.9358138\nscale(x, scale = FALSE) 0.2579681 0.4272894 0.3426288\n\ndat3b.glm |&gt; performance::r2()\n\nRandom effect variances not available. Returned R2 does not account for random effects.\n\n\n# R2 for Mixed Models\n\n  Conditional R2: NA\n     Marginal R2: 0.924\n\n\n\n\n\n\n\n\nTip\n\n\n\nSince this model used a log link, in order to return the estimates to the scale of the response, we need to backtransform by exponentiation.\nImportantly, if using confidence intervals for the purpose of inference testsing:\n\nif using estimates on the link (log) scale, intervals that do not include 0 are “significant”\nif using estimates on the response scale, intervals that do not include 1 are “significant”\n\n\n\nInterpretation:\n\nwhen x=0 (e.g. the average x value since centered), y is expected to be exp(1.936) = 6.93 (the y-intercept).\nfor every one unit change in x, the expected value of y increases by exp(0.343) =\n1.409 (the slope)\nthis is equivalent to a 40.865% increase in the response per one unit change in x\nthe slope (on the link scale) could be as low as 0.258 or as high as 0.427 (95% confidence interval of the slope)\nas the above interval does not include the value of 0, the linear relationship (slope) can be considered significant\nthe slope (on the response scale) could be as low as 1.294 or as high as 1.533 (95% confidence interval of the slope)\nas the above interval does not include the value of 1, the linear relationship (slope) can be considered significant\nwe would reject the null hypothesis of no relationship (p-value for slope is less than 0.05)\n92.397% of the variance in y is explained by its linear relationship with x (R squared value)\n\n\n\n\n\n\n\nFit the modelPerformance model checkingDHARMa (simulated) residualsSummarise model\n\n\nGiven that exploratory data analysis suggested that the assumptions were not likely to be met for a simple gaussian model and that this was subsequently confirmed when we explored the diagnostics resulting from the OLS model of these data, we will instead entertain a poisson regression model. The model will be of the form:\n\n\\begin{align}\ny_i \\sim{}& Pois(\\lambda_i)\\\\\nlog(\\lambda_i) =& \\beta_0 + \\beta_1 (x_i - \\bar{x})/\\sigma_{x}\\\\\n\\end{align}\n\n\ndat3c.glm &lt;- glmmTMB(y ~ scale(x), data = dat3, family = poisson(link = \"log\"))\ndat3c.glm\n\nFormula:          y ~ scale(x)\nData: dat3\n      AIC       BIC    logLik  df.resid \n 50.35456  50.95973 -23.17728         8 \n\nNumber of obs: 10\n\nFixed Effects:\n\nConditional model:\n(Intercept)     scale(x)  \n      1.936        1.037  \n\n\nAnd to explore the diagnostics\n\n\n\ndat3c.glm |&gt; performance::check_model()\n\n\n\n\n\n\n\ndat3c.glm |&gt; performance::check_outliers()\n\nNULL\n\n\nConclusions:\n\nthese diagnostics are broadly acceptable given the small nature of the data\n\n\n\n\ndat.resid &lt;- dat3c.glm |&gt; \n  simulateResiduals(plot = TRUE)\n\n\n\n\n\n\n\n\nConclusions\n\nno obvious issues with these diagnostics\n\n\n\n\ndat3c.glm |&gt; summary()\n\n Family: poisson  ( log )\nFormula:          y ~ scale(x)\nData: dat3\n\n     AIC      BIC   logLik deviance df.resid \n    50.4     51.0    -23.2     46.4        8 \n\n\nConditional model:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   1.9358     0.1411  13.720  &lt; 2e-16 ***\nscale(x)      1.0374     0.1308   7.932 2.15e-15 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ndat3c.glm |&gt; fixef()\n\n\nConditional model:\n(Intercept)     scale(x)  \n      1.936        1.037  \n\ndat3c.glm |&gt; performance::r2()\n\nRandom effect variances not available. Returned R2 does not account for random effects.\n\n\n# R2 for Mixed Models\n\n  Conditional R2: NA\n     Marginal R2: 0.924\n\n\n\n\n\n\n\n\nTip\n\n\n\nSince this model used a log link, in order to return the estimates to the scale of the response, we need to backtransform by exponentiation.\nImportantly, if using confidence intervals for the purpose of inference testsing:\n\nif using estimates on the link (log) scale, intervals that do not include 0 are “significant”\nif using estimates on the response scale, intervals that do not include 1 are “significant”\n\n\n\nInterpretation:\n\nwhen x=0 (e.g. the average x value since centered), y is expected to be exp(1.936) = 6.93 (the y-intercept).\nfor every one unit change in x (which represents a span of approximately 34% of the range of x since standardised), the expected value of y increases by exp(1.037) = 2.822 (the slope)\nthis is equivalent to a 182.176% increase in the response per one unit change in (standardised) x\nthe slope (on the link scale) could be as low as 0.781 or as high as 1.294 (95% confidence interval of the slope)\nas the above interval does not include the value of 0, the linear relationship (slope) can be considered significant\nthe slope (on the response scale) could be as low as 2.184 or as high as 3.646 (95% confidence interval of the slope)\nas the above interval does not include the value of 1, the linear relationship (slope) can be considered significant\nwe would reject the null hypothesis of no relationship (p-value for slope is less than 0.05)\n92.397% of the variance in y is explained by its linear relationship with x (R squared value)\n\n\n\n\n\n\n\n\n\n\nRaw predictorCentered predictorStandardised predictor\n\n\n\nFit the modelPerformance model checkingDHARMa (simulated) residualsSummarise model\n\n\nGiven that exploratory data analysis suggested that the assumptions were not likely to be met for a simple gaussian model and that this was subsequently confirmed when we explored the diagnostics resulting from the OLS model of these data, we will instead entertain a Negative Binomial regression model. The model will be of the form:\n\n\\begin{align}\ny_i \\sim{}& NB(\\lambda_i, \\phi)\\\\\nlog(\\lambda_i) =& \\beta_0 + \\beta_1 x_i\\\\\n\\end{align}\n\n\ndat4.glm &lt;- glmmTMB(y~x, data = dat4, family = nbinom2(link = \"log\"))\n\nWarning in finalizeTMB(TMBStruc, obj, fit, h, data.tmb.old): Model convergence\nproblem; non-positive-definite Hessian matrix. See vignette('troubleshooting')\n\n\nWarning in finalizeTMB(TMBStruc, obj, fit, h, data.tmb.old): Model convergence\nproblem; false convergence (8). See vignette('troubleshooting'),\nhelp('diagnose')\n\ndat4.glm\n\nFormula:          y ~ x\nData: dat4\n     AIC      BIC   logLik df.resid \n      NA       NA       NA        7 \n\nNumber of obs: 10\n\nDispersion parameter for nbinom2 family (): 5.36e+07 \n\nFixed Effects:\n\nConditional model:\n(Intercept)            x  \n     0.3913       0.2792  \n\n\nAnd to explore the diagnostics\n\n\n\ndat4.glm |&gt; performance::check_model()\n\n\n\n\n\n\n\ndat4.glm |&gt; performance::check_outliers()\n\nNULL\n\n\nConclusions:\n\nthese diagnostics are broadly acceptable given the small nature of the data\n\n\n\n\ndat.resid &lt;- dat4.glm |&gt; \n  simulateResiduals(plot = TRUE)\n\n\n\n\n\n\n\n\n\ndat.resid |&gt; testDispersion()\n\n\n\n\n\n\n\n\n\n    DHARMa nonparametric dispersion test via sd of residuals fitted vs.\n    simulated\n\ndata:  simulationOutput\ndispersion = 0.56515, p-value = 0.528\nalternative hypothesis: two.sided\n\n\nConclusions\n\nno obvious issues with these diagnostics\n\n\n\n\ndat4.glm |&gt; summary()\n\n Family: nbinom2  ( log )\nFormula:          y ~ x\nData: dat4\n\n     AIC      BIC   logLik deviance df.resid \n      NA       NA       NA       NA        7 \n\n\nDispersion parameter for nbinom2 family (): 5.36e+07 \n\nConditional model:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   0.3913     0.3415   1.146    0.252    \nx             0.2792     0.0431   6.479 9.24e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ndat4.glm |&gt; confint()\n\n                 2.5 %   97.5 %  Estimate\n(Intercept) -0.2779453 1.060588 0.3913216\nx            0.1947710 0.363723 0.2792470\n\ndat4.glm |&gt; performance::r2()\n\nRandom effect variances not available. Returned R2 does not account for random effects.\n\n\n# R2 for Mixed Models\n\n  Conditional R2: NA\n     Marginal R2: 0.489\n\n\n\n\n\n\n\n\nTip\n\n\n\nSince this model used a log link, in order to return the estimates to the scale of the response, we need to backtransform by exponentiation.\nImportantly, if using confidence intervals for the purpose of inference testsing:\n\nif using estimates on the link (log) scale, intervals that do not include 0 are “significant”\nif using estimates on the response scale, intervals that do not include 1 are “significant”\n\n\n\nInterpretation:\n\nwhen x=0, y is expected to be exp(0.391) = 1.479 (the y-intercept).\nfor every one unit change in x, the expected value of y increases by exp(0.279) =\n1.322 (the slope)\nthis is equivalent to a 32.213% increase in the response per one unit change in x\nthe slope (on the link scale) could be as low as 0.195 or as high as 0.364 (95% confidence interval of the slope)\nas the above interval does not include the value of 0, the linear relationship (slope) can be considered significant\nthe slope (on the response scale) could be as low as 1.215 or as high as 1.439 (95% confidence interval of the slope)\nas the above interval does not include the value of 1, the linear relationship (slope) can be considered significant\nwe would reject the null hypothesis of no relationship (p-value for slope is less than 0.05)\n48.949% of the variance in y is explained by its linear relationship with x (R squared value)\n\n\n\n\n\n\n\nFit the modelPerformance model checkingDHARMa (simulated) residualsSummarise model\n\n\nGiven that exploratory data analysis suggested that the assumptions were not likely to be met for a simple gaussian model and that this was subsequently confirmed when we explored the diagnostics resulting from the OLS model of these data, we will instead entertain a poisson regression model. The model will be of the form:\n\n\\begin{align}\ny_i \\sim{}& NB(\\lambda_i, \\phi)\\\\\nlog(\\lambda_i) =& \\beta_0 + \\beta_1 (x_i - \\bar{x})\\\\\n\\end{align}\n\n\ndat4b.glm &lt;- glmmTMB(y ~ scale(x, scale = FALSE), data = dat4, family = nbinom2(link = \"log\"))\n\nWarning in finalizeTMB(TMBStruc, obj, fit, h, data.tmb.old): Model convergence\nproblem; false convergence (8). See vignette('troubleshooting'),\nhelp('diagnose')\n\ndat4b.glm\n\nFormula:          y ~ scale(x, scale = FALSE)\nData: dat4\n      AIC       BIC    logLik  df.resid \n 51.39412  52.30187 -22.69706         7 \n\nNumber of obs: 10\n\nDispersion parameter for nbinom2 family (): 2.01e+08 \n\nFixed Effects:\n\nConditional model:\n            (Intercept)  scale(x, scale = FALSE)  \n                 1.9272                   0.2792  \n\n\nAnd to explore the diagnostics\n\n\n\ndat4b.glm |&gt; performance::check_model()\n\n\n\n\n\n\n\ndat4b.glm |&gt; performance::check_outliers()\n\nNULL\n\n\nConclusions:\n\nthese diagnostics are broadly acceptable given the small nature of the data\n\n\n\n\ndat.resid &lt;- dat4b.glm |&gt; \n  simulateResiduals(plot = TRUE)\n\n\n\n\n\n\n\n\nConclusions\n\nno obvious issues with these diagnostics\n\n\n\n\ndat4b.glm |&gt; summary()\n\n Family: nbinom2  ( log )\nFormula:          y ~ scale(x, scale = FALSE)\nData: dat4\n\n     AIC      BIC   logLik deviance df.resid \n    51.4     52.3    -22.7     45.4        7 \n\n\nDispersion parameter for nbinom2 family (): 2.01e+08 \n\nConditional model:\n                        Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)               1.9272     0.1362  14.151  &lt; 2e-16 ***\nscale(x, scale = FALSE)   0.2792     0.0431   6.479 9.24e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ndat4b.glm |&gt; confint()\n\n                            2.5 %    97.5 % Estimate\n(Intercept)             1.6602543 2.1941059 1.927180\nscale(x, scale = FALSE) 0.1947701 0.3637238 0.279247\n\ndat4b.glm |&gt; performance::r2()\n\nRandom effect variances not available. Returned R2 does not account for random effects.\n\n\n# R2 for Mixed Models\n\n  Conditional R2: NA\n     Marginal R2: 0.489\n\n\n\n\n\n\n\n\nTip\n\n\n\nSince this model used a log link, in order to return the estimates to the scale of the response, we need to backtransform by exponentiation.\nImportantly, if using confidence intervals for the purpose of inference testsing:\n\nif using estimates on the link (log) scale, intervals that do not include 0 are “significant”\nif using estimates on the response scale, intervals that do not include 1 are “significant”\n\n\n\nInterpretation:\n\nwhen x=0 (e.g. the average x value since centered), y is expected to be exp(1.927) = 6.87 (the y-intercept).\nfor every one unit change in x, the expected value of y increases by exp(0.279) =\n1.322 (the slope)\nthis is equivalent to a 32.213% increase in the response per one unit change in x\nthe slope (on the link scale) could be as low as 0.195 or as high as 0.364 (95% confidence interval of the slope)\nas the above interval does not include the value of 0, the linear relationship (slope) can be considered significant\nthe slope (on the response scale) could be as low as 1.215 or as high as 1.439 (95% confidence interval of the slope)\nas the above interval does not include the value of 1, the linear relationship (slope) can be considered significant\nwe would reject the null hypothesis of no relationship (p-value for slope is less than 0.05)\n48.949% of the variance in y is explained by its linear relationship with x (R squared value)\n\n\n\n\n\n\n\nFit the modelPerformance model checkingDHARMa (simulated) residualsSummarise model\n\n\nGiven that exploratory data analysis suggested that the assumptions were not likely to be met for a simple gaussian model and that this was subsequently confirmed when we explored the diagnostics resulting from the OLS model of these data, we will instead entertain a poisson regression model. The model will be of the form:\n\n\\begin{align}\ny_i \\sim{}& NB(\\lambda_i, \\phi)\\\\\nlog(\\lambda_i) =& \\beta_0 + \\beta_1 (x_i - \\bar{x})/\\sigma_{x}\\\\\n\\end{align}\n\n\ndat4c.glm &lt;- glmmTMB(y ~ scale(x), data = dat4, family = nbinom2(link = \"log\"))\n\nWarning in finalizeTMB(TMBStruc, obj, fit, h, data.tmb.old): Model convergence\nproblem; non-positive-definite Hessian matrix. See vignette('troubleshooting')\n\n\nWarning in finalizeTMB(TMBStruc, obj, fit, h, data.tmb.old): Model convergence\nproblem; false convergence (8). See vignette('troubleshooting'),\nhelp('diagnose')\n\ndat4c.glm\n\nFormula:          y ~ scale(x)\nData: dat4\n     AIC      BIC   logLik df.resid \n      NA       NA       NA        7 \n\nNumber of obs: 10\n\nDispersion parameter for nbinom2 family (): 2.21e+08 \n\nFixed Effects:\n\nConditional model:\n(Intercept)     scale(x)  \n     1.9272       0.8455  \n\n\nAnd to explore the diagnostics\n\n\n\ndat4c.glm |&gt; performance::check_model()\n\n\n\n\n\n\n\ndat4c.glm |&gt; performance::check_outliers()\n\nNULL\n\n\nConclusions:\n\nthese diagnostics are broadly acceptable given the small nature of the data\n\n\n\n\ndat.resid &lt;- dat4c.glm |&gt; \n  simulateResiduals(plot = TRUE)\n\n\n\n\n\n\n\n\nConclusions\n\nno obvious issues with these diagnostics\n\n\n\n\ndat4c.glm |&gt; summary()\n\n Family: nbinom2  ( log )\nFormula:          y ~ scale(x)\nData: dat4\n\n     AIC      BIC   logLik deviance df.resid \n      NA       NA       NA       NA        7 \n\n\nDispersion parameter for nbinom2 family (): 2.21e+08 \n\nConditional model:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   1.9272     0.1362  14.151  &lt; 2e-16 ***\nscale(x)      0.8455     0.1305   6.479 9.25e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ndat4c.glm |&gt; confint()\n\n                2.5 %   97.5 %  Estimate\n(Intercept) 1.6602472 2.194100 1.9271735\nscale(x)    0.5896881 1.101223 0.8454555\n\ndat4c.glm |&gt; performance::r2()\n\nRandom effect variances not available. Returned R2 does not account for random effects.\n\n\n# R2 for Mixed Models\n\n  Conditional R2: NA\n     Marginal R2: 0.489\n\n\n\n\n\n\n\n\nTip\n\n\n\nSince this model used a log link, in order to return the estimates to the scale of the response, we need to backtransform by exponentiation.\nImportantly, if using confidence intervals for the purpose of inference testsing:\n\nif using estimates on the link (log) scale, intervals that do not include 0 are “significant”\nif using estimates on the response scale, intervals that do not include 1 are “significant”\n\n\n\nInterpretation:\n\nwhen x=0 (e.g. the average x value since centered), y is expected to be exp(1.927) = 6.87 (the y-intercept).\nfor every one unit change in x (which represents a span of approximately 34% of the range of x since standardised), the expected value of y increases by exp(0.845) = 2.329 (the slope)\nthis is equivalent to a 132.904% increase in the response per one unit change in (standardised) x\nthe slope (on the link scale) could be as low as 0.59 or as high as 1.101 (95% confidence interval of the slope)\nas the above interval does not include the value of 0, the linear relationship (slope) can be considered significant\nthe slope (on the response scale) could be as low as 1.803 or as high as 3.008 (95% confidence interval of the slope)\nas the above interval does not include the value of 1, the linear relationship (slope) can be considered significant\nwe would reject the null hypothesis of no relationship (p-value for slope is less than 0.05)\n48.948% of the variance in y is explained by its linear relationship with x (R squared value)\n\n\n\n\n\n\n\n\n\n\nRaw predictorCentered predictorStandardised predictor\n\n\n\nFit modelPerformance model checkingDHARMa (simulated) residualsSummarise model\n\n\nGiven that exploratory data analysis suggested that the assumptions were not likely to be met for a simple gaussian model and that this was subsequently confirmed when we explored the diagnostics resulting from the OLS model of these data, we will instead entertain a Bernoulli (Binomial with number of trials = 1) regression model. The model will be of the form:\n\n\\begin{align}\ny_i \\sim{}& Bin(\\pi, 1)\\\\\nlogit(\\frac{\\pi}{1-\\pi}) =& \\beta_0 + \\beta_1 x_i\\\\\n\\end{align}\n\n\ndat5.glm &lt;- glmmTMB(y ~ x, data = dat5, family = binomial(link = \"logit\"))\ndat5.glm\n\nFormula:          y ~ x\nData: dat5\n     AIC      BIC   logLik df.resid \n 9.01379  9.61896 -2.50690        8 \n\nNumber of obs: 10\n\nFixed Effects:\n\nConditional model:\n(Intercept)            x  \n     -5.825        1.295  \n\n\nAnd to explore the diagnostics\n\n\n\ndat5.glm |&gt; performance::check_model()\n\n\n\n\n\n\n\ndat5.glm |&gt; performance::check_outliers()\n\nNULL\n\n\nConclusions:\n\nthese diagnostics are broadly acceptable given the small nature of the data\n\n\n\n\ndat.resid &lt;- dat5.glm |&gt; \n  simulateResiduals(plot = TRUE)\n\n\n\n\n\n\n\n\n\ndat.resid |&gt; testDispersion()\n\n\n\n\n\n\n\n\n\n    DHARMa nonparametric dispersion test via sd of residuals fitted vs.\n    simulated\n\ndata:  simulationOutput\ndispersion = 1.3257, p-value = 0.76\nalternative hypothesis: two.sided\n\n\nConclusions\n\nno obvious issues with these diagnostics\n\n\n\n\ndat5.glm |&gt; summary()\n\n Family: binomial  ( logit )\nFormula:          y ~ x\nData: dat5\n\n     AIC      BIC   logLik deviance df.resid \n     9.0      9.6     -2.5      5.0        8 \n\n\nConditional model:\n            Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)  -5.8246     3.9861  -1.461    0.144\nx             1.2954     0.8451   1.533    0.125\n\ndat5.glm |&gt; confint()\n\n                  2.5 %   97.5 %  Estimate\n(Intercept) -13.6371807 1.987979 -5.824601\nx            -0.3609074 2.951782  1.295437\n\ndat5.glm |&gt; performance::r2()\n\nRandom effect variances not available. Returned R2 does not account for random effects.\n\n\n# R2 for Mixed Models\n\n  Conditional R2: NA\n     Marginal R2: 0.824\n\n\n\n\n\n\n\n\nTip\n\n\n\nSince this model used a logit link (log(\\frac{\\pi}{1-\\pi})), in order to return the estimates to the scale of the response, we can either:\n\nfor the y-intercept:\n\nbacktransform by exponentiation to odds ratio (\\frac{\\pi}{1-\\pi}).\nbacktransform to probability by exponentiation of the odds ratio (\\pi).\n\nfor the slopes:\n\nbacktransform by exponentiation to odds ratio.\n\n\nImportantly, if using confidence intervals for the purpose of inference testsing:\n\nif using estimates on the link (logit) scale, intervals that do not include 0 are “significant”\nif using estimates on the response scale, intervals that do not include 1 are “significant”\n\n\n\nInterpretation:\n\nwhen x=0:\n\nthe odds of y being 1 vs 0 is expected to be exp(-5.825) = 0.003 (the y-intercept).\nthe probability of y being 1 is expected to be plogis(-5.825) = 0.003 (the y-intercept).\n\nfor every one unit change in x, the odds of y increases by exp(1.295) =\n3.653 (the slope)\nthis is equivalent to a 265.259% increase in the odds of the response being 1 per one unit change in x\nthe slope (on the link scale) could be as low as -0.361 or as high as 2.952 (95% confidence interval of the slope)\nas the above interval does not include the value of 0, the linear relationship (slope) can be considered significant\nthe slope (on the response scale) could be as low as 0.697 or as high as 19.14 (95% confidence interval of the slope)\nas the above interval does not include the value of 1, the linear relationship (slope) can be considered significant\nwe would reject the null hypothesis of no relationship (p-value for slope is less than 0.05)\n82.382% of the variance in y is explained by its linear relationship with x (R squared value)\n\n\n\n\n\n\n\nFit the modelPerformance model checkingDHARMa (simulated) residualsSummarise model\n\n\nGiven that exploratory data analysis suggested that the assumptions were not likely to be met for a simple gaussian model and that this was subsequently confirmed when we explored the diagnostics resulting from the OLS model of these data, we will instead entertain a poisson regression model. The model will be of the form:\n\n\\begin{align}\ny_i \\sim{}& Bin(\\pi_i, 1)\\\\\nlog(\\frac{\\pi_i}{1-\\pi_i}) =& \\beta_0 + \\beta_1 (x_i - \\bar{x})\\\\\n\\end{align}\n\n\ndat5b.glm &lt;- glmmTMB(y ~ scale(x, scale = FALSE), data = dat5, family = binomial(link = \"logit\"))\ndat5b.glm\n\nFormula:          y ~ scale(x, scale = FALSE)\nData: dat5\n     AIC      BIC   logLik df.resid \n 9.01379  9.61896 -2.50690        8 \n\nNumber of obs: 10\n\nFixed Effects:\n\nConditional model:\n            (Intercept)  scale(x, scale = FALSE)  \n                  1.300                    1.295  \n\n\nAnd to explore the diagnostics\n\n\n\ndat5b.glm |&gt; performance::check_model()\n\n\n\n\n\n\n\ndat5b.glm |&gt; performance::check_outliers()\n\nNULL\n\n\nConclusions:\n\nthese diagnostics are broadly acceptable given the small nature of the data\n\n\n\n\ndat.resid &lt;- dat5b.glm |&gt; \n  simulateResiduals(plot = TRUE)\n\n\n\n\n\n\n\n\nConclusions\n\nno obvious issues with these diagnostics\n\n\n\n\ndat5b.glm |&gt; summary()\n\n Family: binomial  ( logit )\nFormula:          y ~ scale(x, scale = FALSE)\nData: dat5\n\n     AIC      BIC   logLik deviance df.resid \n     9.0      9.6     -2.5      5.0        8 \n\n\nConditional model:\n                        Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)               1.3003     1.4106   0.922    0.357\nscale(x, scale = FALSE)   1.2954     0.8451   1.533    0.125\n\ndat5b.glm |&gt; confint()\n\n                             2.5 %   97.5 % Estimate\n(Intercept)             -1.4643927 4.064993 1.300300\nscale(x, scale = FALSE) -0.3609129 2.951788 1.295437\n\ndat5b.glm |&gt; performance::r2()\n\nRandom effect variances not available. Returned R2 does not account for random effects.\n\n\n# R2 for Mixed Models\n\n  Conditional R2: NA\n     Marginal R2: 0.824\n\n\n\n\n\n\n\n\nTip\n\n\n\nSince this model used a logit link (log(\\frac{\\pi}{1-\\pi})), in order to return the estimates to the scale of the response, we can either:\n\nfor the y-intercept:\n\nbacktransform by exponentiation to odds ratio (\\frac{\\pi}{1-\\pi}).\nbacktransform to probability by exponentiation of the odds ratio (\\pi).\n\nfor the slopes:\n\nbacktransform by exponentiation to odds ratio.\n\n\nImportantly, if using confidence intervals for the purpose of inference testsing:\n\nif using estimates on the link (logit) scale, intervals that do not include 0 are “significant”\nif using estimates on the response scale, intervals that do not include 1 are “significant”\n\n\n\nInterpretation:\n\nwhen x=0 (e.g. the average x value since centered):\n\nthe odds of y being 1 vs 0 is expected to be exp(1.3) = 3.67 (the y-intercept).\nthe probability of y being 1 is expected to be plogis(1.3) = 0.786 (the y-intercept).\n\nfor every one unit change in x, the odds of y increases by exp(1.295) =\n3.653 (the slope)\nthis is equivalent to a 265.259% increase in the odds of the response being 1 per one unit change in x\nthe slope (on the link scale) could be as low as -0.361 or as high as 2.952 (95% confidence interval of the slope)\nas the above interval does not include the value of 0, the linear relationship (slope) can be considered significant\nthe slope (on the response scale) could be as low as 0.697 or as high as 19.14 (95% confidence interval of the slope)\nas the above interval does not include the value of 1, the linear relationship (slope) can be considered significant\nwe would reject the null hypothesis of no relationship (p-value for slope is less than 0.05)\n82.382% of the variance in y is explained by its linear relationship with x (R squared value)\n\n\n\n\n\n\n\nFit the modelPerformance model checkingDHARMa (simulated) residualsSummarise model\n\n\nGiven that exploratory data analysis suggested that the assumptions were not likely to be met for a simple gaussian model and that this was subsequently confirmed when we explored the diagnostics resulting from the OLS model of these data, we will instead entertain a poisson regression model. The model will be of the form:\n\n\\begin{align}\ny_i \\sim{}& Bin(\\pi_i, \\phi)\\\\\nlog(\\frac{\\pi_i}{1-\\pi}) =& \\beta_0 + \\beta_1 (x_i - \\bar{x})/\\sigma_{x}\\\\\n\\end{align}\n\n\ndat5c.glm &lt;- glmmTMB(y ~ scale(x), data = dat5, family = binomial(link = \"logit\"))\ndat5c.glm\n\nFormula:          y ~ scale(x)\nData: dat5\n     AIC      BIC   logLik df.resid \n 9.01379  9.61896 -2.50690        8 \n\nNumber of obs: 10\n\nFixed Effects:\n\nConditional model:\n(Intercept)     scale(x)  \n      1.300        3.922  \n\n\nAnd to explore the diagnostics\n\n\n\ndat5c.glm |&gt; performance::check_model()\n\n\n\n\n\n\n\ndat5c.glm |&gt; performance::check_outliers()\n\nNULL\n\n\nConclusions:\n\nthese diagnostics are broadly acceptable given the small nature of the data\n\n\n\n\ndat.resid &lt;- dat5c.glm |&gt; \n  simulateResiduals(plot = TRUE)\n\n\n\n\n\n\n\n\nConclusions\n\nno obvious issues with these diagnostics\n\n\n\n\ndat5c.glm |&gt; summary()\n\n Family: binomial  ( logit )\nFormula:          y ~ scale(x)\nData: dat5\n\n     AIC      BIC   logLik deviance df.resid \n     9.0      9.6     -2.5      5.0        8 \n\n\nConditional model:\n            Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)    1.300      1.411   0.922    0.357\nscale(x)       3.922      2.559   1.533    0.125\n\ndat5c.glm |&gt; confint()\n\n                2.5 %   97.5 % Estimate\n(Intercept) -1.464392 4.064999 1.300304\nscale(x)    -1.092721 8.936984 3.922131\n\ndat5c.glm |&gt; performance::r2()\n\nRandom effect variances not available. Returned R2 does not account for random effects.\n\n\n# R2 for Mixed Models\n\n  Conditional R2: NA\n     Marginal R2: 0.824\n\n\n\n\n\n\n\n\nTip\n\n\n\nSince this model used a logit link (log(\\frac{\\pi}{1-\\pi})), in order to return the estimates to the scale of the response, we can either:\n\nfor the y-intercept:\n\nbacktransform by exponentiation to odds ratio (\\frac{\\pi}{1-\\pi}).\nbacktransform to probability by exponentiation of the odds ratio (\\pi).\n\nfor the slopes:\n\nbacktransform by exponentiation to odds ratio.\n\n\nImportantly, if using confidence intervals for the purpose of inference testsing:\n\nif using estimates on the link (logit) scale, intervals that do not include 0 are “significant”\nif using estimates on the response scale, intervals that do not include 1 are “significant”\n\n\n\nInterpretation:\n\nwhen x=0 (e.g. the average x value since centered):\n\nthe odds of y being 1 vs 0 is expected to be exp(1.3) = 3.67 (the y-intercept).\nthe probability of y being 1 is expected to be plogis(1.3) = 0.786 (the y-intercept).\n\nfor every one unit change in x (which represents a span of approximately 34% of the range of x since standardised), the odds of y increases by exp(3.922) =\n50.508 (the slope)\nthis is equivalent to a 4950.799% increase in the odds of the response being 1 per one unit change in (standardised) x\nthe slope (on the link scale) could be as low as -1.093 or as high as 8.937 (95% confidence interval of the slope)\nas the above interval does not include the value of 0, the linear relationship (slope) can be considered significant\nthe slope (on the response scale) could be as low as 0.335 or as high as 7608.213 (95% confidence interval of the slope)\nas the above interval does not include the value of 1, the linear relationship (slope) can be considered significant\nwe would reject the null hypothesis of no relationship (p-value for slope is less than 0.05)\n82.382% of the variance in y is explained by its linear relationship with x (R squared value)\n\n\n\n\n\n\n\n\n\n\nRaw predictorCentered predictorStandardised predictor\n\n\n\nFit modelPerformance model checkingDHARMa (simulated) residualsSummarise model\n\n\nGiven that exploratory data analysis suggested that the assumptions were not likely to be met for a simple gaussian model and that this was subsequently confirmed when we explored the diagnostics resulting from the OLS model of these data, we will instead entertain a Binomial regression model. The model will be of the form:\n\n\\begin{align}\ny_i \\sim{}& Bin(\\pi, n)\\\\\nlogit(\\frac{\\pi}{1-\\pi}) =& \\beta_0 + \\beta_1 x_i\\\\\n\\end{align}\n\n\ndat6.glm &lt;- glmmTMB(cbind(count, total - count) ~ x, data = dat6, family = binomial(link = \"logit\"))\ndat6.glm\n\nFormula:          cbind(count, total - count) ~ x\nData: dat6\n     AIC      BIC   logLik df.resid \n22.93434 23.53951 -9.46717        8 \n\nNumber of obs: 10\n\nFixed Effects:\n\nConditional model:\n(Intercept)            x  \n    -3.3295       0.8234  \n\n\nAnd to explore the diagnostics\n\n\n\ndat6.glm |&gt; performance::check_model()\n\n\n\n\n\n\n\ndat6.glm |&gt; performance::check_outliers()\n\nNULL\n\n\nConclusions:\n\nthese diagnostics are broadly acceptable given the small nature of the data\n\n\n\n\ndat.resid &lt;- dat6.glm |&gt; \n  simulateResiduals(plot = TRUE)\n\n\n\n\n\n\n\n\n\ndat.resid |&gt; testDispersion()\n\n\n\n\n\n\n\n\n\n    DHARMa nonparametric dispersion test via sd of residuals fitted vs.\n    simulated\n\ndata:  simulationOutput\ndispersion = 1.0614, p-value = 0.76\nalternative hypothesis: two.sided\n\n\nConclusions\n\nno obvious issues with these diagnostics\n\n\n\n\ndat6.glm |&gt; summary()\n\n Family: binomial  ( logit )\nFormula:          cbind(count, total - count) ~ x\nData: dat6\n\n     AIC      BIC   logLik deviance df.resid \n    22.9     23.5     -9.5     18.9        8 \n\n\nConditional model:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -3.3295     1.0441  -3.189 0.001429 ** \nx             0.8234     0.2246   3.667 0.000246 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ndat6.glm |&gt; confint()\n\n                 2.5 %    97.5 %   Estimate\n(Intercept) -5.3758818 -1.283040 -3.3294608\nx            0.3832516  1.263571  0.8234113\n\ndat6.glm |&gt; performance::r2()\n\nRandom effect variances not available. Returned R2 does not account for random effects.\n\n\n# R2 for Mixed Models\n\n  Conditional R2: NA\n     Marginal R2: 0.654\n\n\n\n\n\n\n\n\nTip\n\n\n\nSince this model used a logit link (log(\\frac{\\pi}{1-\\pi})), in order to return the estimates to the scale of the response, we can either:\n\nfor the y-intercept:\n\nbacktransform by exponentiation to odds ratio (\\frac{\\pi}{1-\\pi}).\nbacktransform to probability by exponentiation of the odds ratio (\\pi).\n\nfor the slopes:\n\nbacktransform by exponentiation to odds ratio.\n\n\nImportantly, if using confidence intervals for the purpose of inference testsing:\n\nif using estimates on the link (logit) scale, intervals that do not include 0 are “significant”\nif using estimates on the response scale, intervals that do not include 1 are “significant”\n\n\n\nInterpretation:\n\nwhen x=0:\n\nthe odds of y being 1 vs 0 is expected to be exp(-3.329) = 0.036 (the y-intercept).\nthe probability of y being 1 is expected to be plogis(-3.329) = 0.035 (the y-intercept).\n\nfor every one unit change in x, the odds of y increases by exp(0.823) =\n2.278 (the slope)\nthis is equivalent to a 127.826% increase in the odds of the response being 1 per one unit change in x\nthe slope (on the link scale) could be as low as 0.383 or as high as 1.264 (95% confidence interval of the slope)\nas the above interval does not include the value of 0, the linear relationship (slope) can be considered significant\nthe slope (on the response scale) could be as low as 1.467 or as high as 3.538 (95% confidence interval of the slope)\nas the above interval does not include the value of 1, the linear relationship (slope) can be considered significant\nwe would reject the null hypothesis of no relationship (p-value for slope is less than 0.05)\n65.388% of the variance in y is explained by its linear relationship with x (R squared value)\n\n\n\n\n\n\n\nFit the modelPerformance model checkingDHARMa (simulated) residualsSummarise model\n\n\nGiven that exploratory data analysis suggested that the assumptions were not likely to be met for a simple gaussian model and that this was subsequently confirmed when we explored the diagnostics resulting from the OLS model of these data, we will instead entertain a poisson regression model. The model will be of the form:\n\n\\begin{align}\ny_i \\sim{}& Bin(\\pi_i, n)\\\\\nlog(\\frac{\\pi_i}{1-\\pi_i}) =& \\beta_0 + \\beta_1 (x_i - \\bar{x})\\\\\n\\end{align}\n\n\ndat6b.glm &lt;- glmmTMB(cbind(count, total - count) ~ scale(x, scale = FALSE), data = dat6, family = binomial(link = \"logit\"))\ndat6b.glm\n\nFormula:          cbind(count, total - count) ~ scale(x, scale = FALSE)\nData: dat6\n     AIC      BIC   logLik df.resid \n22.93434 23.53951 -9.46717        8 \n\nNumber of obs: 10\n\nFixed Effects:\n\nConditional model:\n            (Intercept)  scale(x, scale = FALSE)  \n                 1.1993                   0.8234  \n\n\nAnd to explore the diagnostics\n\n\n\ndat6b.glm |&gt; performance::check_model()\n\n\n\n\n\n\n\ndat6b.glm |&gt; performance::check_outliers()\n\nNULL\n\n\nConclusions:\n\nthese diagnostics are broadly acceptable given the small nature of the data\n\n\n\n\ndat.resid &lt;- dat6b.glm |&gt; \n  simulateResiduals(plot = TRUE)\n\n\n\n\n\n\n\n\nConclusions\n\nno obvious issues with these diagnostics\n\n\n\n\ndat6b.glm |&gt; summary()\n\n Family: binomial  ( logit )\nFormula:          cbind(count, total - count) ~ scale(x, scale = FALSE)\nData: dat6\n\n     AIC      BIC   logLik deviance df.resid \n    22.9     23.5     -9.5     18.9        8 \n\n\nConditional model:\n                        Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)               1.1993     0.5016   2.391 0.016801 *  \nscale(x, scale = FALSE)   0.8234     0.2246   3.667 0.000246 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ndat6b.glm |&gt; confint()\n\n                            2.5 %   97.5 %  Estimate\n(Intercept)             0.2162207 2.182382 1.1993015\nscale(x, scale = FALSE) 0.3832501 1.263573 0.8234114\n\ndat6b.glm |&gt; performance::r2()\n\nRandom effect variances not available. Returned R2 does not account for random effects.\n\n\n# R2 for Mixed Models\n\n  Conditional R2: NA\n     Marginal R2: 0.654\n\n\n\n\n\n\n\n\nTip\n\n\n\nSince this model used a logit link (log(\\frac{\\pi}{1-\\pi})), in order to return the estimates to the scale of the response, we can either:\n\nfor the y-intercept:\n\nbacktransform by exponentiation to odds ratio (\\frac{\\pi}{1-\\pi}).\nbacktransform to probability by exponentiation of the odds ratio (\\pi).\n\nfor the slopes:\n\nbacktransform by exponentiation to odds ratio.\n\n\nImportantly, if using confidence intervals for the purpose of inference testsing:\n\nif using estimates on the link (logit) scale, intervals that do not include 0 are “significant”\nif using estimates on the response scale, intervals that do not include 1 are “significant”\n\n\n\nInterpretation:\n\nwhen x=0 (e.g. the average x value since centered):\n\nthe odds of y being 1 vs 0 is expected to be exp(1.199) = 3.318 (the y-intercept).\nthe probability of y being 1 is expected to be plogis(1.199) = 0.768 (the y-intercept).\n\nfor every one unit change in x, the odds of y increases by exp(0.823) =\n2.278 (the slope)\nthis is equivalent to a 127.826% increase in the odds of the response being 1 per one unit change in x\nthe slope (on the link scale) could be as low as 0.383 or as high as 1.264 (95% confidence interval of the slope)\nas the above interval does not include the value of 0, the linear relationship (slope) can be considered significant\nthe slope (on the response scale) could be as low as 1.467 or as high as 3.538 (95% confidence interval of the slope)\nas the above interval does not include the value of 1, the linear relationship (slope) can be considered significant\nwe would reject the null hypothesis of no relationship (p-value for slope is less than 0.05)\n65.388% of the variance in y is explained by its linear relationship with x (R squared value)\n\n\n\n\n\n\n\nFit the modelPerformance model checkingDHARMa (simulated) residualsSummarise model\n\n\nGiven that exploratory data analysis suggested that the assumptions were not likely to be met for a simple gaussian model and that this was subsequently confirmed when we explored the diagnostics resulting from the OLS model of these data, we will instead entertain a poisson regression model. The model will be of the form:\n\n\\begin{align}\ny_i \\sim{}& Bin(\\pi_i, n)\\\\\nlog(\\frac{\\pi_i}{1-\\pi}) =& \\beta_0 + \\beta_1 (x_i - \\bar{x})/\\sigma_{x}\\\\\n\\end{align}\n\n\ndat6c.glm &lt;- glmmTMB(cbind(count, total - count) ~ scale(x), data = dat6, family = binomial(link = \"logit\"))\ndat6c.glm\n\nFormula:          cbind(count, total - count) ~ scale(x)\nData: dat6\n     AIC      BIC   logLik df.resid \n22.93434 23.53951 -9.46717        8 \n\nNumber of obs: 10\n\nFixed Effects:\n\nConditional model:\n(Intercept)     scale(x)  \n      1.199        2.493  \n\n\nAnd to explore the diagnostics\n\n\n\ndat6c.glm |&gt; performance::check_model()\n\n\n\n\n\n\n\ndat6c.glm |&gt; performance::check_outliers()\n\nNULL\n\n\nConclusions:\n\nthese diagnostics are broadly acceptable given the small nature of the data\n\n\n\n\ndat.resid &lt;- dat6c.glm |&gt; \n  simulateResiduals(plot = TRUE)\n\n\n\n\n\n\n\n\nConclusions\n\nno obvious issues with these diagnostics\n\n\n\n\ndat6c.glm |&gt; summary()\n\n Family: binomial  ( logit )\nFormula:          cbind(count, total - count) ~ scale(x)\nData: dat6\n\n     AIC      BIC   logLik deviance df.resid \n    22.9     23.5     -9.5     18.9        8 \n\n\nConditional model:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   1.1993     0.5016   2.391 0.016801 *  \nscale(x)      2.4930     0.6799   3.667 0.000246 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ndat6c.glm |&gt; confint()\n\n                2.5 %   97.5 % Estimate\n(Intercept) 0.2162206 2.182382 1.199301\nscale(x)    1.1603468 3.825657 2.493002\n\ndat6c.glm |&gt; performance::r2()\n\nRandom effect variances not available. Returned R2 does not account for random effects.\n\n\n# R2 for Mixed Models\n\n  Conditional R2: NA\n     Marginal R2: 0.654\n\n\n\n\n\n\n\n\nTip\n\n\n\nSince this model used a logit link (log(\\frac{\\pi}{1-\\pi})), in order to return the estimates to the scale of the response, we can either:\n\nfor the y-intercept:\n\nbacktransform by exponentiation to odds ratio (\\frac{\\pi}{1-\\pi}).\nbacktransform to probability by exponentiation of the odds ratio (\\pi).\n\nfor the slopes:\n\nbacktransform by exponentiation to odds ratio.\n\n\nImportantly, if using confidence intervals for the purpose of inference testsing:\n\nif using estimates on the link (logit) scale, intervals that do not include 0 are “significant”\nif using estimates on the response scale, intervals that do not include 1 are “significant”\n\n\n\nInterpretation:\n\nwhen x=0 (e.g. the average x value since centered):\n\nthe odds of y being 1 vs 0 is expected to be exp(1.199) = 3.318 (the y-intercept).\nthe probability of y being 1 is expected to be plogis(1.199) = 0.768 (the y-intercept).\n\nfor every one unit change in x (which represents a span of approximately 34% of the range of x since standardised), the odds of y increases by exp(2.493) =\n12.098 (the slope)\nthis is equivalent to a 1109.753% increase in the odds of the response being 1 per one unit change in (standardised) x\nthe slope (on the link scale) could be as low as 1.16 or as high as 3.826 (95% confidence interval of the slope)\nas the above interval does not include the value of 0, the linear relationship (slope) can be considered significant\nthe slope (on the response scale) could be as low as 3.191 or as high as 45.863 (95% confidence interval of the slope)\nas the above interval does not include the value of 1, the linear relationship (slope) can be considered significant\nwe would reject the null hypothesis of no relationship (p-value for slope is less than 0.05)\n65.388% of the variance in y is explained by its linear relationship with x (R squared value)",
    "crumbs": [
      "Linear modelling",
      "Generalised linear models"
    ]
  },
  {
    "objectID": "30_glm.html#summary",
    "href": "30_glm.html#summary",
    "title": "Generalised linear models",
    "section": "9.1 Summary",
    "text": "9.1 Summary\nThe callout below contains a lot of detail that some might find excessive and unnecessary if you only need to know how to fit a Generalised Linear Model (GLM). Whilst it is useful to have a more in-depth understanding of the concepts and mathematics behind statistical routines in preparation to fitting models, the follwing summary might suffice for those seeking a quicker overview.\nThe essential points are:\n\nGLM’s take on the form of:\n\n  \\begin{align}\n  Y \\sim{}& D(\\boldsymbol{\\eta}, ...)\\\\\n  g(\\boldsymbol{\\eta}) =& \\boldsymbol{X}\\boldsymbol{\\beta}\\\\\n  \\end{align}\n  \nwhere D represents the nominated distribution to model the data against and g() represents a link function that is used to map the scale of the expected values [-\\infty, \\infty] to the scale of data under the nominated distribution.\nthere is no closed form solution to find the values of the parameters that best fit the data.\nrather than find a solution based on minimising the residuals, GLM’s operate on likelihood. Likelihood is a calculation of the probability of obtaining the observed data for a specific combination of parameter values. maximising the (log) likelihood of the given set of parameters given the observed data.\nMaximum Likelihood Estimation (MLE) is the process of finding the specific combination of parameter values that maximise the (log) likelihood of observing the data.\nthere are numerous optimisation algorithms designed to efficiently explore the entire possible parameter space tp find the MLE\nthe role of many R GLM model fitting routines is to define the appropriate likelihood function for the specified model and data and to pass this on to a optimisation routine.\n\nIf all you desire is a overview, you can now skip down to the section on Fitting GLM’s.\n\n\n\n\n\n\nDetails about Maximum Likelihood and Optimasation\n\n\n\n\n\nRecall from the previous section that the simple linear model can be written as: \n\\begin{align}\ny_i =& \\beta_0 + \\beta_1 x_i + \\varepsilon_i\\\\\nY =& \\boldsymbol{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}\n\\end{align}\n\nThis could be re-written as:\n\n\\begin{align}\ny_i \\sim{}& N(\\mu_i, \\sigma^2I)\\\\\n\\mu_i =& \\beta_0 + \\beta_1 x_i\\\\\n\\end{align}\n\nindicating that the response (y) is distributed as a Gaussian (normal) the mean of which is determined by the linear relationship with x and there is a constant variance (\\sigma^2) - all observations are independently and identically distributed (iid) - independently drawn from a distribution with the same variance.\nThe I signifies an identity matrix (a square matrix whose diagonals are all one).\nMore generally, the above can be written in vector/matrix form as: \n\\begin{align}\nY \\sim{}& N(\\boldsymbol{\\mu}, \\sigma^2I)\\\\\n\\boldsymbol{\\mu} =& \\boldsymbol{X}\\boldsymbol{\\beta}\\\\\n\\end{align}\n\nIf we suspect that the residuals are not independent and identically distributed (if for example they were Poisson distributed), then we could alter (generalise) the above to:\n\n\\begin{align}\nY \\sim{}& N(\\boldsymbol{\\mu}, \\sigma^2\\boldsymbol{W}^{-1})\\\\\n\\boldsymbol{\\mu} =& \\boldsymbol{X}\\boldsymbol{\\beta}\\\\\n\\end{align}\n\nwhere \\boldsymbol{W} is a matrix of positive diagonals .\nThis allows us to generalise to other (exponential) families (such as Binomial, Poisson, Negative Binomial, Gamma etc). For example, if our response (Y) were count data, we might consider a Poisson.\n\n\\begin{align}\nY \\sim{}& Pois(\\boldsymbol{\\lambda})\\\\\n\\boldsymbol{\\lambda} =& e^{\\boldsymbol{X}\\boldsymbol{\\beta}}\\\\\n&\\text{OR equivalently}\\\\\nlog(\\boldsymbol{\\lambda}) =& \\boldsymbol{X}\\boldsymbol{\\beta}\\\\\n\\end{align}\n\nThe Poisson distribution (P(x|\\lambda) = e^{-\\lambda}\\lambda^x/x!) is parameterised by a single parameter (\\lambda) that represents both the mean and variance (as well as degrees of freedom). Poisson data are bound at the lower end by zero (negative values are not defined) - it is not possible to count fewer than 0 things. The Poisson family includes an exponential term, therefore to map the expected values back onto the natural scale (scale of the observed data), we use a link function (in the case of the Poission, this link is a log link). Hence the above can be generalized even further to:\n\n\\begin{align}\nY \\sim{}& D(\\boldsymbol{\\eta}, ...)\\\\\ng(\\boldsymbol{\\eta}) =& \\boldsymbol{X}\\boldsymbol{\\beta}\\\\\n\\end{align}\n\nwhere D is the nominated family, g is the link function and ... represents any additional parameters required by the nominated distribution (such as \\sigma^2\\boldsymbol{W}^{-1} in the case of the Gaussian distribution).\n\n9.2 Maximum likelihood\nOrdinary Least Squares provides an elegant solution for when the data satisfy certain assumptions (normality, homogeneity, independence, etc), yet for many other situations, it is not appropriate.\nFor the following demonstrations, we will use the data from Motivating example 1.\nAs with OLS, lets start with motivating example 1 data\n\ndat$y\n\n [1]  9.64  3.79 11.00 27.88 32.84 32.56 37.84 29.86 45.05 47.65\n\n\nA more general alternative to OLS is Maximum Likelihood (ML).\nLikelihood is a measure of how probable (likely) a set of observations are at following (or being drawn from) a specific distribution. For example, we could evaluate the likelihood that the observations could have come from a normal (Gaussian) distribution with a specific mean and standard deviation.\nBefore we can understand likelihood, we must first remind ourselves of a couple of things about probability.\n\nfor any continuous distribution, the probability of obtaining a specific values (that is, that a specific value (X) is equal to a particular reference values (x)) is infinitely small (Pr(X=x)=0). We can only directly estimate probabilities of obtaining values less than (or greater than) nominated reference points (quantiles).\nit is possible to calculate the probability that a specific value (X) is between two reference points (Q1 and Q2). This is just the probability of X being less than Q1 minus the probability of X being less than Q2 (Pr(Q1 &lt; X &gt; Q2)).\n\nSo although we cant estimate the probability that Pr(X=x) directly from the distributions’ function (f(x)), we can approximate this by calculating the probability that X is in the interval [x,\nx+\\Delta]:\n\n\\frac{Pr(x &lt; X \\le x + \\Delta)}{\\Delta}\n\nThe smaller \\Delta (so long as it is larger than 0), the more accurate the estimate. This becomes a simple calculus problem. The derivative (f'(x)) of the distributions’ function is a probability density function (PDF). The PDF allows us to approximate the probability of obtaining a single value from a distribution.\nProvided the data are all iid (individually and identically distributed), and thus from the same distribution, the probability (likelihood) that a set of values (X) comes from a specific distribution (described by its parameters, \\theta) can be calculated as the product of their individual probabilities.\n\nL(X|\\theta) = \\prod^n_{i=1} f'(x_i|\\theta)\n\nThe products of probability densities can soon become very small and this can lead to computation and rounding issues. Hence it is more usual to work with the logarithms of likelihood. The log laws indicate that the log of a product is the same as the sum of the individual logs (log(A\\times B) = log(A) +\nlog(B)).\n\nLL(X|\\theta) = \\sum^n_{i=1} log(f'(x_i|\\theta))\n\nThe PDF of a Gaussian distribution is:\n\nP(x|\\mu, \\sigma) = \\frac{1}{\\sqrt{\\sigma^2 2\\pi}} exp^{-\\frac{1}{2}((x-\\mu)/\\sigma)^2}\n\nSo in order to estimate the optimum values for the parameters (\\mu and \\sigma), given our data (x), we would maximize the following:\nReturning to our example,\n\n\\begin{align}\nLL(x_1, x_2, x_3, ..., x_n|\\mu, \\sigma^2) =& \\sum^n_{i=1}ln(\\frac{1}{\\sqrt{\\sigma^2 2\\pi}} exp^{-\\frac{1}{2}((x_i-\\mu)/\\sigma)^2})\\\\\n=& -\\frac{n}{2}ln(2\\pi\\sigma^2)-\\frac{1}{2\\sigma^2}\\sum^n_{i=1}(x_i-\\mu)^2\n\\end{align}\n\nNote optimization routines usually attempt to minimize rather than maximize. Nevertheless, finding the maximum of a log-likelihood (what we are after) is the same as finding the minimum of the negative log-likelihood.\nFor the examples and routines that follow, we will write our own code from first principles as well as use existing built in R functions. Although the code that we write could be written to find maximums, the inbuilt functions typically optimise on minimums. As a result, to enable us to compare routines, we will work with minimizing negative log-likelihood.\nDo, lets now write a function for calculating the negative log-likelihood for our data given a Gaussian distribution based on the formula above.\n\nLL.gaus = function(theta, x) {\n    m=theta[1]\n    s=theta[2]\n    ll = -(length(x)/2)*(log(2*pi*s^2)) + (-1/(2*s^2)) * sum((x-m)^2)\n    ##OR\n    ## ll = sum(dnorm(x, mean=m, sd=s, log=TRUE))\n    return(-ll)\n}\n\nIn a similar manner to the brute force approach we used to approximate OLS earlier, lets use a brute force approach to explore the partial negative log-likelihood profile for the mean and then approximate the mean (we will fix the standard deviation at 1). We refer to it as a partial profile, because it is holding the other parameter constant.\n\nmu = seq(15,40,len=1000)\ntheta=cbind(mu=mu,sigma=1)\nB=apply(theta, 1, LL.gaus, dat$y)\ntheta[which(B==min(B)),]\n\n      mu    sigma \n27.81281  1.00000 \n\nggplot(data=NULL) + geom_line(aes(y=B, x=theta[,'mu']))\n\n\n\n\n\n\n\n\nAgain, this estimation is very close to the ‘true’ value.\n\n\n9.3 Optimization\nOptimization algorithms are essentially search algorithms. They are attempting to find a single point in multidimensional space. There are numerous types of algorithms, each of which offers different advantages and disadvantages under different circumstances. For example, some assume that the underlying likelihood is very smooth (changes very gradually) and gain efficiencies out of being able to located minimums via differentiation. Others are less efficient, yet more accurate when the likelihood is not smooth or there are multiple local minima.\nLets use an analogy to gain a better appreciation of the problem and solutions. Imagine we had a big block of land and we wanted to install a well from which to draw water from an underground aquifer. Although there are no physical restrictions on where the well can be positioned, we are keen on it being as shallow as possible (perhaps because it is cheaper to drill a shallow well).\nThe depth from the land surface down to the aquifer is not constant over space and we want to be able to put our well in the shallowest point. Although we do not know the true underground topography, we can drill narrow pilot holes and accurately measure the depth down to the aquifer at any point in space.\nTo put this another way, although we do not know what the underground profile looks like throughout the entire spatial domain (all possible latitide/longitude values), we can estimate its value (depth) at any point (single latitude/longitude).\n\n\nTo find the optimum location for our well, we need a search algorithm. One that is able to find the latitude/longitude associated with the minimum depth. We will showcase a few different options and try to describe the advantages and disadvantages of each. For example, in our well analogy, if the depth profile was very smooth (undulated very slowly), we might be able to use approximations to the curvature of the undulations to find where any minimums are. On the other hand, if the profile is not smooth (perhaps there are underground caves or other abrupt underground geological features), such approximations may be very inaccurate and more exhaustive searching (such as a grid of pilot holes) may be required.\nJust like with the underground aquifer, although a (negative) log-likelihood function has an unknown profile in multidimensional space (one dimension for each parameter to estimate), we can evaluate it for any combination of parameters.\n\nBrute forceSimplex methodsDerivative methodsStochastic global optimisation\n\n\nOne conceptually simple way of searching for the minimum of a function is to evaluate the function for a large number of parameter combinations (perhaps in a grid). For example, to drill a pilot hole every 100m in a grid. If the grid is fine enough, it will located the minimum (maximum) no matter what the functions profile is. However, the finer the grid, the more effort is required - lower efficiency.\nThe following code chunk evaluates and plots the negative log-likelihood for a full (100\\times 100) grid of parameter combinations. Negative log-likelihood is represented as a colour along the green to white spectrum. The blue lines represent major contours in the profile. The optimum parameters (those associated with the minimum negative log-likelihood and thus maximum log-likelihood) are indicated by the black solid point.\n\nmu = seq(15,40,len=100)\nsigma=seq(10,20,len=100)\ntheta = expand.grid(mu=mu, sigma=sigma)\ntheta$LL=apply(theta, 1, LL.gaus, x=dat$y)\nggplot(data=theta,aes(y=mu, x=sigma, fill=LL)) +\n    geom_tile(show.legend=FALSE) + geom_contour(aes(z=LL)) +\n    geom_point(data=theta[which(theta$LL==min(theta$LL)),], aes(y=mu, x=sigma), fill='black') +\n    scale_fill_gradientn(colors=terrain.colors(12)) +\n    scale_x_continuous(expand=c(0,0)) +\n    scale_y_continuous(expand=c(0,0))\n\nWarning: The following aesthetics were dropped during statistical transformation: fill\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\n\n\n\n\n\n\ntheta[which(theta$LL==min(theta$LL)),]\n\n           mu    sigma       LL\n4252 27.87879 14.24242 40.76342\n\n\nThe optimum solution yielded an estimated mean of 27.8787879 and sigma of 14.2424242. These values are very similar to the empirical calculations.\nThe brute force approach is a useful illustration, but it is practically limited to just a one or two parameters and since the entire search space within the domain needs to be evaluated. This is essentially an exhaustive search algorithm. Furthermore, the accuracy is dependent on the resolution of the grid. The finer the grid, the more accurate, however it does require n^p function evaluations (where n is the number of grid increments per p parameters.\nOf course, we can always use a relatively course search grid and having identified the ‘optimum’ parameter configuration within this grid, we could apply a finer resolution grid over a narrower search domain. This is analogous to starting with a 100x100m drilling grid and then centring a 1x1m drilling grid around the ‘best’ location.\n\n\nA simplex is a multidimensional space that has one more vertex than there are parameter dimensions (p+1). In this case, we are estimating two parameters, therefore the simplex is a triangle (three vertices). The most common form of simplex optimisation is the Nelder-Mead algorithm. As with most optimisation algorithms, the Nelder-Mead algorithm is a search algorithm that aims to find a point in multidimensional space as efficiently as possible.\nOptimisation routines work on a wide variety of function (not just likelihood functions). To keep the terminology about what is being optimised general, the function to be optimised is often referred to as a loss or objective function. A loss function is any function that evaluates the performance (or fit) of a set of events (i.e. data). The further the loss is from zero, the worse the fit (hence the desire to find the minimum).\nThe Nelder-Mead algorithm can be described as (keep in mind that it is a minimisation rather than maximisation):\n\nStart with some initial estimate values - e.g. a set of p+1 vertices for each parameter.\nIdentify the vertex with the highest (ie worst) loss (negative log-likelihood).\nReflect the simplex around the centroid of the other vertices 3a. If the reflected point is better (lower negative log-likelihood) than the second worst point, but not better than the best point, replace the worst point with the reflected point. 3b. If instead, the reflected point is the best vertex, then expand this point by doubling the reflection distance\n 3b.1.  If this expanded point is better than the reflected point,\n        replace the worst point with the expanded point\n 3b.2   Otherwise replace the worst point with the reflected point.\n3c. If instead neither 3a or 3b (the reflected point is not better than the second worst point, then contract the point by halving the reflection distance.\n 3c.1.  If this contracted point is better than the worst point, replace the worst point with the contracted point\n 3c.2.  Otherwise, shrink the entire simplex (contract all vertices towards the centroid)\nRepeat Steps 2-3 until either the maximum number of iterations have occurred or the change in loss between two successive iterations is below a certain threshold.\n\nClearly, the more iterations are performed, the more accurate the estimates, and yet the longer the search will take.\nIn the well analogy, the simplex represents three points of the search triangle. Reflecting the triangle allows us to move away from the direction we know to be deeper. We expand the triangle in order to explore a new direction and contract the triangle to narrow our search area.\nCode for illustrating the process is listed as details below. This code is a modification of the code presented in https://github.com/nicolaivicol/nelder-mead-R/blob/master/optimNM.R\n\n\n\n\n\n\nSee the function\n\n\n\n\n\n\nget.optim.NM &lt;- function(X.vert, params.init, objective.fn, iter.max=250, abs.tol=0.0001, x, control=list(fnscale=1,refl=1,expan=2,contr=-0.5,shrink=0.5))\n{\n  # input dimension\n  X.len &lt;- length(params.init)\n  # initialize controls before iterations of searching\n  iter &lt;- 0; not.converged &lt;- 1; not.max.iter &lt;- 1\n  X.optim &lt;- params.init; f_X.optim &lt;- control$fnscale*objective.fn(X.optim, x=x)\n  # while loop, iterations\n  while (not.converged & not.max.iter)\n  {\n    # get values at vertices\n    f_X.vert &lt;- control$fnscale*apply(X = X.vert, MARGIN = 1, FUN = objective.fn, x) \n    # order ascending X.vert and f(X.vert), by f(X.vert)\n    X.order &lt;- sort(f_X.vert, index.return = TRUE)$ix\n    X.vert &lt;- X.vert[X.order, ]\n    f_X.vert &lt;- f_X.vert[X.order]\n    # get centroid (mean on each dimension) of all points except the worst\n    X.centr &lt;- apply(X = X.vert[1:X.len, ], MARGIN = 2, FUN = mean)\n    # get reflected point\n    X.refl &lt;- X.centr + control$refl*(X.centr - X.vert[X.len+1, ])\n    f_X.refl &lt;- control$fnscale*objective.fn(X.refl,x)\n    if ((f_X.vert[1] &lt;= f_X.refl) & (f_X.refl &lt; f_X.vert[X.len]))\n    { \n      # if the reflected point is better than the second worst, but not better than the best...\n      # ... then obtain a new simplex by replacing the worst point with the reflected point\n      X.vert[X.len+1, ] &lt;- X.refl \n    } else if (f_X.refl &lt; f_X.vert[1]) {\n      # if the reflected point is the best point so far\n      # ... then compute the expanded point\n      X.expan &lt;- X.centr + control$expan*(X.centr - X.vert[X.len+1, ])\n      f_X.expan &lt;- control$fnscale*objective.fn(X.expan,x)\n      # ... if the expanded point is better than the reflected point\n      if (f_X.expan &lt; f_X.refl)\n      {\n        # ... then obtain a new simplex by replacing the worst point with the expanded point\n        X.vert[X.len+1, ] &lt;- X.expan   \n      } else {\n        # ... else obtain a new simplex by replacing the worst point with the reflected point\n        X.vert[X.len+1, ] &lt;- X.refl\n      }\n    } else {\n      # ... reflected point is not better than second worst\n      # ... then compute the contracted point\n      X.contr &lt;- X.centr + control$contr*(X.centr - X.vert[X.len+1, ])\n      f_X.contr &lt;- control$fnscale*objective.fn(X.contr,x)\n      # ... if the contracted point is better than the worst point\n      if (f_X.contr &lt; f_X.vert[X.len+1])\n      {\n        # ... then obtain a new simplex by replacing the worst point with the contracted point\n        X.vert[X.len+1, ] &lt;- X.contr\n      } else {\n        # ... shrink the simplex: X = X1 + coef.shrink(X-X1)\n        X.vert &lt;- sweep(control$shrink*sweep(X.vert, 2, X.vert[1, ], FUN = \"-\"), 2, X.vert[1, ], FUN=\"+\")\n      }    \n    }\n    # get values at vertices\n    f_X.vert &lt;- control$fnscale*apply(X = X.vert, MARGIN = 1, FUN = objective.fn, x) \n    # order asc X.vert and f(X.vert)\n    X.order &lt;- sort(f_X.vert, index.return = TRUE)$ix\n    X.vert &lt;- X.vert[X.order, ]\n    f_X.vert &lt;- f_X.vert[X.order]   \n    # update controls\n    iter &lt;- iter + 1 \n    not.max.iter &lt;- (iter &lt; iter.max)*1\n    not.converged &lt;- (abs(control$fnscale*objective.fn(X.vert[X.len, ],x)- control$fnscale*objective.fn(X.vert[1, ],x)) &gt; abs.tol)*1\n    \n    X.optim &lt;- X.vert[1, ]; f_X.optim &lt;- control$fnscale*objective.fn(X.optim,x)\n  }\n  return(list(X.optim=X.optim, f_X.optim=f_X.optim, X.vert=X.vert, iter=iter))   \n}\n\n\n\n\nWe can illustrate the iterative process by plotting the outcome of a limited number of iterations - in this case 10 iterations. In this illustration, the filled in triangle represents the current optimum simplex. Previous simplexes remain as open triangle.\n\n## Starting values at the center of the vectices\nparams.init &lt;- c(mu = 20, sigma = 12)\nd.mu &lt;- 0.5\nd.sigma &lt;- 0.3\nsimplex &lt;- rbind(Vertex1 = params.init + c(d.mu,d.sigma),\n  Vertex2 = params.init + c(-d.mu, d.sigma),\n  Vertex3 = params.init + c(-d.mu, -d.sigma)) |&gt;\n  data.frame()\nbase.plot &lt;- ggplot(data = theta, aes(y = mu, x = sigma)) +\n  geom_tile(aes(fill = LL), show.legend = FALSE) + \n  geom_contour(aes(z = LL)) +\n  scale_fill_gradientn(colors = terrain.colors(12)) +\n  scale_x_continuous(expand = c(0,0)) +\n  scale_y_continuous(expand = c(0,0))\n\nnewdata &lt;- vector('list', 15)\na &lt;- get.optim.NM(X.vert = as.matrix(simplex), params.init, objective.fn = LL.gaus, x = dat$y, iter.max = 1, control = list(fnscale = 1, refl = 1, expan = 2, contr = -0.5, shrink = 0.5))\nnewdata[[1]] &lt;- data.frame(a$X.vert)\nfor (i in 2:15) {\n  a &lt;- get.optim.NM(X.vert = a$X.vert, params.init = a$X.optim, objective.fn = LL.gaus, x = dat$y, iter.max = 1, control = list(fnscale = 1, refl = 1, expan = 2, contr = -0.5, shrink = 0.5))\n  newdata[[i]] &lt;- data.frame(a$X.vert)\n}\nnewdata &lt;- bind_rows(newdata, .id = 'Iter') |&gt; \n  mutate(Iter=factor(Iter, levels = unique(Iter)))\ng &lt;- base.plot + \n  geom_polygon(data = newdata, aes(y = mu, x = sigma, group = Iter), \n    color = 'black', fill = 'grey40') +\n  transition_manual(Iter) + \n  shadow_trail(distance = 0.1, alpha = 0.4, color = 'grey40', fill = NA) + \n  labs(title = 'Iter: {current_frame}')\nga &lt;- animate(g, fps = 20, duration = 15)\n#ga=animate(g, renderer=av_renderer())\nanim_save('simplexAnim.gif', animation = ga, path = '../tut/resources/',  renderer =av_renderer()) \n\n\nHaving seen this illustration, we could allow the Nelder-Mead simplex optimization to iterate more thoroughly. We will now instruct the routine to iterate a maximum of 250 times. Along with setting a maximum number of iterations, most optimizations also have a stopping trigger based around the extent of improvement between iterations. This convergence tolerance defines a threshold difference below which two successive iteration outcomes are considered the same. The lower the value, the more accuracy is demanded.\nIt is also a good idea to repeat the iterations again from multiple starting configurations to ensure that any single optimization has not just settled n a local minimum (maximum).\n\nget.optim.NM(X.vert = as.matrix(simplex), params.init, objective.fn = LL.gaus, \n  x = dat$y, iter.max = 250, abs.tol = 1e-08, \n  control = list(fnscale = 1, refl = 1, expan = 2, contr = -0.5, shrink = 0.5))\n\n$X.optim\n      mu    sigma \n27.81238 14.25925 \n\n$f_X.optim\n   sigma \n40.76329 \n\n$X.vert\n              mu    sigma\nVertex3 27.81238 14.25925\nVertex2 27.80955 14.25881\nVertex1 27.81327 14.25858\n\n$iter\n[1] 32\n\nget.optim.NM(X.vert = as.matrix(simplex - c(-0.5, 0.2)), params.init, objective.fn = LL.gaus, \n  x = dat$y, iter.max = 250, abs.tol = 1e-08, \n  control = list(fnscale = 1, refl = 1, expan = 2, contr = -0.5, shrink = 0.5))\n\n$X.optim\n      mu    sigma \n27.81122 14.25812 \n\n$f_X.optim\n   sigma \n40.76329 \n\n$X.vert\n              mu    sigma\nVertex1 27.81122 14.25812\nVertex2 27.80970 14.25939\nVertex3 27.81149 14.26005\n\n$iter\n[1] 38\n\n\nThe two sets of estimated parameters (listed as X.optim) are very similar (converged) and are very similar to those calculated empirically.\nR has an inbuilt function (optim()) that is an interface to numerous optimization algorithms, and the default algorithm is Nelder-Mead. As with other optimizations, optim() defaults to minimizing. To force it to maximize (if our likelihood function returned log-likelihood rather than negative log-likelihood), we can indicate that the fnscale is -1. Other important optimization control parameters include:\n\nmaxit - the maximum number of iterations to perform (100 for Nelder-Mead).\nabstol - the absolute convergence tolerance. The lower the tolerance, the smaller the change in optimized value (log-likelihood) required before convergence is reached.\n\n\noptim(par = c(20, 12), LL.gaus, x = dat$y, control = list(fnscale = 1))\n\n$par\n[1] 27.81143 14.25771\n\n$value\n[1] 40.76329\n\n$counts\nfunction gradient \n      53       NA \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n\nWe can see that the negative log-likelihood calculation was performed 51 times before convergence. Whilst this is not the same as the number of iterations, it does provide an estimate of the total computation load.\nIf we compare this to the brute force approach (which required 100\\times 100=10,000 evaluations), the Nelder-Mead simplex approach is a substantial improvement.\n\n\nIf the profile of a function is smooth enough, and the function itself is doubly differentiable (can be differentiated into first and second order derivatives), then we can make use of a group of algorithms based on a root-finding algorithm devised by Isaac Newton. In mathematical contexts, a root is the value of the parameters (x) when the function equals zero (f(x)=0).\nA simplified version of Newton’s method, the Newton-Raphson method shows that root (x_{n+1}) of a function (f(x_n)) can be approximated by iteratively solving:\n\nx_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}\n\nwhere x_n is the initial parameter estimate, x_{n+1} is the improved parameter estimate, f(x_n) is the value of the function at x_ n and f'(x_n) is the first order derivative (the slope) of the function at x_n).\nBefore we use this approach to estimate maximum likelihood estimates, lets see how it works with a polynomial function (e.g. y=0.1x^4 +\nx^3 - x^2 + 30).\nIn the following animation, we will use the Newton-Raphson method to estimate the value of x when y=0. The animation will go through five iterations. For each iteration, the red line represents the slope associated with the initial estimate of x. The point where this line intersects with the dashed line (y=0) is the updated estimate for x. We can see that by the fifth iteration, the estimated x is has began to stabalise (converge) on a value of approximately 1.98.\n\n\n\n\n\n\nCode used to implement Newton-Raphson method for simple functions and generate animation\n\n\n\n\n\n\nNR &lt;- function(f, x, return.grid=FALSE) {\n    if (is.function(f)) f=body(f)[[2]]    \n    f_x = eval(f)\n    f._x = eval(D(f,'x'))\n    if (return.grid) {\n        list(x1=x - f_x/f._x, f_x=f_x, f._x=f._x)\n    }else {\n        list(x1=x - f_x/f._x)\n    }\n}\noptim.NR &lt;- function(f, x0, abs.tol=1.0E-6, iter.max=250, return.grid=FALSE) {\n    iter &lt;- 0; not.converged &lt;- 1; not.max.iter &lt;- 1\n    fgrid &lt;- list()\n    while (not.converged & not.max.iter) {\n        nr &lt;- NR(f, x = x0, return.grid)\n        x1 &lt;- nr$x1\n        iter &lt;- iter + 1 \n        not.max.iter &lt;- (iter &lt; iter.max)*1\n        not.converged &lt;- (abs(x0-x1) &gt; abs.tol)*1\n        if (return.grid) fgrid[[iter]] &lt;- c(x0 = x0, x1 = x1, f_x = nr$f_x, f._x = nr$f._x)\n        x0 &lt;- x1\n    }\n    list(X.optim = x0, iter = iter, grid = do.call('rbind', fgrid))\n}\n\n\nf &lt;- function(x) {0.1*x^4 + x^3 - x^2 + 10*x + 30} \na &lt;- optim.NR(f, x0 = 3, abs.tol = 0.001, return.grid = TRUE)\n\nx &lt;- seq(-4, 4, len = 100)\ndat1 &lt;- data.frame(x = x, y = f(x))\ngdat &lt;- a$grid %&gt;% as.data.frame |&gt;\n    mutate(Iter = 1:n(),\n           x = ifelse(x0&gt;0, x0+1, x0-1),\n           y = f_x + (x-x0)*f._x)\n\ng &lt;- ggplot() + geom_line(data = dat1, aes(y = y, x = x)) +\n    geom_hline(yintercept = 0, linetype = 'dashed') +\n    geom_segment(data = gdat, aes(x = x, y = y, xend = x1, yend = 0), color = 'red') +\n    geom_segment(data = gdat, aes(x = x0, xend = x0, y = 0, yend = f_x), linetype = 'dashed')+\n    geom_text(data = gdat |&gt; filter(Iter&lt;4), aes(x = x0, y = -5, label = \"x[0]\"), parse = TRUE) +\n    geom_text(data = gdat %&gt;% filter(Iter&lt;4), aes(x = x1, y = -5, label = \"x[1]\"), parse = TRUE) +\n    geom_point(data = gdat, aes(x = x0, y = f_x)) +\n    geom_text(data = gdat |&gt; filter(Iter&lt;4), aes(x = x0, y = f_x, label = \"f(x)\"), \n      parse = TRUE, nudge_x = 0.1, nudge_y = -5, hjust = 0, vjust = 0) +\n    geom_text(data = gdat |&gt; filter(Iter&lt;4), aes(x = x0+0.3+(x1-x0)/2, y = (f_x/2)-5, label = \"f*minute*(x)\"), \n      parse = TRUE, color = 'red') +\n    geom_text(data = gdat, aes(x = -4, y = 140, label = paste0(\"Iter == \", Iter)), parse = TRUE, hjust = 0) +\n    geom_text(data = gdat, aes(x = -4, y = 120, label = paste0(\"x[1] == \", round(x1, 3))), parse = TRUE, hjust = 0) +\n    transition_manual(Iter)\nga &lt;- animate(g, fps = 20, duration = 5)\nanim_save('NMAnim1.gif', animation = ga, path = '../tut/resources')\n\n\n\n\n\nIf we allow our implementation of the Newton-Raphson method to run from an initial guess of 3 until it converges:\n\noptim.NR(f, x0=3)\n\n$X.optim\n[1] -1.982394\n\n$iter\n[1] 6\n\n$grid\nNULL\n\n\nWe see that it takes just 6 to converge.\nIn the above, we estimated the root (value of x when f(x)=0) of a function. If instead, we want to find the value of x when the function is at its minimum (e.g. optimisation), then we want to find the root of the first derivative (f'(x)) of the function - that is, we want to find the value of x where the slope of f(x) is 0.\nAgain, in order to illustrate the principles of what we are trying to achieve, we must digress from our actual example. Rather than try to find the root of a function, we will now try to find the root of the derivative of a function (so as to find the minimum of a function). So we are now shifting our focus away from the profile of the function and onto the profile of the derivative of the function (since the derivative of a function is a slope profile).\nThe left hand side of the following figure represents the profile of the function (y = 0.001x^3 - 0.001x^2 - 0.3x + 5). The right hand side represents the derivative (0.001(3x^2) - 0.301(2x)) of that function.\n\nf &lt;- function(x) {0.001*x^3 - 0.001*x^2 -0.3*x + 5}\nf1 &lt;- D(body(f)[[2]], 'x')\nx &lt;- seq(0, 20, len = 100)\ndat1 &lt;- data.frame(x = x, y = f(x))\ng1 &lt;- ggplot() + \n  geom_line(data = dat1, aes(y = y, x = x)) \n\ng2 &lt;- ggplot() + \n  geom_line(data = data.frame(x = x, y = eval(f1, envi = list(x = x))), aes(y = y, x = x)) +\n  geom_hline(yintercept = 0, linetype = 'dashed')\ngrid.arrange(g1, g2, nrow = 1)\n\n\n\n\n\n\n\n(a &lt;- optim.NR(f1, x0 = 3, abs.tol = 0.001, return.grid = TRUE))\n\n$X.optim\n[1] 10.33889\n\n$iter\n[1] 6\n\n$grid\n           x0       x1           f_x       f._x\n[1,]  3.00000 20.43750 -2.790000e-01 0.01600000\n[2,] 20.43750 12.87523  9.121992e-01 0.12062500\n[3,] 12.87523 10.59535  1.715639e-01 0.07525136\n[4,] 10.59535 10.34209  1.559353e-02 0.06157209\n[5,] 10.34209 10.33889  1.924166e-04 0.06005255\n[6,] 10.33889 10.33889  3.079948e-08 0.06003333\n\n\nIf we animate the process:\n\n\n\n\n\n\nAnimation code\n\n\n\n\n\n\ndat1 &lt;- data.frame(x, y = eval(f1, env = list(x = x)))\ngdat &lt;- a$grid |&gt;\n  as.data.frame() |&gt;\n  mutate(Iter = 1:n(),\n           x = ifelse(x0&gt;0,x0+1, x0-1),\n           y = f_x + (x-x0)*f._x)\n\ng2 &lt;- ggplot() + \n  geom_line(data = dat1, aes(y = y, x = x)) +\n  geom_hline(yintercept = 0, linetype = 'dashed') +\n  geom_segment(data = gdat, aes(x = x, y = y, xend = x1, yend = 0), color = 'red') +\n  geom_segment(data = gdat, aes(x = x0, xend = x0, y = 0, yend = f_x), linetype = 'dashed') +\n  geom_text(data = gdat |&gt; filter(Iter&lt;4), aes(x = x0, y = -0.05, label = \"x[0]\"), parse = TRUE) +\n  geom_text(data = gdat |&gt; filter(Iter&lt;4), aes(x = x1, y = -0.05, label = \"x[1]\"), parse = TRUE) +\n  geom_point(data = gdat, aes(x = x0, y = f_x)) +\n  geom_text(data = gdat |&gt; filter(Iter&lt;4), aes(x = x0, y = f_x, label = \"f(x)\"), \n    parse = TRUE, nudge_x = 0.1, nudge_y = -0.05, hjust = 0, vjust = 0) +\n  geom_text(data = gdat |&gt; filter(Iter&lt;4), aes(x = x0+0.3+(x1-x0)/2, y = (f_x/2)-0.05, label = \"f*minute*(x)\"), \n    parse = TRUE, color = 'red') +\n  geom_text(data = gdat, aes(x = 0, y = 0.8, label = paste0(\"Iter == \", Iter)), \n    parse = TRUE, hjust = 0) +\n  geom_text(data = gdat, aes(x = -0, y = 1, label = paste0(\"x[1] == \", round(x1, 3))), parse = TRUE, hjust = 0) +\n  transition_manual(Iter)\nga &lt;- animate(g2, fps = 20, duration = 6)\nanim_save('NMAnim2.gif', animation = ga, path='../tut/resources')\n\n\n\n\n\nNote that we are now looking for were the slope of the profile is equal to zero. This makes no distinction between a peak (maximum) and a valley (minimum) as both have a slope of zero. Provided the (negative)log-likelihood profile is monotonic (has either a single peak or valley), this should be OK. It can however be problematic if the profile has local minima and maxima. To minimize issues, it is best to select starting values (inital parameter values) that are likely to be reasonably close to the optimum values.\nOk, great. Now how do we use this approach to optimize for multiple parameters.\nRecall that the Newton-Raphson method for optimisation estimates the root by subtracting the ratio of the first order derivative of the (loss) function by the second order derivative of the function.\n x_{n+1} = x_n - \\frac{f'(x_n)}{f''(x_n)} \nWhen there are multiple parameters to estimate, then there are multiple partial gradients (slopes).\nNow that we are back to estimating the mean and variance of our example data, we have two parameters to estimate. Therefore we need:\n\nthe partial derivative of the negative log-likelihood with respect to the mean parameter\nthe partial derivative of the negative log-likelihood with respect to the sigma parameter\nthe second order partial derivative with respect to mean of the first order derivative with respect to the mean\nthe second order partial derivative with respect to sigma of the first order derivative with respect to the mean\nthe second order partial derivative with respect to mean of the first order derivative with respect to the sigma\nthe second order partial derivative with respect to sigma of the first order derivative with respect to the sigma\n\nThe second order partial derivatives form a square matrix called a Hessian matrix.\nWhen there are multiple parameters to estimate, the above formula becomes:\n\n\\boldsymbol{x_{n+1}} = \\boldsymbol{x_n} - \\frac{\\boldsymbol{g_n}}{\\boldsymbol{H_n}}\n\nwhere \\boldsymbol{g_n} is a vector of partial gradients (first order derivatives), \\boldsymbol{H_n} is a matrix of second order partial derivatives and \\boldsymbol{x_n} and \\boldsymbol{x_{n+1}} are the previous and updated parameter estimates respectively.\nRecall that matrices cannot be divided. Rather we must multiply by the matrix inverse. Hence the above equation becomes:\n\n\\boldsymbol{x_{n+1}} = \\boldsymbol{x_n} - (\\boldsymbol{H_n})^{-1}\\boldsymbol{g_n}\n\nRecall also to invert a matrix, it must be decomposed into an identity matrix and the inverse. In R, this can be achieved via either solve() or qr.solve.\nOn top of this, there is the need to calculate the first and second order derivatives for a function for which there is no equation. Hence, we need to approximate the derivatives using finite differences. That is,we can estimate a derivative (gradient at a specific point on a profile), by calculating the rise over run for a very small run (\\Delta)\n\n\\frac{df(x)}{dx} \\approx \\frac{(f(x) + \\Delta x/2) - (f(x - \\Delta x/2))}{\\Delta x}\n\nNow that all the pieces are in place, we can demonstrate this by iterating through a number of Newton-Raphson cycles to estimate the mean and sigma of our example 1 data.\n\n\n\n\n\n\nNewton-Raphson and animation code\n\n\n\n\n\n\nparams.init &lt;- c(mu = 20, sigma = 12)\n\n## The following function calculates the difference-quotient approximation of gradient\napprox.grad &lt;- function(theta, fn, eps = 1e-05) {\n    p &lt;- length(theta)\n    nf &lt;- length(fn(theta))\n    eps.mat &lt;- diag(p) * (eps/2)\n    Gmat &lt;- array(0, dim = c(nf, p))\n    for (i in 1:p) {\n        Gmat[,i] &lt;- (fn(theta + eps.mat[,i]) - fn(theta - eps.mat[,i]))/eps\n    }\n    if (nf&gt;1) Gmat else c(Gmat)\n}\n\n\noptim.NR &lt;- function(params.init, fn, x, iter.max = 250, abs.tol = 1e-05, control = list(eps = 1e-06)) {\n    fnc &lt;- function(bb) t(approx.grad(bb, fn))\n    eps &lt;- control$eps\n    gradfn &lt;- function(x) approx.grad(x, fnc, eps)\n    iter &lt;- 0; not.converged &lt;- 1; not.max.iter &lt;- 1\n    newpar &lt;- params.init\n    oldpar &lt;- params.init - 1\n    while (not.converged & not.max.iter) {\n        oldpar &lt;- newpar\n        newpar &lt;- oldpar - solve(gradfn(oldpar), t(fnc(oldpar)))\n        iter &lt;- iter + 1\n        not.max.iter &lt;- (iter &lt; iter.max)*1\n        not.converged &lt;- (abs(fn(oldpar) - fn(newpar))&gt;abs.tol)*1\n    }\n    list(iter=iter, final = as.vector(newpar), LL = fn(as.vector(newpar)), \n      gradient = fnc(newpar), hessian = gradfn(newpar))\n}\n\n\n#optim.NR(params.init, fn=function(t) LL.gaus(t, x=y), iter.max=1)\n\nmu &lt;- seq(15, 040, len = 100)\nsigma &lt;- seq(10, 20, len = 100)\ntheta &lt;- expand.grid(mu = mu, sigma = sigma)\ntheta$LL &lt;- apply(theta, 1, LL.gaus, x = dat$y)\n\nbase.plot &lt;- ggplot(data = theta, aes(y = mu, x = sigma)) +\n    geom_tile(aes(fill = LL), show.legend = FALSE) + \n    geom_contour(aes(z = LL)) +\n    scale_fill_gradientn(colors = terrain.colors(12)) +\n    scale_x_continuous(expand = c(0,0)) +\n    scale_y_continuous(expand = c(0,0))\n\nnewdata &lt;- vector('list', 10)\nnewdata[[1]] &lt;- data.frame(t(setNames(params.init, c('mu', 'sigma'))))\np.i &lt;- params.init\nfor (i in 2:10) {\n    a &lt;- optim.NR(params.init = p.i, fn = function(t) LL.gaus(t, x = dat$y), iter.max = 1)\n    newdata[[i]] &lt;- data.frame(t(setNames(a$final,c('mu','sigma'))))\n    p.i &lt;- as.vector(a$final)\n}\nnewdata &lt;- bind_rows(newdata, .id='Iter') |&gt;\n  mutate(Iter = factor(Iter, levels = unique(Iter)), nIter = as.numeric(Iter))\ng &lt;- base.plot + \n    geom_point(data = newdata, aes(y = mu, x = sigma, group = Iter), color = 'black') +\n    geom_path(data = newdata, aes(y = mu, x = sigma), color = 'black') +\n    transition_reveal(nIter) +\n    labs(title = \"Iteration: {frame_along}\")\nga=animate(g, nframes = 10, fps = 1)\n\nanim_save('NMAnim3.gif', animation = ga, path = '../tut/resources')\n\n\n\n\n\nNow lets allow the routine (optim.NR() defined in the concealed code above) to run to convergence.\n\noptim.NR(params.init, fn = function(t) LL.gaus(t, x = dat$y))\n\n$iter\n[1] 5\n\n$final\n[1] 27.81098 14.25903\n\n$LL\n[1] 40.76329\n\n$gradient\n             [,1]          [,2]\n[1,] -1.04734e-06 -5.478284e-07\n\n$hessian\n             [,1]      [,2]\n[1,] 0.0490274488 0.0000000\n[2,] 0.0007105427 0.0980549\n\n\nAgain, we see that these estimates (final in the output) are very similar to the empirical calculations. Furthermore, notice that convergence took only 5 iterations.\nThe inverse of the hessian matrix is the variance-covariance matrix. Therefore, we can also generate estimates of the standard error of the estimates:\n\nSE = \\sqrt{diag(\\boldsymbol{H})}\n\n\nH=optim.NR(params.init, fn=function(t) LL.gaus(t, x=dat$y))$hessian\nsqrt(diag(solve(H)))\n\n[1] 4.516275 3.193488\n\n\nR has an inbuilt routine (nlm()) that performs the Newton-like method for optimisation.\n\nnlm(LL.gaus, params.init, x = dat$y, hessian = TRUE, gradtol = 1e-03)\n\n$minimum\n[1] 40.76329\n\n$estimate\n[1] 27.81205 14.25769\n\n$gradient\n[1]  5.249938e-05 -1.312170e-04\n\n$hessian\n              [,1]          [,2]\n[1,]  4.919280e-02 -1.686155e-05\n[2,] -1.686155e-05  9.836416e-02\n\n$code\n[1] 1\n\n$iterations\n[1] 9\n\nH &lt;- nlm(LL.gaus, params.init, x = dat$y, hessian = TRUE, gradtol = 1e-06*2)$hessian\nsqrt(diag(solve(H)))\n\n[1] 4.509099 3.189208\n\n\n\n\nWhen the profile of a function is not smooth, derivative methods can struggle to locate a minimum efficiently (if at all) and when the profile has multiple local minima, both derivative and simplex methods can fail to converge on the “true” parameter values. Stochastic global methods add a random (stochastic) component to increase the potential of finding the global minimum when there are multiple local minima.\nIf we return briefly to our well analogy, we might appreciate that there could be multiple underground caves and depending on where our initial pilot hole is drilled, the more efficient search algorithms might quickly hone in on a point that is locally shallow yet not the shallowest location in the entire landscape. Of course repeating the search from multiple random locations could help alleviate this issue, yet it might still be difficult to discover the shallowest point if this is associated with a very abrupt (rather than gradual) underground geological feature.\nThe classic stochastic global method is called simulated annealing or the Metropolis algorithm and it works as follows:\n\nStart with some initial estimate values (one for each parameter) and calculate the loss function (e.i. the negative log-likelihood)\nPick a new set of parameter estimates close to the previous - e.g. jump a small distance in multidimensional space - and again calculate the loss function.\nIf the value of the loss function is better (lower), accept the new parameters and return to Step 2.\nIf the value of the loss function is not better,\n\nCalculate the difference (\\Delta L = L_{new} - L_{old}) in loss between the new and old parameter estimates\nPick a random number between 0 and 1\nAccept the new parameter estimates if the random number is less than e^{-\\Delta L/k} (where k is a constant that regulates the acceptance propensity), otherwise retain the previous parameter estimates.\nPeriodically (e.g. every 100 iterations), decrease k so as to reduce the acceptance propensity.\nReturn to Step 2.\n\n\nRather than have a stopping rule based on convergence, simulated annealing continues until the maximum number of iterations have been performed. Along with the capacity and encouragement to occasionally move ‘uphill’ (towards higher loss values), a large number of iterations increases the chances that the global minima will be discovered even in the presence of multiple minima. The iterations, keep track of the parameters associated with the ‘best’ configuration.\nThere are variants of this algorithm that control the jump distance used to select the next point. By adaptively increasing and reducing the jump length following acceptance and non-acceptance respectively, these variances are able to further encourage wider exploration of the parameter space.\nThe following animation illustrates 4096 iterations (stepping up in log_2 increments). The red point indicates the current ‘best’ parameter estimates. Note that whilst the algorithm ‘discovered’ the ‘best’ solution after approximately 100 iterations, it continued to explore the profile thoroughly. If there had been other minima, it is likely to have discovered them as well.\n\n\n\n\n\n\nSimulated Annealing code\n\n\n\n\n\n\noptim.SANN &lt;- function(params.init, fn, iter.max = 2500, jump = 0.05, k = 100) {\n    bestpar &lt;- newpar &lt;- oldpar &lt;- params.init\n    iter &lt;- 0; not.max.iter &lt;- 1\n    inck &lt;- k/10\n    while(not.max.iter) {\n        ## jump to new parameter set\n        newpar &lt;- oldpar + replicate(length(params.init), runif(1, -jump, jump))\n        deltaL &lt;- fn(newpar)-fn(oldpar)\n        if (deltaL&lt;0) { #improvement\n            oldpar &lt;- newpar\n        } else {\n            rnd &lt;- runif(1)\n            if (rnd &lt;= exp(-deltaL/k)) oldpar &lt;- newpar\n        }\n        if (fn(newpar)&lt;fn(bestpar)) bestpar &lt;- newpar\n        iter &lt;- iter+1\n        if ((iter %% inck)==0) k &lt;- k/10\n        not.max.iter &lt;- (iter &lt; iter.max)*1\n    }\n    list(iter = iter, final = as.vector(bestpar), LL = fn(bestpar), last = as.vector(oldpar), LL = fn(bestpar))\n}\n#optim.SANN(params.init=c(-0.25,1), fn=function(p) LL.gaus(p, x=y), iter.max=2500)\n\n\n\n\n\n\n\n\n\n\nSimulated Annealing animation code\n\n\n\n\n\n\np.i &lt;- params.init\niter.cnt &lt;- 1\niters &lt;- 2500\nnewdata &lt;- vector('list', iters)\nbestpar &lt;- p.i\nfor (i in 1:iters) {\n    a &lt;- optim.SANN(params.init = p.i, fn = function(t) LL.gaus(t, x = dat$y), iter.max = 1, jump = 0.5, k = 0.1)\n    if (LL.gaus(a$last, x = dat$y)&lt;LL.gaus(bestpar, x = dat$y)) bestpar = a$last\n    newdata[[i]] &lt;- data.frame(t(setNames(a$last, c('mu', 'sigma'))), iter = floor(log2(i))+1, t(setNames(bestpar, c('bestmu', 'bestsigma'))))\n    p.i &lt;- as.vector(a$last)\n}\nnewdata &lt;- bind_rows(newdata, .id = 'Iter') |&gt; \n  mutate(iter = factor(iter, levels = unique(iter)), nIter = as.numeric(as.character(iter)))\ng &lt;- base.plot + \n    geom_point(data = newdata, aes(y = mu, x = sigma, group = iter), color = 'black') +\n    geom_path(data = newdata, aes(y = mu, x = sigma), color = 'black') +\n    geom_point(data = newdata, aes(y = bestmu, x = bestsigma), color = 'red') + \n    transition_reveal(nIter) +\n    labs(title = \"Iteration: {2^frame_along}\")\nga &lt;- animate(g, nframes = 12, fps = 1)\n#ga\nanim_save('SANNAnim1.gif', animation = ga, path = '../tut/resources')\n\n\n\n\nAllowing the simulated annealing to iterate 2500 times (with updating k):\n\noptim.SANN(params.init = c(20, 12), fn = function(t) LL.gaus(t, x = dat$y), iter.max = 2500, jump = 0.5, k = 0.1)\n\n$iter\n[1] 2500\n\n$final\n[1] 27.79769 14.20603\n\n$LL\n[1] 40.76343\n\n$last\n[1] 28.81665 13.30448\n\n$LL\n[1] 40.76343\n\n\nWhich is again very similar the empirical estimates.",
    "crumbs": [
      "Linear modelling",
      "Generalised linear models"
    ]
  },
  {
    "objectID": "005_setup.html",
    "href": "005_setup.html",
    "title": "Setup instructions",
    "section": "",
    "text": "1 Installing R\nThe latest version of an R installation binary (or source code) can be downloaded from one of the Comprehensive R Archive Network (or CRAN) mirrors. Having selected one of the (Australian) mirrors, follow one of the sets of instructions below (depending on your operating system).\n\nWindowsMacOSxLinux\n\n\n\nDownload R:\n\nGo to the CRAN R-project website https://cran.r-project.org/ and click on “Download R for Windows”.\nSelect the “base” subdirectory\nSelect the “Download R-X.X.X for Windows” option (where X.X.X are a series of version and release numbers) to download.\n\nRun the installer: Double-click the downloaded .exe file and follow the installation wizard. Accept the default settings unless you have specific needs.\nOptional: Set R as the default: Check the checkbox to set R as the default for R scripts during installation. This allows you to run R scripts by double-clicking them.\nVerify installation:\n\nOpen a new command prompt (Start &gt; Run &gt; cmd) and type R. If the R console opens, the installation was successful.\nAlternatively, search for R in the Start menu\n\n\n\n\n\nDownload R:\n\nGo to the CRAN R-project website (https://cran.r-project.org/) and click on “Download R for macOS”.\nChoose the latest stable version that is appropriate for your architecture.\n\nOpen the disk image: Double-click the downloaded .pkg file and drag the R application icon to your Applications folder.\nVerify installation:\n\nOpen Terminal: Go to Applications &gt; Utilities and open Terminal.\nType R in the Terminal window. If the R console opens, the installation was successful.\n\n\n\n\n\nOpen Terminal: You can access Terminal through your application launcher or search bar.\nInstall R: The commands vary slightly depending on your Linux distribution. Here are common examples:\n\nDebian/Ubuntu: sudo apt install r-base\nFedora/CentOS: sudo yum install R\nArch Linux: sudo pacman -S R\n\nVerify installation: Type R in the Terminal window. If the R console opens, the installation was successful.\n\n\n\n\n\n\n2 Installing Rstudio\n\nInstalling RStudio on Windows:Installing RStudio on macOS:Installing RStudio on Linux:\n\n\n\nDownload R:\n\nRStudio requires R to be installed. If you have not already done so, download and install R from the official CRAN website.\n\nDownload RStudio:\n\nVisit the RStudio Download page and select the “RStudio Desktop” version compatible with your Windows operating system.\n\nInstall RStudio:\n\nRun the downloaded RStudio installer and follow the installation wizard.\nAccept the default settings unless you have specific preferences.\n\nLaunch RStudio:\n\nAfter installation, launch RStudio from the Start menu or desktop shortcut.\n\n\n\n\n\nDownload R:\n\nIf you have not already done so, download and install R on macOS from the official CRAN website.\n\nDownload RStudio:\n\nNavigate to the RStudio Download page and choose the “RStudio Desktop” version for macOS.\n\nInstall RStudio:\n\nRun the downloaded RStudio package, and macOS will guide you through the installation process.\n\nLaunch RStudio:\n\nOpen RStudio from the Applications folder or use Spotlight to search for it.\n\n\n\n\n\nDownload R:\n\nIf you have not already done so, install R on your Linux distribution using the package manager. For example, on Ubuntu, run:\n\n\n\nsudo apt-get install r-base\n\n\nDownload RStudio:\n\nVisit the RStudio Download page and choose the appropriate RStudio Desktop version for your Linux distribution.\n\nInstall RStudio:\n\nRun the downloaded RStudio package, and follow any additional instructions based on your Linux distribution.\n\nLaunch RStudio:\n\nOpen a terminal and type rstudio to launch RStudio.\n\n\n\n\n\n\n\n3 Installating git\n\nWindowsMacOSxLinux\n\n\nGit Bash (Command Line Version):\n\nDownload the Git for Windows installer from Git for Windows\n\nClick the Download button\nSelect the latest version from the list of Assets\n\nRun the installer and follow the installation prompts.\nChoose the default options unless you have specific preferences.\nSelect the default text editor (usually Vim) or choose another editor like Nano or Notepad++.\nChoose to use Git from the Windows Command Prompt (recommended).\nComplete the installation.\n\n\n\nUsing Homebrew:\n\nOpen Terminal.\nInstall Homebrew if not installed:\n\n\n\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n\n\n\nInstall Git using Homebrew:\n\n\n\nbrew install git\n\n\n\n\n\nOpen Terminal.\n\n\nUbuntu/Debian:\n\nsudo apt update\nsudo apt install git\n\n\n\nFedora:\n\nsudo dnf install git\n\n\n\nArch Linux:\n\nsudo pacman -S git\n\n\n\nLinux (Red Hat/CentOS):\n\nsudo yum install git\n\n\n\n\n\nTo verify that the software is installed and accessible, open a terminal and issue the following:\n\ngit --version\n\ngit version 2.43.2\n\n\n\n\n\n\n\n\nUnsure how to open a terminal?\n\n\n\n\n\nWindows:\nOn Windows, you can access a terminal via one of the following:\n\nvia the command Prompt:\n\nPress Win + R to open the Run dialog.\nType cmd and press Enter.\n\nvia PowerShell:\n\nPress Win + X and select “Windows PowerShell.”\n\nGit Bash (Optional):\n\nif Git is installed (which we are hoping it is!), open “Git Bash” for a Unix-like terminal experience.\n\n\nMacOS:\n\nvia Terminal:\n\nPress Cmd + Space to open Spotlight.\nType terminal and press Enter.\n\n\nLinux:\nOh please. You cannot seriously tell me that you are using Linux and don’t know how to access a terminal.\n\n\n\nIn the command above, pay particular attention to the number of hyphens in the above command - there are two in a row and no spaces between the -- and the word version.\nIf you get output similar to above (an indication of what version of git you have on your system), then it is likely to be properly installed. If instead you get an error message, then it is likely that git is not properly installed and you should try again.\n\n\n4 Setup a free github account\nTo create a free github account:\n\nvisit https://github.com and click “Sign up for github”\nregister by providing your prefered email address, a username and a password when prompted\nto complete the account activation, you will need to verify your details via an email sent to your nominated email address\n\nAs of the start of 2024, github now requires Two-Factor Authentication (2FA) for enhanced security. Whenever you login to github (or are prompted for a password, you will also need to use 2FA. To setup 2FA:\n\nclick on your profile picture in the top right corner.\nselect “Settings” from the dropdown menu.\nselect “Password and authentication” in the left sidebar.\nunder “Two-factor authentication” section, click “Enable”.\nchoose your preferred method (authenticator app or SMS) and follow the prompts to set it up.\n\nPasswords and Two-Factor Authentication (2FA) are used when you (as a human) securely login and interact directly with the GitHub website. However, it is also possible to have other tools (such as git) interact with Github on your behalf via an Application Programming Interfacet (API). Passwords/2FA are not appropriate to authenticate these machine to machine communications. Instead, Github requires the use of a Personal Access Token (PAT). PATs offer a more secure and granular approach, allowing users to control access without exposing their account password.\nTo generate a Personal Access Token (PAT):\n\nclick on your profile picture in the top right corner.\nselect “Settings” from the dropdown menu.\nselect “Developer settings” from the bottom of the left sidebar.\nselect “Personal access tokens” from the left sidebar.\nselect “Tokens (classic)” from the dropdown menu\nclick “Generate new token”\nselect “Generate new token (classic)” from the dropdown menu\nat this point you will likely be prompted for your password\nprovide a “note” - this is more of a short description of what the token is to be used for (in the example below, I have entered “git push/pull” to remind me that this is a simple token for regular push/pull interaction between my local and remote repositories).\n\nYou also need to provide an expiration. Although not secure or recommended, I have selected “No expiration” as I don’t want to have to re-do my PAT across multiple machines too regularly.\nFinally, you also need to indicate scope (what activities you are granting permission for the tools to be able to perform). In this case, I have ticked the “repo” box. This grants general rea/write access to my repositories. I have not granted permission for more administration like activities such as managing teams, deleting repositories, etc - these activities I am happy to perform myself via the website.\nclick “Generate token” and securely copy the generated token. Until this is stored safely (see below) do not close the page, because Github will never show you this PAT again.\n\n\n\n\n\n\n\n\nImportant\n\n\n\n\n\nImportant: Store your PAT safely as you won’t be able to see it again! Ideally, you should store this PAT in a digital wallet. Digital wallets vary according to operating systems. R users might like to use the r function from the asdf package (which you will need to install prior) as follows in order to store the PAT.\nIn an R console, enter:\n\ngitcreds::gitcreds_set()\n\nWhen propted for a password, paste in the copied PAT that hopefully is still in your clipboard - else you might need to re-copy it.\nTo confirm that you have successfully stored your PAT in your wallet, you can:\n\ngitcreds::gitcreds_get()\n\nand confirm that it indicates that there is a hidden password.\n\n\n\n\n\n\n5 Installing stan (for Bayesian modelling)\n\nWindowsMacOSxLinux\n\n\n\nInstall and setup Rtools (a collection of R focused build tools for windows)\n\ngo to CRAN Rtools website https://cran.r-project.org/bin/windows/Rtools/\nclick on the Rtools version that matches the major version of R you are using.\nclick on the installer link (midway down the page) to download the installer\nrun the installer\nfollow all defaults during the installation process\n\nInstall cmdstan (an implementation of the STAN language)\n\nusing selected instructions from https://mc-stan.org/cmdstanr/articles/cmdstanr.html\n\nopen a new R session and issue the following\n\ninstall.packages(\"cmdstanr\", repos = c(\"https://mc-stan.org/r-packages/\", getOption(\"repos\")))\n\nmake sure the package loads\n\nlibrary(cmdstanr)\n\nThis is cmdstanr version 0.7.1\n\n\n- CmdStanR documentation and vignettes: mc-stan.org/cmdstanr\n\n\n- CmdStan path: /home/runner/.cmdstan/cmdstan-2.34.1\n\n\n- CmdStan version: 2.34.1\n\n\nensure that the c++ toolchain (from Rtools) is correctly installed and configured\n\ncheck_cmdstan_toolchain()\n\nif the toolchain is correctly configured, install cmdstan\n\ninstall_cmdstan(cores = 2)\n\n\n\nEnsure that cmdstan is properly configured by compiling a built in example\n\nfile &lt;- file.path(cmdstan_path(), \"examples\", \"bernoulli\", \"bernoulli.stan\")\nmod &lt;- cmdstan_model(file)\ndata_list &lt;- list(N = 10, y = c(0,1,0,0,0,0,0,0,0,1))\n\nfit &lt;- mod$sample(\n  data = data_list,\n  seed = 123,\n  chains = 4,\n  parallel_chains = 4,\n  refresh = 500 # print update every 500 iters\n)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 0.0 seconds.\nChain 2 finished in 0.0 seconds.\nChain 3 finished in 0.0 seconds.\nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.3 seconds.\n\n\nIf you get output resembling the above, then cmdstan is setup correctly.\nInstall the brms - an R package that provides a more familiar R model fitting interface to STAN.\n\ninstall the package\n\ninstall.packages(\"brms\")\n\ntest whether the whole tool chain works\n\nlibrary(cmdstanr)\nlibrary(brms)\n\nLoading required package: Rcpp\n\n\nLoading 'brms' package (version 2.21.0). Useful instructions\ncan be found by typing help('brms'). A more detailed introduction\nto the package is available through vignette('brms_overview').\n\n\n\nAttaching package: 'brms'\n\n\nThe following object is masked from 'package:stats':\n\n    ar\n\ndat &lt;- data.frame(y = rnorm(10), x = rnorm(10))\nbrm(y ~ x, data = dat)\n\nCompiling Stan program...\n\n\nStart sampling\n\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 9e-06 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.09 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.013 seconds (Warm-up)\nChain 1:                0.013 seconds (Sampling)\nChain 1:                0.026 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 4e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.013 seconds (Warm-up)\nChain 2:                0.012 seconds (Sampling)\nChain 2:                0.025 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 4e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.012 seconds (Warm-up)\nChain 3:                0.013 seconds (Sampling)\nChain 3:                0.025 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 4e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.012 seconds (Warm-up)\nChain 4:                0.013 seconds (Sampling)\nChain 4:                0.025 seconds (Total)\nChain 4: \n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: y ~ x \n   Data: dat (Number of observations: 10) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -0.11      0.38    -0.87     0.65 1.00     2807     2145\nx            -0.05      0.34    -0.72     0.65 1.00     2735     2402\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.16      0.34     0.71     1.99 1.00     1795     2086\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nAgain, if you get output similar to that above, then the complete Bayesian toolchain is correctly configured and ready for use.\n\n\n\n\n\nInstall and setup Xcode (a collection of build tools for MacOSX)\n\nopen a terminal and enter\n\n\nxcode-select --install\n\nInstall cmdstan (an implementation of the STAN language)\n\nusing selected instructions from https://mc-stan.org/cmdstanr/articles/cmdstanr.html\n\nopen a new R session and issue the following\n\ninstall.packages(\"cmdstanr\", repos = c(\"https://mc-stan.org/r-packages/\", getOption(\"repos\")))\n\nmake sure the package loads\n\nlibrary(cmdstanr)\n\nensure that the c++ toolchain (from Rtools) is correctly installed and configured\n\ncheck_cmdstan_toolchain()\n\nif the toolchain is correctly configured, install cmdstan\n\ninstall_cmdstan(cores = 2)\n\n\n\nEnsure that cmdstan is properly configured by compiling a built in example\n\nfile &lt;- file.path(cmdstan_path(), \"examples\", \"bernoulli\", \"bernoulli.stan\")\nmod &lt;- cmdstan_model(file)\ndata_list &lt;- list(N = 10, y = c(0,1,0,0,0,0,0,0,0,1))\n\nfit &lt;- mod$sample(\n  data = data_list,\n  seed = 123,\n  chains = 4,\n  parallel_chains = 4,\n  refresh = 500 # print update every 500 iters\n)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 0.0 seconds.\nChain 2 finished in 0.0 seconds.\nChain 3 finished in 0.0 seconds.\nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.2 seconds.\n\n\nIf you get output resembling the above, then cmdstan is setup correctly.\nInstall the brms - an R package that provides a more familiar R model fitting interface to STAN.\n\ninstall the package\n\ninstall.packages(\"brms\")\n\ntest whether the whole tool chain works\n\nlibrary(cmdstanr)\nlibrary(brms)\ndat &lt;- data.frame(y = rnorm(10), x = rnorm(10))\nbrm(y ~ x, data = dat)\n\nCompiling Stan program...\n\n\nStart sampling\n\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 8e-06 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.013 seconds (Warm-up)\nChain 1:                0.012 seconds (Sampling)\nChain 1:                0.025 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 4e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.013 seconds (Warm-up)\nChain 2:                0.013 seconds (Sampling)\nChain 2:                0.026 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 3e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.013 seconds (Warm-up)\nChain 3:                0.012 seconds (Sampling)\nChain 3:                0.025 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 4e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.012 seconds (Warm-up)\nChain 4:                0.013 seconds (Sampling)\nChain 4:                0.025 seconds (Total)\nChain 4: \n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: y ~ x \n   Data: dat (Number of observations: 10) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.51      0.35    -0.17     1.20 1.00     2807     2086\nx            -0.12      0.51    -1.14     0.91 1.00     2485     1893\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.02      0.30     0.61     1.79 1.00     2257     2398\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nAgain, if you get output similar to that above, then the complete Bayesian toolchain is correctly configured and ready for use.\n\n\n\n\n\nEnsure that you have installed (via your package manager) the following dependencies:\n\nbuild-essential\ng++\ngcc\ncurl\nlibcurl4-openssl-dev\n\nInstall cmdstan (an implementation of the STAN language)\n\nusing selected instructions from https://mc-stan.org/cmdstanr/articles/cmdstanr.html\n\nopen a new R session and issue the following\n\ninstall.packages(\"cmdstanr\", repos = c(\"https://mc-stan.org/r-packages/\", getOption(\"repos\")))\n\nmake sure the package loads\n\nlibrary(cmdstanr)\n\nensure that the c++ toolchain (from Rtools) is correctly installed and configured\n\ncheck_cmdstan_toolchain()\n\nif the toolchain is correctly configured, install cmdstan\n\ninstall_cmdstan(cores = 2)\n\n\n\nEnsure that cmdstan is properly configured by compiling a built in example\n\nfile &lt;- file.path(cmdstan_path(), \"examples\", \"bernoulli\", \"bernoulli.stan\")\nmod &lt;- cmdstan_model(file)\ndata_list &lt;- list(N = 10, y = c(0,1,0,0,0,0,0,0,0,1))\n\nfit &lt;- mod$sample(\n  data = data_list,\n  seed = 123,\n  chains = 4,\n  parallel_chains = 4,\n  refresh = 500 # print update every 500 iters\n)\n\nRunning MCMC with 4 parallel chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 0.0 seconds.\nChain 2 finished in 0.0 seconds.\nChain 3 finished in 0.0 seconds.\nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.2 seconds.\n\n\nIf you get output resembling the above, then cmdstan is setup correctly.\nInstall the brms - an R package that provides a more familiar R model fitting interface to STAN.\n\ninstall the package\n\ninstall.packages(\"brms\")\n\ntest whether the whole tool chain works\n\nlibrary(cmdstanr)\nlibrary(brms)\ndat &lt;- data.frame(y = rnorm(10), x = rnorm(10))\nbrm(y ~ x, data = dat)\n\nCompiling Stan program...\n\n\nStart sampling\n\n\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 1e-05 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.1 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.013 seconds (Warm-up)\nChain 1:                0.012 seconds (Sampling)\nChain 1:                0.025 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 5e-06 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.013 seconds (Warm-up)\nChain 2:                0.012 seconds (Sampling)\nChain 2:                0.025 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 4e-06 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.012 seconds (Warm-up)\nChain 3:                0.013 seconds (Sampling)\nChain 3:                0.025 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 4e-06 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.012 seconds (Warm-up)\nChain 4:                0.011 seconds (Sampling)\nChain 4:                0.023 seconds (Total)\nChain 4: \n\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: y ~ x \n   Data: dat (Number of observations: 10) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -0.40      0.43    -1.23     0.44 1.00     2791     2071\nx             0.10      0.60    -1.07     1.27 1.00     2690     1692\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.31      0.38     0.80     2.26 1.00     1665     1970\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nAgain, if you get output similar to that above, then the complete Bayesian toolchain is correctly configured and ready for use.",
    "crumbs": [
      "Setup"
    ]
  },
  {
    "objectID": "40_synthetic_world.html",
    "href": "40_synthetic_world.html",
    "title": "Synthetic world and data",
    "section": "",
    "text": "1 Synopsis\nThis document illustrates the generation of synthetic data that can be used to test the processing and modelling routines used in the ReefCloud statistical modelling components. Specifically, it will illustrate the generation of a full spatio-temporal grid for hard coral (HC), soft coral (SC) and macroalgae (MA) within a fabricated area by establishing a baseline for each in space and then perturbing them over time by overlaying synthetic disturbances (degree heating weeks, cyclones and other disturbances) as well as between disturbance growth (recovery).\nThe established full spatio-temporal grid for each of HC, SC and MA can thereafter be considered the “true” synthetic world from which a range of sampling designs and analyses can be explored.\n\n\n2 Preparations\nWe will start by loading the required r packages.\n\nlibrary(knitr)\nlibrary(tidyverse)\nlibrary(simstudy)\nlibrary(gstat)\nlibrary(sf)\nlibrary(RandomFields)\nRFoptions(install = \"no\")\nlibrary(INLA)\nlibrary(inlabru)\nlibrary(mgcv)\nlibrary(stars)\nlibrary(patchwork)\nlibrary(Hmisc)\n#library(downloadthis)\n\nThe generated data are synthetic. As such they are the result of a combination of deterministic (based on precise repeatable rules and equations) and stochastic (based on random variability) processes. To ensure repeatability (in the presence of the stochasticity), we will set a random seed value that we can utilise any time we are enacting a stochastic (random) element.\n\nseed &lt;- 123\n\n\n\n3 Define the spatial and temporal domains\nThe spatial and temporal domains will be used to represent our synthetic (fabricated) “world”. Before we can project any spatial and temporal patterns in benthic cover, we first need to define the bounds of this world. Thereafter, cover will broadly be generated as the baseline cover (cover at time 0) plus the spatial patterns over time plus random noise.\n\ncover \\sim{} baseline + spatial\\times temporal + noise\n\n\nTemporalSpatialGenerate reefs\n\n\nWe will simulate 12 annual sampling events. Although these will be assumed to be evenly spaced in time (descrete times), it is acknowledged that this is typically not now sampling programs collect data.\n\nyears &lt;- 1:12\n\n\n\nLets define the spatial domain.\n\n\n\nspatial_domain &lt;- st_geometry(\n  st_multipoint(\n    x = rbind(\n      c(0, -10),\n      c(3, -10),\n      c(10, -20),\n      c(1, -21),\n      c(2, -16),\n      c(0, -10)\n    )\n  )\n) |&gt;\n  st_set_crs(4326) |&gt;\n  st_cast(\"POLYGON\")\n\nspatial_domain |&gt;\n  ggplot() +\n  geom_sf() +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow we will generate a grid of 10,000 points within the spatial domain. This will essentially represent the full continuous spatial surface.\n\n\n\nset.seed(seed)\nspatial_grid &lt;- spatial_domain |&gt;\n  st_set_crs(NA) |&gt;\n  st_sample(size = 10000, type = \"regular\") |&gt;\n  st_set_crs(4236)\n\nspatial_grid |&gt; ggplot() +\n  geom_sf(size = 0.1) +\n  theme_bw()\n\n## Compile the spatial data frame - note it is important for\n## RFsimulate that the data be sorted by Longitude then Latitude\nspatial_grid_pts_df &lt;- spatial_grid |&gt;\n  st_coordinates() |&gt;\n  as.data.frame() |&gt;\n  dplyr::rename(Longitude = X, Latitude = Y) |&gt;\n  arrange(Longitude, Latitude)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAlthough the above represents the broad spatial domain, the entire domain will not be saturated with coral reef. Indeed only a small fraction of the full domain will contain coral reef. Furthermore, the coral reef will not be distributed uniformly throughout the domain.\nTo simulate this, we will create a collection of discrete reefs that vary in size and shape and be sprinkled throughout the spatial domain. This can be acheived by creating a gaussian markov random field to defines a noisy variable in space. If we think of the noisy variable as being a third dimension (such as height), then this field is like a very rough topography. “Reefs” can be created by only keeping the parts of the topography that are higher than a specific value (picture viewing the hilly landscape from a plane as a large, flat cloud slowly decends towards the ground gradually revealing a set of seperate peaks).\nCoral reefs tend to consist of a shallow, sand-filled lagoon surrounded by a sloping escarpment of hard substrate. The coral of a coral reef is typically restricted to the escarpment and thus only the 5-20 meter perimeter of a reef. Hence, to further the realism of the simulated reefs, we will further modify the “reefs” so that they are a series of irregular frames.\n\n\n\nRFoptions(seed = 1)\nthreshold &lt;- 1.75\nmodel &lt;- RMexp(var = 1, scale = 0.1)\nsim &lt;- RFsimulate(model,\n  x = as.vector(scale(spatial_grid_pts_df$Longitude,\n    scale = FALSE\n  )),\n  y = as.vector(scale(spatial_grid_pts_df$Latitude,\n    scale = FALSE\n  ))\n)\n## combine with spatial data\nreefs &lt;- spatial_grid_pts_df |&gt;\n  mutate(Y = as.vector(sim))\n\nreefs |&gt;\n  ggplot() +\n  geom_tile(aes(y = Latitude, x = Longitude, fill = Y)) +\n  coord_sf(crs = 4326) +\n  theme_bw() +\n  theme(\n    axis.title = element_blank(),\n    legend.position = c(0.95, 0.95),\n    legend.justification = c(1, 1)\n  )\n\nreefs |&gt;\n  mutate(Y = Y &gt; threshold) |&gt;\n  ggplot() +\n  geom_tile(aes(y = Latitude, x = Longitude, fill = Y)) +\n  coord_sf(crs = 4326) +\n  theme_bw() +\n  theme(\n    axis.title = element_blank(),\n    legend.position = c(0.95, 0.95),\n    legend.justification = c(1, 1)\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can then filter the field to exclude Y values that are less than 1.75, convert the data into a simple features points object, generate square buffers around each point, convert the points into polygons and then combine overlapping and connected polygons into single polygons,\nThe result will be a set of irregular and randomly postioned polygons that can represent coral reefs.\n\n\n\nreefs_sf &lt;- reefs |&gt;\n  st_as_sf(coords = c(\"Longitude\", \"Latitude\")) |&gt;\n  filter(Y &gt; threshold) |&gt;\n  st_buffer(0.05, endCapStyle = \"SQUARE\") |&gt;\n  st_cast(\"POLYGON\") |&gt;\n  st_union() |&gt;\n  st_set_crs(4326)\nreefs_sf |&gt;\n  ggplot() +\n  geom_sf(data = spatial_domain, fill = NA) +\n  geom_sf(fill = \"red\") + # nolint\n  theme_bw() +\n  theme(\n    axis.title = element_blank(),\n    legend.position = c(0.95, 0.95),\n    legend.justification = c(1, 1)\n  )\nreefs_full_sf &lt;- reefs_sf\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinally, we generate the difference between an enlarged and shrunk versions of each reef so as to yield polygons that resembled frames. To illustrate, the figure is zoomed in on a small collection of reefs.\n\n\n\nsf_use_s2(FALSE) # negative buffers dont work if this is true\nreefs_sf &lt;- reefs_sf |&gt;\n  st_buffer(0.01) |&gt;\n  st_difference(reefs_sf |&gt; st_buffer(-0.01))\nsf_use_s2(TRUE)\nreefs_sf |&gt; ggplot() +\n  geom_sf() +\n  coord_sf(xlim = c(2.4, 2.9), ylim = c(-16.75, -16.25)) +\n  theme_bw() +\n  theme(\n    axis.title = element_blank(),\n    legend.position = c(0.95, 0.95),\n    legend.justification = c(1, 1)\n  )\nreefs_poly_sf &lt;- reefs_sf |&gt;\n  st_cast(\"POLYGON\") |&gt;\n  st_as_sf() |&gt;\n  mutate(Reef = paste0(\"Reef\", 1:n()))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4 Synthetic broad scale patterns\nIn this section, we will use a range of statistical routines to project spatial and temporal patterns (as well as noise) onto the entire spatio-temporal domain ignoring the fact that the reefs only occur in the limited bordered shaped defined in the previous section. We will this later point in the next section.\nThe response data (hard coral, soft coral and macroalgae cover) will be effected by the following:\n\nbase coral cover - the global average coral cover (pooled over space and time)\nspatial pattern in this base cover which reflects the spatial pattern at T0\nannual growth (e.g. 5-10% annual increase)\ninfluence of covariates (spatio-temporal effects)\nrandom noise\n\n\nDefine the SPDEBaseline hard coral coverBaseline soft coral coverBaseline macroalgae coverDegree heating weeksCyclonesOther disturbancesAll effectsSynthetic hard coral cover dataSynthetic soft coral cover dataSynthetic macroalgae cover data\n\n\n\nCreate the SPDE mesh\n\n\n\nvariance &lt;- 1\nkappa &lt;- 1\n\nalpha &lt;- 2\nmesh_pars &lt;- c(1, 0.5, 0.1, 1, 0.5) *\n  sqrt(alpha - ncol(spatial_grid_pts_df) / 2) / kappa\ns &lt;- inla.mesh.segment(\n  spatial_grid_pts_df[chull(spatial_grid_pts_df), ]\n)\nmesh &lt;- inla.mesh.2d(\n  spatial_grid_pts_df[chull(spatial_grid_pts_df), ],\n  max.edge = mesh_pars[1:2],\n  cutoff = mesh_pars[3],\n  offset = mesh_pars[4:5],\n  boundary = s\n)\n\nggplot() +\n  gg(mesh) +\n  geom_sf(data = spatial_domain, fill = NA, size = 2) +\n  coord_sf(crs = 4326) +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreate a SPDE for a Matern model. If you want to apply PC priors, use inla.spde2.pcmatern instead. Matern SPDE model with spatial scale paramter kappa(u) and variance rescaling. parameter tau(u). Alpha is the Fractional operator order where nu = \\alpha-d/2.\n\nspde &lt;- inla.spde2.matern(mesh, alpha = alpha)\n\ncalculate the precision matrix from the parameter values (theta)\n\ntheta &lt;- c(-0.5 * log(4 * pi * variance * kappa^2), log(kappa))\nQ &lt;- inla.spde2.precision(spde, theta = theta)\n\ncalculate a lattice projection to and from the mesh\n\nA &lt;- inla.spde.make.A(\n  mesh = mesh,\n  loc = as.matrix(spatial_grid_pts_df)\n)\n# OR\n## A &lt;- inla.mesh.project(mesh = mesh,\n##                        loc = as.matrix(spatial_grid_pts_df ))$A\n\n\n\n\nThe baseline represents the spatial pattern of hard coral cover the year prior to sampling. This spatial pattern is defined as a simple sine wave (applied to the centered latitudes) and rotated slightly and projected onto the SPDE grid.\n\ncover_i = longitude_i + sin(latitude_i) + 1.5\\times (longitude_i + latitude_i)\n\nNote, these values are on the expected link scale (logit). The second (bottom) figure displays the baseline on the response (percent cover) scale.\n\n\n\nbaseline_sample_hcc &lt;- mesh$loc[, 1:2] |&gt;\n  as.data.frame() |&gt;\n  dplyr::select(Longitude = V1, Latitude = V2) |&gt;\n  mutate(\n    clong = as.vector(scale(Longitude, scale = FALSE)),\n    clat = as.vector(scale(Latitude, scale = FALSE)),\n    Y = clong + sin(clat) + # rnorm(1,0,1) +\n      1.5 * clong + clat\n  ) |&gt;\n  mutate(Y = scales::rescale(Y, to = c(-2, 0.8)))\n\nbaseline_effects_hcc &lt;- baseline_sample_hcc |&gt;\n  dplyr::select(Y) |&gt;\n  as.matrix()\nbaseline_pts_sample_hcc &lt;- inla.mesh.project(mesh,\n  loc = as.matrix(spatial_grid_pts_df[, 1:2]),\n  baseline_effects_hcc\n)\nbaseline_pts_effects_hcc &lt;- baseline_pts_sample_hcc |&gt;\n  cbind() |&gt;\n  as.matrix() |&gt;\n  as.data.frame() |&gt;\n  cbind(spatial_grid_pts_df) |&gt;\n  pivot_longer(\n    cols = c(-Longitude, -Latitude),\n    names_to = c(\"Year\"),\n    names_pattern = \"sample:(.*)\",\n    values_to = \"Value\"\n  ) |&gt;\n  mutate(Year = as.numeric(Year))\n\nggplot(baseline_pts_effects_hcc, aes(y = Latitude, x = Longitude)) +\n  geom_tile(aes(fill = Value)) +\n  scale_fill_gradientn(colors = terrain.colors(10)) +\n  coord_sf(crs = 4236) +\n  theme_bw(base_size = 7) +\n  theme(\n    axis.title = element_blank(),\n    legend.position = c(0.95, 0.95),\n    legend.justification = c(1, 1)\n  )\n\nggplot(baseline_pts_effects_hcc, aes(y = Latitude, x = Longitude)) +\n  geom_tile(aes(fill = 100 * plogis(Value))) +\n  scale_fill_gradientn(\"Cover (%)\", colors = terrain.colors(10)) +\n  coord_sf(crs = 4236) +\n  theme_bw(base_size = 7) +\n  theme(\n    axis.title = element_blank(),\n    legend.position = c(0.95, 0.95),\n    legend.justification = c(1, 1)\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe baseline represents the spatial pattern of soft coral cover the year prior to sampling. Similar to hard coral cover, this spatial pattern is defined as a simple sine wave (applied to the centered latitudes), yet it is rotated slightly more before being projected onto the SPDE grid.\n\ncover_i = longitude_i + sin(latitude_i) + 1.5\\times longitude_i - 1.5\\times latitude_i\n\nNote again, these values are on the expected link scale (logit). The second (bottom) figure displays the baseline on the response (percent cover) scale.\n\n\n\nbaseline_sample_sc &lt;- mesh$loc[, 1:2] |&gt;\n  as.data.frame() |&gt;\n  dplyr::select(Longitude = V1, Latitude = V2) |&gt;\n  mutate(\n    clong = as.vector(scale(Longitude, scale = FALSE)),\n    clat = as.vector(scale(Latitude, scale = FALSE)),\n    Y = clong + sin(clat) + # rnorm(1,0,1) +\n      1.5 * clong + -1.5 * clat\n  ) |&gt;\n  mutate(Y = scales::rescale(Y, to = c(-4, -2)))\n\nbaseline_effects_sc &lt;- baseline_sample_sc |&gt;\n  dplyr::select(Y) |&gt;\n  as.matrix()\nbaseline_pts_sample_sc &lt;- inla.mesh.project(mesh,\n  loc = as.matrix(spatial_grid_pts_df[, 1:2]),\n  baseline_effects_sc\n)\nbaseline_pts_effects_sc &lt;- baseline_pts_sample_sc |&gt;\n  cbind() |&gt;\n  as.matrix() |&gt;\n  as.data.frame() |&gt;\n  cbind(spatial_grid_pts_df) |&gt;\n  pivot_longer(\n    cols = c(-Longitude, -Latitude),\n    names_to = c(\"Year\"),\n    names_pattern = \"sample:(.*)\",\n    values_to = \"Value\"\n  ) |&gt;\n  mutate(Year = as.numeric(Year))\n\nggplot(baseline_pts_effects_sc, aes(y = Latitude, x = Longitude)) +\n  geom_tile(aes(fill = Value)) +\n  scale_fill_gradientn(colors = terrain.colors(10)) +\n  coord_sf(crs = 4236) +\n  theme_bw(base_size = 7) +\n  theme(\n    axis.title = element_blank(),\n    legend.position = c(0.95, 0.95),\n    legend.justification = c(1, 1)\n  )\nggplot(baseline_pts_effects_sc, aes(y = Latitude, x = Longitude)) +\n  geom_tile(aes(fill = 100 * plogis(Value))) +\n  scale_fill_gradientn(\"Cover (%)\", colors = terrain.colors(10)) +\n  coord_sf(crs = 4236) +\n  theme_bw(base_size = 7) +\n  theme(\n    axis.title = element_blank(),\n    legend.position = c(0.95, 0.95),\n    legend.justification = c(1, 1)\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this simulated world, macroalgae simply fills the space not occupied by hard and soft coral. Consequently, we will not generate an initial baseline for this. Rather, we will calculate it from the hard and soft coral spatio-temporal projections later.\n\n\nThe degree heating weeks effects represent relative spatio-temporal patterns. We start by establishing an overall temporal trend in degree heating weeks defined as:\n\n\\begin{align}\ncyear &= year - 1\\\\\ndhw_i &\\sim{} Beta(\\alpha_i, 1)\\\\\nlog\\left(\\frac{\\alpha_i}{1-\\alpha_i}\\right) &= 0.2 + cyear_i + sin(cyear_i)\\\\\ndhw_i &= 5 * (dhw_i - min(dhw)) / range(dhw)\n\\end{align}\n\n\n\n\nset.seed(seed)\ndhw_temporal &lt;- data.frame(Year = years) %&gt;%\n  mutate(\n    cYear = Year - 1, # as.vector(scale(Year, scale=FALSE)),\n    Y = 0.2 * cYear + sin(cYear),\n    Y = Y * rbeta(length(years), Y, 1),\n    Y = scales::rescale(Y - min(Y), to = c(0, 5))\n  )\ndhw_temporal %&gt;%\n  ggplot(aes(y = Y, x = Year)) +\n  geom_line() +\n  theme_bw(base_size = 7)\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow we propagate this temporal trend across a random field with a time varying autocorrelation coefficient drawn from beta distribution with shape parameters of 0.2 and 1.\n\nset.seed(seed)\ndhw_sample &lt;- inla.qsample(length(years),\n  Q,\n  seed = seed,\n  constr = spde$f$extraconstr\n)\n\nrho &lt;- rep(0.7, length(years))\nrho &lt;- rbeta(length(years), 0.2, 1)\nx &lt;- dhw_sample\nfor (j in 2:length(years)) {\n  x[, j] &lt;- rho[j] * x[, j - 1] + sqrt(1 - rho[j]^2) * dhw_sample[, j]\n}\nx &lt;- sweep(x, 2, dhw_temporal$Y, FUN = \"+\")\ndhw_effects &lt;- scales::rescale(x, to = c(0, 1))\ndhw_pts_sample &lt;- inla.mesh.project(mesh,\n  loc = as.matrix(spatial_grid_pts_df[, 1:2]),\n  dhw_effects\n)\n\ndhw_pts_effects_df &lt;- dhw_pts_sample %&gt;%\n  as.matrix() %&gt;%\n  as.data.frame() %&gt;%\n  cbind(spatial_grid_pts_df) %&gt;%\n  pivot_longer(\n    cols = c(-Longitude, -Latitude),\n    names_to = c(\"Year\"),\n    names_pattern = \"sample:(.*)\",\n    values_to = \"Value\"\n  ) %&gt;%\n  mutate(Year = as.numeric(Year))\n## Value=scales::rescale(Value, to=c(0,1)))\n\n## dhw.effect &lt;- dhw.gmrf %&gt;% mutate(Value=scales::rescale(Value, to=c(0,-0.3)))\nggplot(dhw_pts_effects_df, aes(y = Latitude, x = Longitude)) +\n  geom_tile(aes(fill = Value)) +\n  facet_wrap(~Year, nrow = 2) +\n  scale_fill_gradientn(colors = rev(heat.colors(10))) +\n  coord_sf(crs = 4236) +\n  theme_bw(base_size = 12) +\n  theme(axis.title = element_blank())\n\n\n\n\n\n\n\n\n\n\nFor each year, we calculate the probability that a cyclone has occurred somewhere in the spatial domain, the cyclone intensity and a sine wave path for the cyclone to follow through the spatial domain.\n\n\n\nset.seed(seed)\ncyc &lt;- vector(\"list\", length(years))\n\n## spatial_grid_pts_df |&gt;\n##     mutate(clong = as.vector(scale(Longitude, scale=FALSE)),\n##            clat = as.vector(scale(Latitude, scale=FALSE)))\n\nfor (yr in years) {\n  cat(paste(\"Year:\", yr, \"\\n\"))\n  cyc_occur &lt;- rbinom(1, 1, prob = min(0.05 * yr^2, 0.6))\n  cat(paste(\"Cyclone Occurance:\", cyc_occur, \"\\n\"))\n  cyc_intensity &lt;- rbeta(1, 2, 1) |&gt; round(2)\n  cat(paste(\"Cyclone intensity:\", cyc_intensity, \"\\n\"))\n  ## cyc_spatial &lt;- spatial_grid_pts_df  |&gt;\n  lat_offset &lt;- runif(1, 0, 5)\n  cyc_spatial &lt;- mesh$loc[, 1:2] |&gt;\n    as.data.frame() |&gt;\n    dplyr::select(Longitude = V1, Latitude = V2) |&gt;\n    mutate(\n      clong = as.vector(scale(Longitude, scale = FALSE)),\n      clat = as.vector(scale(Latitude, scale = FALSE)),\n      Y = lat_offset + runif(1, -1, 1) * clong + runif(1, -1, 1) *\n        clat + sin(clat),\n      # Y= Y - runif(1,-10,10),\n      Y = abs(Y),\n      Y = ifelse(Y &gt; cyc_intensity, cyc_intensity, Y),\n      Y = cyc_intensity - Y,\n      Value = Y * cyc_occur\n    )\n  cyc[[yr]] &lt;- cyc_spatial |&gt; mutate(Year = yr)\n}\ncyc &lt;- do.call(\"rbind\", cyc)\ncyc_effects_df &lt;- cyc |&gt;\n  mutate(Value = scales::rescale(Value, to = c(0, 1)))\n\ncyc_effects_df |&gt;\n  group_by(Year) |&gt;\n  summarise(\n    Mean = mean(Value),\n    Median = median(Value)\n  ) |&gt;\n  ggplot(aes(x = Year)) +\n  geom_line(aes(y = Mean), color = \"blue\") +\n  geom_line(aes(y = Median), color = \"red\") +\n  theme_bw(base_size = 7)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncyc_effects &lt;- cyc_effects_df |&gt;\n  dplyr::select(-clong, -clat, -Y) |&gt;\n  pivot_wider(\n    id_cols = c(Longitude, Latitude),\n    names_prefix = \"sample:\",\n    names_from = Year,\n    values_from = Value\n  ) |&gt;\n  dplyr::select(-Longitude, -Latitude) |&gt;\n  as.matrix()\n\ncyc_pts_sample &lt;- inla.mesh.project(mesh,\n  loc = as.matrix(spatial_grid_pts_df[, 1:2]),\n  cyc_effects\n)\ncyc_pts_effects &lt;- cyc_pts_sample |&gt;\n  as.matrix() |&gt;\n  as.data.frame() |&gt;\n  cbind(spatial_grid_pts_df) |&gt;\n  pivot_longer(\n    cols = c(-Longitude, -Latitude),\n    names_to = c(\"Year\"),\n    names_pattern = \"sample:(.*)\",\n    values_to = \"Value\"\n  ) |&gt;\n  mutate(Year = as.numeric(Year))\n## Value=scales::rescale(Value, to=c(0,-0.5)))\n\nggplot(cyc_pts_effects, aes(y = Latitude, x = Longitude)) +\n  geom_tile(aes(fill = Value)) +\n  facet_wrap(~Year, nrow = 2) +\n  scale_fill_gradientn(colors = terrain.colors(10)) +\n  coord_sf(crs = 4236) +\n  theme_bw(base_size = 12) +\n  theme(\n    axis.title = element_blank(),\n    legend.position = c(0.95, 0.95),\n    legend.justification = c(1, 1)\n  )\n\n\n\n\n\n\n\n\n\n\nSimilar to degree heating weeks, other disturbances are generated by defining local effects that are autocorrelated. This is a catchall for all other disturbances including crown of thorns, disease etc.\n\nset.seed(seed + 1)\nother_sample &lt;- inla.qsample(length(years),\n  Q,\n  seed = seed + 1,\n  constr = spde$f$extraconstr\n)\n\nrho &lt;- rep(0.7, length(years))\nrho &lt;- rbeta(length(years), 0.2, 1)\nx &lt;- other_sample\nfor (j in 2:length(years)) {\n  x[, j] &lt;- rho[j] * x[, j - 1] + sqrt(1 - rho[j]^2) * other_sample[, j]\n}\nother_effects &lt;- scales::rescale(x, to = c(0, 1))\nother_pts_sample &lt;- inla.mesh.project(mesh,\n  loc = as.matrix(spatial_grid_pts_df[, 1:2]),\n  other_effects\n)\n\nother_pts_effects &lt;- other_pts_sample |&gt;\n  as.matrix() |&gt;\n  as.data.frame() |&gt;\n  cbind(spatial_grid_pts_df) |&gt;\n  pivot_longer(\n    cols = c(-Longitude, -Latitude),\n    names_to = c(\"Year\"),\n    names_pattern = \"sample:(.*)\",\n    values_to = \"Value\"\n  ) |&gt;\n  mutate(Year = as.numeric(Year)) # ,\n## Value=scales::rescale(Value, to=c(0,1)))\n\nggplot(other_pts_effects, aes(y = Latitude, x = Longitude)) +\n  geom_tile(aes(fill = Value)) +\n  facet_wrap(~Year, nrow = 2) +\n  scale_fill_gradientn(colors = terrain.colors(10)) +\n  coord_sf(crs = 4236) +\n  theme_bw(base_size = 12) +\n  theme(axis.title = element_blank())\n\n\n\n\n\n\n\nother_pts_effects |&gt;\n  group_by(Year) |&gt;\n  summarise(\n    Mean = mean(Value, na.rm = TRUE),\n    Median = median(Value, na.rm = TRUE)\n  ) |&gt;\n  ggplot(aes(x = Year)) +\n  geom_line(aes(y = Mean))\n\n\n\n\n\n\n\n\n\n\nWe can now compile all the effects (disturbances as well as growth) together. If we work on the link scale, we can simply calculate a cumulative sum of effects per pixel.\nWe will define the relative influence (annual decline weighting) of each of the disturbances as:\n\nDegree heating weeks (0.5)\nCyclones (0.5)\nAll others (0.2)\n\nIn addition, we will indicate growth (annual increase) of:\n\nHard coral (0.3)\nSoft coral (0.3)\n\nMacrolgae will respond differently. Rather than respond directly, macroalgae will take up the remaining available space (e.g MA =\nTotal~available~space - HCC - SC).\n\ndisturb_effects &lt;-\n  (0.5 * dhw_effects) +\n  (0.4 * cyc_effects) +\n  (0.1 * other_effects) |&gt;\n  as.data.frame() # |&gt;\nall_effects_df &lt;- mesh$loc[, 1:2] |&gt;\n  as.data.frame() |&gt;\n  dplyr::rename(Longitude = V1, Latitude = V2) |&gt;\n  cbind(disturb_effects) |&gt;\n  pivot_longer(\n    cols = c(-Longitude, -Latitude),\n    names_to = \"Year\",\n    names_pattern = \"sample:(.*)\",\n    values_to = \"Y\"\n  ) |&gt;\n  mutate(Year = factor(Year, levels = sort(unique(as.numeric(\n    as.character(Year)\n  ))))) |&gt;\n  group_by(Longitude, Latitude) |&gt;\n  mutate(\n    Growth_HCC = 0.3, ## Add growth onto this\n    Growth_SC = 0.3,\n    Y_HCC = cumsum(-Y + Growth_HCC), ## cumsum on link scale will accumulate effects\n    Y_SC = cumsum(-Y + Growth_SC)\n  )\n\nThe marginalized temporal effects are approximately:\n\n\n\nall_effects_df |&gt;\n  group_by(Year) |&gt;\n  summarise(\n    Mean = mean(Y_HCC, na.rm = TRUE),\n    Median = median(Y_HCC, na.rm = TRUE)\n  ) |&gt;\n  ggplot(aes(x = as.numeric(as.character(Year)))) +\n  geom_line(aes(y = Mean, color = \"Mean\")) +\n  geom_line(aes(y = Median, color = \"Median\")) +\n  scale_x_continuous(\"Year\") +\n  scale_y_continuous(\"Effect on HCC\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd the spatio-temporal effects are:\n\nall_effects_df |&gt;\n  ggplot(aes(y = Latitude, x = Longitude)) +\n  geom_point(aes(color = Y_HCC)) +\n  ## geom_tile(aes(fill = Y_HCC)) +\n  facet_wrap(~Year, nrow = 2) +\n  scale_color_gradient2(\"HCC\", low = \"red\", high = \"green\", mid = \"white\") +\n  coord_sf(crs = 4236) +\n  theme_bw(base_size = 12) +\n  theme(axis.title = element_blank())\n\n\n\n\n\n\n\nall_effects &lt;- all_effects_df |&gt;\n  pivot_wider(\n    id_cols = c(Longitude, Latitude),\n    names_prefix = \"sample:\",\n    names_from = Year,\n    values_from = Y_HCC\n  )\n\n## Project onto the spatial grid\ndisturb_pts_sample &lt;- inla.mesh.project(mesh,\n  loc = as.matrix(spatial_grid_pts_df[, 1:2]),\n  all_effects |&gt;\n    ungroup() |&gt; \n    dplyr::select(-Longitude, -Latitude) |&gt;\n    as.matrix()\n)\ndisturb_pts_effects &lt;- disturb_pts_sample |&gt;\n  as.matrix() |&gt;\n  as.data.frame() |&gt;\n  cbind(spatial_grid_pts_df) |&gt;\n  pivot_longer(\n    cols = c(-Longitude, -Latitude),\n    names_to = c(\"Year\"),\n    names_pattern = \"sample:(.*)\",\n    values_to = \"Value\"\n  ) |&gt;\n  mutate(Year = as.numeric(Year))\n\nggplot(disturb_pts_effects, aes(y = Latitude, x = Longitude)) +\n  geom_tile(aes(fill = Value)) +\n  facet_wrap(~Year, nrow = 2) +\n  scale_fill_gradientn(colors = terrain.colors(10)) +\n  coord_sf(crs = 4236) +\n  theme_bw(base_size = 12) +\n  theme(\n    axis.title = element_blank(),\n    legend.position = c(0.95, 0.95),\n    legend.justification = c(1, 1)\n  )\n\n\n\n\n\n\n\n\n\n\n\n## Do all this on the link scale so that can use cumsum\nall_effects_hcc &lt;- all_effects_df |&gt;\n  full_join(baseline_sample_hcc |&gt;\n    dplyr::select(Longitude, Latitude, BASE_HCC = Y)) |&gt;\n  group_by(Longitude, Latitude) |&gt;\n  mutate(HCC = BASE_HCC + Y_HCC) |&gt;\n  ungroup() |&gt;\n  dplyr::select(-BASE_HCC, -Y_HCC) |&gt;\n  pivot_wider(\n    id_cols = c(Longitude, Latitude),\n    names_prefix = \"sample:\",\n    names_from = Year,\n    values_from = HCC\n  ) |&gt;\n  dplyr::select(-Longitude, -Latitude) |&gt;\n  as.matrix()\n\nall_pts_sample_hcc &lt;- inla.mesh.project(mesh,\n  loc = as.matrix(spatial_grid_pts_df[, 1:2]),\n  all_effects_hcc\n)\nall_pts_effects_hcc &lt;- all_pts_sample_hcc |&gt;\n  as.matrix() |&gt;\n  as.data.frame() |&gt;\n  cbind(spatial_grid_pts_df) |&gt;\n  pivot_longer(\n    cols = c(-Longitude, -Latitude),\n    names_to = c(\"Year\"),\n    names_pattern = \"sample:(.*)\",\n    values_to = \"Value\"\n  ) |&gt;\n  mutate(\n    Year = as.numeric(Year),\n    ## Value = Value+qlogis(0.3))\n    Value = Value\n  )\n## Value=scales::rescale(Value, to=c(-0.5,0)))\n\n\n\n\nall_pts_effects_hcc |&gt;\n  mutate(Value = plogis(Value)) |&gt; \n  group_by(Year) |&gt;\n  summarise(Mean = mean(Value),\n    Median = median(Value)) |&gt;\n  ggplot(aes(x = Year)) +\n  geom_line(aes(y = Mean), color = \"blue\") +\n  geom_line(aes(y = Median), color = \"red\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nall_pts_effects_hcc |&gt;\n  mutate(Value = plogis(Value)) |&gt; \n  ggplot(aes(y = Latitude, x = Longitude)) +\n  geom_tile(aes(fill = Value)) +\n  scale_fill_gradientn(colors = terrain.colors(10)) + \n  coord_sf(crs = 4236) +\n  facet_wrap(~Year, nrow = 2) + \n  theme_bw(base_size = 7) +\n  theme(axis.title = element_blank(),\n    legend.position = c(0.95,0.95),\n    legend.justification = c(1,1))\n\n\n\n\n\n\n\n\n\n\n\n## Do all this on the link scale so that can use cumsum\nall_effects_sc &lt;- all_effects_df |&gt;\n  full_join(baseline_sample_sc |&gt; dplyr::select(Longitude, Latitude, BASE_SC=Y)) |&gt;\n  group_by(Longitude, Latitude) |&gt;\n  mutate(SC = BASE_SC + Y_SC) |&gt;\n  ungroup() |&gt;\n  dplyr::select(-BASE_SC, -Y_SC) |&gt;\n  pivot_wider(id_cols = c(Longitude, Latitude), \n    names_prefix = 'sample:',\n    names_from = Year, \n    values_from = SC) |&gt;\n  dplyr::select(-Longitude, -Latitude) |&gt;\n    as.matrix()\n\nall_pts_sample_sc &lt;- inla.mesh.project(mesh,\n  loc = as.matrix(spatial_grid_pts_df [,1:2]),\n  all_effects_sc)\nall_pts_effects_sc = all_pts_sample_sc |&gt; \n  as.matrix() |&gt; \n  as.data.frame() |&gt;\n  cbind(spatial_grid_pts_df ) |&gt; \n  pivot_longer(cols = c(-Longitude, -Latitude),\n    names_to = c('Year'),\n    names_pattern = 'sample:(.*)',\n    values_to = 'Value') |&gt;\n  mutate(Year = as.numeric(Year),\n    ## Value = Value+qlogis(0.3))\n    Value = Value)\n## Value=scales::rescale(Value, to=c(-0.5,0)))\n\n\n\n\nall_pts_effects_sc |&gt;\n  mutate(Value = plogis(Value)) |&gt; \n  group_by(Year) |&gt;\n  summarise(Mean = mean(Value),\n    Median = median(Value)) |&gt;\n  ggplot(aes(x = Year)) +\n  geom_line(aes(y = Mean), color = \"blue\") +\n  geom_line(aes(y = Median), color = \"red\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nall_pts_effects_sc |&gt;\n  mutate(Value = plogis(Value)) |&gt; \n  ggplot(aes(y = Latitude, x = Longitude)) +\n  geom_tile(aes(fill = Value)) +\n  scale_fill_gradientn(colors = terrain.colors(10)) + \n  coord_sf(crs = 4236) +\n  facet_wrap(~Year, nrow = 2) + \n  theme_bw(base_size = 7) +\n  theme(axis.title = element_blank(),\n    legend.position = c(0.95,0.95),\n    legend.justification = c(1,1))\n\n\n\n\n\n\n\n\n\n\n\n## Do all this on the link scale so that can use cumsum\nall_pts_effects_ma &lt;- all_pts_effects_hcc |&gt;\n  dplyr::rename(HCC=Value) |&gt; \n  bind_cols(all_pts_effects_sc |&gt;\n              dplyr::select(SC=Value)) |&gt;\n  mutate(Total_Avail = 0.8 - plogis(HCC) + plogis(SC),\n    ## MA = Total_Avail*rbeta(n(), 2, 1),\n    MA = Total_Avail,\n    Value = qlogis(MA)) |&gt;\n  dplyr::select(-HCC, -SC, -Total_Avail, -MA)\n\n\n\n\nall_pts_effects_ma |&gt;\n  mutate(Value = plogis(Value)) |&gt; \n  group_by(Year) |&gt;\n  summarise(Mean = mean(Value),\n    Median = median(Value)) |&gt;\n  ggplot(aes(x = Year)) +\n  geom_line(aes(y = Mean), color = \"blue\") +\n  geom_line(aes(y = Median), color = \"red\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nall_pts_effects_ma |&gt;\n  mutate(Value = plogis(Value)) |&gt; \n  ggplot(aes(y = Latitude, x = Longitude)) +\n  geom_tile(aes(fill = Value)) +\n  scale_fill_gradientn(colors = terrain.colors(10)) + \n  coord_sf(crs = 4236) +\n  facet_wrap(~Year, nrow = 2) + \n  theme_bw(base_size = 7) +\n  theme(axis.title = element_blank(),\n    legend.position = c(0.95,0.95),\n    legend.justification = c(1,1))\n\n\n\n\n\n\n\n\n\n\n\n\n\n5 Broad scale reef patterns\nAs indicated previously, the entire spatial domain is not covered with coral reef. Rather coral reefs are usually sprinkled throughout the spatial domain. Therefore, to represent reality more closely, we should restict the broad spatial patterns to just the reefs (which are themselves the set of frames we created earlier).\nNote, as the ‘reefs’ (frames) take up relatively little space within the full spatial domain, the figures in this section will be zoomed in on a specific section of the spatial domain (so as to highlight the reef fraction of the space).\n\nrasterize the reefs frame\nconvert to points (centroids of raster cells)\nfilter to the values of 1\nextract coordinates\nconvert to data frame\n\n\ndata_reefs_sf &lt;- reefs_sf |&gt;\n  st_as_stars(dx = 0.01) |&gt;  # rasterize\n  st_as_sf(as_points = TRUE) |&gt;\n  filter(values == 1L)\n\ndata_reefs_df &lt;- data_reefs_sf |&gt;\n  st_coordinates() |&gt;\n  as.data.frame() |&gt;\n  dplyr::rename(Longitude = X, Latitude = Y)\n\n\nHard coral coverSoft coral coverMacroalgae cover\n\n\n\ndata_reefs_sample_hcc &lt;- inla.mesh.project(mesh,\n  loc = as.matrix(data_reefs_df[, 1:2]),\n  all_effects_hcc\n)\ndata_reefs_hcc &lt;- data_reefs_sample_hcc |&gt;\n  as.matrix() |&gt;\n  as.data.frame() |&gt;\n  cbind(data_reefs_df) |&gt;\n  pivot_longer(\n    cols = c(-Longitude, -Latitude),\n    names_to = c(\"Year\"),\n    names_pattern = \"sample:(.*)\",\n    values_to = \"Value\"\n  ) |&gt;\n  mutate(\n    Year = as.numeric(Year),\n    ## Value = Value + qlogis(0.3))\n    Value = Value\n  )\n\ndata_reefs_pts_hcc_sf &lt;- data_reefs_hcc |&gt;\n  st_as_sf(coords = c(\"Longitude\", \"Latitude\")) |&gt;\n  st_set_crs(st_crs(data_reefs_sf))\nsf_use_s2(FALSE)\ndata_reefs_pts_hcc_sf &lt;- data_reefs_pts_hcc_sf |&gt;\n  st_intersection(reefs_poly_sf)\nsf_use_s2(TRUE)\n\nGiven that the entire spatial domain is very large and thus it would be very difficult to discern any within individual reef variations from a plot of the entire spatial domain, we will zoom in on a small region for the purposes of illustrating the result of projecting onto the reefs frames.\n\nsf_use_s2(FALSE)\ndata_reefs_pts_hcc_sf |&gt;\n  st_crop(xmin = 2.5, xmax = 3, ymin = -16.75, ymax = -16.25) |&gt;\n  mutate(Y = plogis(Value)) |&gt;\n  ggplot() +\n  geom_sf(aes(color = Y)) +\n  facet_wrap(~Year, nrow = 2) +\n  scale_color_gradientn(colors = terrain.colors(10)) +\n  coord_sf(\n    crs = 4236,\n    xlim = c(2.5, 3),\n    ylim = c(-16.75, -16.25)\n  ) +\n  theme_bw(base_size = 12) +\n  theme(axis.title = element_blank())\n\n\n\n\n\n\n\nsf_use_s2(TRUE)\n\n\n\n\ndata_reefs_sample_sc &lt;- inla.mesh.project(mesh,\n  loc = as.matrix(data_reefs_df[, 1:2]),\n  all_effects_sc\n)\ndata_reefs_sc &lt;- data_reefs_sample_sc |&gt;\n  as.matrix() |&gt;\n  as.data.frame() |&gt;\n  cbind(data_reefs_df) |&gt;\n  pivot_longer(\n    cols = c(-Longitude, -Latitude),\n    names_to = c(\"Year\"),\n    names_pattern = \"sample:(.*)\",\n    values_to = \"Value\"\n  ) |&gt;\n  mutate(\n    Year = as.numeric(Year),\n    Value = Value\n  )\n\ndata_reefs_pts_sc_sf &lt;- data_reefs_sc |&gt;\n  st_as_sf(coords = c(\"Longitude\", \"Latitude\")) |&gt;\n  st_set_crs(st_crs(data_reefs_sf))\nsf_use_s2(FALSE)\ndata_reefs_pts_sc_sf &lt;- data_reefs_pts_sc_sf |&gt;\n  st_intersection(reefs_poly_sf)\nsf_use_s2(TRUE)\n\nGiven that the entire spatial domain is very large and thus it would be very difficult to discern any within individual reef variations from a plot of the entire spatial domain, we will zoom in on a small region for the purposes of illustrating the result of projecting onto the reefs frames.\n\nsf_use_s2(FALSE)\ndata_reefs_pts_sc_sf |&gt;\n    st_crop(xmin = 2.5, xmax = 3, ymin = -16.75, ymax = -16.25) |&gt;\n    mutate(Y = plogis(Value)) |&gt;\n    ggplot() +\n    geom_sf(aes(color = Y)) +\n    facet_wrap(~Year, nrow = 2) +\n    scale_color_gradientn(colors = terrain.colors(10)) + \n    coord_sf(crs = 4236, \n             xlim = c(2.5,3), \n             ylim = c(-16.75,-16.25)) +\n    theme_bw(base_size = 12) +\n    theme(axis.title = element_blank())\n\n\n\n\n\n\n\nsf_use_s2(TRUE)\n\n\n\n\ndata_reefs_ma &lt;- data_reefs_hcc |&gt;\n  rename(HCC = Value) |&gt;\n  full_join(data_reefs_sc |&gt; rename(SC = Value)) |&gt;\n  mutate(\n    Total_Avail = 0.8 - plogis(HCC) + plogis(SC),\n    MA = Total_Avail,\n    Value = qlogis(MA)\n  ) |&gt;\n  dplyr::select(-HCC, -SC, -Total_Avail, -MA)\n\ndata_reefs_pts_ma_sf &lt;- data_reefs_ma |&gt;\n  st_as_sf(coords = c(\"Longitude\", \"Latitude\")) |&gt;\n  st_set_crs(st_crs(data_reefs_sf))\nsf_use_s2(FALSE)\ndata_reefs_pts_ma_sf &lt;- data_reefs_pts_ma_sf |&gt;\n  st_intersection(reefs_poly_sf)\nsf_use_s2(TRUE)\n\nGiven that the entire spatial domain is very large and thus it would be very difficult to discern any within individual reef variations from a plot of the entire spatial domain, we will zoom in on a small region for the purposes of illustrating the result of projecting onto the reefs frames.\n\nsf_use_s2(FALSE)\ndata_reefs_pts_ma_sf |&gt;\n  st_crop(xmin = 2.5, xmax = 3, ymin = -16.75, ymax = -16.25) |&gt;\n  mutate(Y = plogis(Value)) |&gt;\n  ggplot() +\n  geom_sf(aes(color = Y)) +\n  facet_wrap(~Year, nrow = 2) +\n  scale_color_gradientn(colors = terrain.colors(10)) +\n  coord_sf(\n    crs = 4236,\n    xlim = c(2.5, 3),\n    ylim = c(-16.75, -16.25)\n  ) +\n  theme_bw(base_size = 12) +\n  theme(axis.title = element_blank())\n\n\n\n\n\n\n\nsf_use_s2(TRUE)\n\n\n\n\n\n\n6 Sampling designs (large scale components)\nBefore going any further, we will combine the HCC, SC and MA data together. Since each of these are projected onto the same grid, we can simply bind the columns together.\n\ndata_reefs_pts_sf &lt;-\n  data_reefs_pts_hcc_sf |&gt;\n  rename(HCC = Value) |&gt;\n  bind_cols(data_reefs_pts_sc_sf |&gt;\n              dplyr::select(SC = Value) |&gt; st_drop_geometry()) |&gt;\n  bind_cols(data_reefs_pts_ma_sf |&gt;\n              dplyr::select(MA = Value) |&gt; st_drop_geometry())\n\nRarely (if ever) can we actually saturate a spatial (and temporal) domain with sampling locations. More typically, monitoring sampling designs comprise either:\n\na limited number of fixed location that are repeatidly visited annually (or perhaps biennially). Such a design is thought to provide more power for exploring the temporal trends, yet the absolute values in the trend are biased towards the actual sampling locations.\nan approximately set number of random locations are visited annually (or perhaps biennially). Such a design is throught to provide more accurace (less biased and thus more representative) estimates of the absolute value of the response (or at least not have a consistent bias), albeit with (potentially) less power for exploring temporal trends.\n\nOn top of this, as already described above, the full spatial domain does not consist only of reef. Indeed, much of the spatial domain is open water. Hence, sampling locations must be selected from within the reef areas.\nSo far we have created a large number of standard features objects and data frames. Going forward, we only need a single one: data_reefs_points_sf. This is a standard features object containing the Latitude and Longitude of the pool of potential sampling locations along with the underlying hard coral, softcoral and macroalgae cover associated with each year (for each location).\nLets start by assuming that we want to sample from two sites within each of 25 locations (reefs) annually.\n\nnLocs &lt;- 25 # Number of 'reefs'\nnSites &lt;- 2 # Number of 'sites' within 'reefs'\n\nIn selecting our annual sampling locations, we don’t want to have multiple locations from the one reef (we will define subsample ‘sites’ within Reefs later). Instead, we want to select 25 Reefs.\n\nFixed locationsRandom locations\n\n\n\nset.seed(seed)\n## Start by randomly selecting nLocs Reefs\nReefs_fixed &lt;- data_reefs_pts_sf |&gt;\n  st_drop_geometry() |&gt;\n  dplyr::select(Reef) |&gt;\n  distinct() |&gt;\n  sample_n(size = nLocs) |&gt;\n  pull(Reef)\n## Then filter to these Reefs before selecting a single location within\n## each of the Reefs\ndata_fixed_locs_sf &lt;- data_reefs_pts_sf |&gt;\n  filter(Reef %in% Reefs_fixed) |&gt;\n  dplyr::select(Reef, geometry) |&gt;\n  distinct(.keep_all = TRUE) |&gt; \n  group_by(Reef) |&gt;\n  sample_n(nSites) |&gt;\n  mutate(Site = paste0(\"S\", 1:n())) |&gt;\n  ungroup() |&gt; \n  st_join(data_reefs_pts_sf |&gt; \n            dplyr::select(-Reef))\n\n\ng &lt;-\n  data_fixed_locs_sf |&gt;\n  pivot_longer(cols = c(HCC, SC, MA),\n    names_to = \"Group\",\n    values_to = \"Value\") |&gt;\n  group_by(Group) |&gt;\n  nest() |&gt;  \n  mutate(G = purrr::map2(.x=data, .y=Group,\n    .f=function(.x, .y) {\n      .x |&gt; st_as_sf() |&gt;\n        mutate(Value = plogis(Value)) |&gt;\n        ggplot() +\n        geom_sf(data = reefs_poly_sf, color = \"lightgray\") +\n        geom_sf(data = spatial_domain, fill = NA) +\n        geom_sf(aes(color = Value, size = Value)) +\n        scale_color_gradientn(colors = terrain.colors(10)) + \n        facet_wrap(~Year, nrow = 2) +\n        coord_sf(crs = 4236) +\n        theme_bw(base_size = 12) +\n        theme(axis.title = element_blank()) +\n        ggtitle(.y)\n    }))\n\ng$G |&gt;\n  patchwork::wrap_plots(nrow = 3) \n\n\n\n\n\n\n\n\n\ndata_reefs_sum &lt;- data_reefs_pts_sf |&gt;\n  st_drop_geometry() |&gt;\n  group_by(Year) |&gt;\n  dplyr::summarise(across(c(HCC, SC, MA), mean)) |&gt;\n  pivot_longer(\n    cols = c(HCC, SC, MA),\n    names_to = \"Group\",\n    values_to = \"Reef_mean\"\n  )\ndata_fixed_locs_sum &lt;- data_fixed_locs_sf |&gt;\n  st_drop_geometry() |&gt;\n  pivot_longer(\n    cols = c(HCC, SC, MA),\n    names_to = \"Group\",\n    values_to = \"Value\"\n  ) |&gt;\n  group_by(Year, Group) |&gt;\n  dplyr::summarise(mean_cl_boot(Value))\n\ng &lt;-\n  data_reefs_sum |&gt;\n  full_join(data_fixed_locs_sum) |&gt;\n  group_by(Group) |&gt;\n  nest() |&gt;\n  mutate(G = purrr::map2(\n    .x = data, .y = Group,\n    .f = function(.x, .y) {\n      .x |&gt;\n        mutate(across(c(y, ymin, ymax, Reef_mean), plogis)) |&gt;\n        ggplot() +\n        geom_line(aes(y = Reef_mean, x = Year, color = \"Reef level\")) +\n        geom_point(aes(y = Reef_mean, x = Year, color = \"Reef level\")) +\n        geom_line(aes(y = y, x = Year, color = \"Site level\")) +\n        geom_ribbon(aes(\n          y = y, x = Year, ymin = ymin, ymax = ymax,\n          fill = \"Site level\"\n        ), alpha = 0.2) +\n        scale_y_continuous(\"Cover\", labels = function(x) x * 100) +\n        scale_color_manual(\"\",\n          breaks = c(\"Reef level\", \"Site level\"),\n          values = c(\"red\", \"blue\"), limits = c(\"Reef level\", \"Site level\")\n        ) +\n        scale_fill_manual(\"\", breaks = c(\"Reef level\", \"Site level\"), values = c(\"red\", \"blue\"), limits = c(\"Reef level\", \"Site level\")) +\n        theme_bw() +\n        ggtitle(.y)\n    }\n  ))\n\ng$G |&gt; patchwork::wrap_plots(nrow = 3)\n\n\n\n\n\n\n\n\n\n\n\nset.seed(seed)\n## Start by randomly selecting nLocs different Reefs per year\nReefs_random &lt;- data_reefs_pts_sf |&gt;\n  st_drop_geometry() |&gt;\n  dplyr::select(Reef, Year) |&gt;\n  distinct() |&gt;\n  group_by(Year) |&gt;\n  sample_n(nLocs) |&gt;\n  ungroup()\n## Then select a single location within each of the reefs per year\ndata_random_locs_sf &lt;- data_reefs_pts_sf |&gt;\n  right_join(Reefs_random) |&gt;\n  group_by(Reef, Year) |&gt;\n  sample_n(nSites) |&gt;\n  mutate(Site = paste0(\"S\", 1:n())) |&gt;\n  arrange(Year, Reef) |&gt;\n  ungroup()\n\n\ng &lt;-\n  data_random_locs_sf |&gt;\n  pivot_longer(\n    cols = c(HCC, SC, MA),\n    names_to = \"Group\",\n    values_to = \"Value\"\n  ) |&gt;\n  group_by(Group) |&gt;\n  nest() |&gt;\n  mutate(G = purrr::map2(.x = data, .y = Group,\n    .f = function(.x, .y) {\n      .x |&gt;\n        st_as_sf() |&gt;\n        mutate(Value = plogis(Value)) |&gt;\n        ggplot() +\n        geom_sf(data = reefs_poly_sf, color = \"lightgray\") +\n        geom_sf(data = spatial_domain, fill = NA) +\n        geom_sf(aes(color = Value, size = Value)) +\n        scale_color_gradientn(colors = terrain.colors(10)) +\n        facet_wrap(~Year, nrow = 2) +\n        coord_sf(crs = 4236) +\n        theme_bw(base_size = 12) +\n        theme(axis.title = element_blank()) +\n        ggtitle(.y)\n    }))\n\ng$G |&gt;\n  patchwork::wrap_plots(nrow = 3)\n\n\n\n\n\n\n\n\n\ndata_reefs_sum &lt;- data_reefs_pts_sf |&gt;\n  st_drop_geometry() |&gt;\n  group_by(Year) |&gt;\n  dplyr::summarise(across(c(HCC, SC, MA), mean)) |&gt;\n  pivot_longer(\n    cols = c(HCC, SC, MA),\n    names_to = \"Group\",\n    values_to = \"Reef_mean\"\n  )\n\ndata_random_locs_sum &lt;- data_random_locs_sf |&gt;\n  st_drop_geometry() |&gt;\n  pivot_longer(cols = c(HCC, SC, MA),\n    names_to = \"Group\",\n    values_to = \"Value\") |&gt;\n  group_by(Year, Group) |&gt;\n  dplyr::summarise(mean_cl_boot(Value))\n\ng &lt;-\n  data_reefs_sum |&gt;\n  full_join(data_random_locs_sum) |&gt;\n  group_by(Group) |&gt;\n  nest() |&gt;\n  mutate(G = purrr::map2(\n    .x = data, .y = Group,\n    .f = function(.x, .y) {\n      .x |&gt;\n        mutate(across(c(y, ymin, ymax, Reef_mean), plogis)) |&gt;\n        ggplot() +\n        geom_line(aes(y = Reef_mean, x = Year, color = \"Reef level\")) +\n        geom_point(aes(y = Reef_mean, x = Year, color = \"Reef level\")) +\n        geom_line(aes(y = y, x = Year, color = \"Site level\")) +\n        geom_ribbon(aes(\n          y = y, x = Year, ymin = ymin, ymax = ymax,\n          fill = \"Site level\"\n        ), alpha = 0.2) +\n        scale_y_continuous(\"Cover\", labels = function(x) x * 100) +\n        scale_color_manual(\"\",\n          breaks = c(\"Reef level\", \"Site level\"),\n          values = c(\"red\", \"blue\"), limits = c(\"Reef level\", \"Site level\")\n        ) +\n        scale_fill_manual(\"\",\n          breaks = c(\"Reef level\", \"Site level\"),\n          values = c(\"red\", \"blue\"), limits = c(\"Reef level\", \"Site level\")\n        ) +\n        theme_bw() +\n        ggtitle(.y)\n    }\n  ))\ng$G |&gt; patchwork::wrap_plots(nrow = 3)\n\n\n\n\n\n\n\n\n\n\n\nConclusions:\n\nthe fixed design yields trends in site means that are more consistent with the trends in the “true” reef level means\nthe accuracy of the fixed design remains relatively constant each year\nbootstrap confidence intervals calculated from the fixed site level data typically contain the “true” mean values of the reef level cover\nthe random design yields estimates of cover that vary in their accuracy each year\nindeed, the estimates of cover from the random design were rarely more accurate than those of the fixed design\nthe estimates from site level data in this section were all based on simple means and bootstrap intervals. It is expected that if the estimates were generated from more sophisticed statistical models, the uncertainty around the fixed design estimates would be narrower than those around the random design estimates.\nhence arguably the fixed design is more suitable for depicting the temporal trends\n\n\n\n7 Finer sampling design components\nNow that we have the location (e.g Reef) cover estimates for each of hard coral, soft coral and macroalgae, we need to spread this out over the fine scale sampling design (e.g sites, transects, photos etc). That is, we want the structure of the data to resemble the structure of real collected within the reefCloud platform.\nFor each of the fixed and random reef/site location designs, we will define a hierarchical structure in which there are a set number (5) of fixed transects within each site, there are a set number photos (100) within each transect and a set number of points (5) within each photo.\nThe hierarchical structures will be illustrated via a diagram at the top of the respective sections (tabs).\nIn order to decompose a Reef mean into (for example) two Sites, for each Site, I will draw two random numbers from a Gaussian distribution with mean of 0 and variance equal to a set parameter (which differs between HCC, SC and MA). These numbers will be added to the Reef value so as to yield to new values (one for each Site). Similarly, to decompose these Site values into Transects values, random numbers will be drawn from a zero mean Gaussian. These calculation are performed on the logit scale before the values are transformed onto a 0-100 (percentage) scale.\nTo decompose into multiple depths, random numbers will be again drawn from a zero-centered Gaussian. These random draws will themselves be normalized and ordered (such that there is a relationship between depth and cover) and added to the 0-100 scaled values.\nFinally, cover values will be converted into total integer counts per transect before being partitioned into Frames (photos) and Points.\n\nNumber_of_transects_per_site &lt;- 5\nDepths &lt;- 2\nNumber_of_frames_per_transect &lt;- 100\nPoints_per_frame &lt;- 5\n\n\n## Note, the following are on the link scale\nhcc_site_sigma &lt;- 0.5        # variability in Sites within Locations\nhcc_transect_sigma &lt;- 0.2    # variability in Transects within Sites\nhcc_sigma &lt;- 0.1             # random noise\n\nsc_site_sigma &lt;- 0.05        # variability in Sites within Locations\nsc_transect_sigma &lt;- 0.02    # variability in Transects within Sites\nsc_sigma &lt;- 0.01             # random noise\n\nma_site_sigma &lt;- 0.5        # variability in Sites within Locations\nma_transect_sigma &lt;- 0.2    # variability in Transects within Sites\nma_sigma &lt;- 0.1             # random noise\n\n\nFixed locationsRandom locations\n\n\n\n\nThe design comprises of:\n\na single region (synthetic)\n25 Reefs (of which Reef118 is the first)\n2 Sites (S1 and ~S2) within each Reef\n5 Transects (T1 – T5) within each Site\neach Transect is sampled annually for 12 Years (2010 – 2021). Colours indicate different years,\nin each Year, 100 photos are collected along each Transect. Although photos represent a spatial scale under Transect, unlike Transect (which is anchored at a fixed location that can be visited annually), it is not possible to guarantee that Photo 1 is in exactly the same location each year - hence Photos are treated as random within the transects.\n5 Points per Photo (not depicted in diagram).\n\n\nset.seed(seed)\ndata_fixed_locs_obs &lt;- data_fixed_locs_sf |&gt;\n  bind_cols(data_fixed_locs_sf |&gt;\n              st_coordinates() |&gt;\n              as.data.frame() |&gt;\n              dplyr::rename(Longitude = X, Latitude = Y)) |&gt;\n  st_drop_geometry() |&gt;\n  as.data.frame() |&gt;\n  group_by(Longitude, Latitude, Reef) |&gt;\n  crossing(\n    Transect = paste0(\"T\",1:Number_of_transects_per_site)) |&gt;\n  group_by(Site, .add = TRUE) |&gt;\n  mutate(\n    SiteEffects_HCC = rnorm(1, 0, hcc_site_sigma),\n    SiteEffects_SC = rnorm(1, 0, sc_site_sigma),\n    SiteEffects_MA = rnorm(1, 0, ma_site_sigma)\n  ) |&gt;\n  group_by(Transect, .add = TRUE) |&gt;\n  mutate(\n    TransectEffects_HCC = rnorm(1, 0, hcc_transect_sigma),\n    TransectEffects_SC = rnorm(1, 0, sc_transect_sigma),\n    TransectEffects_MA = rnorm(1, 0, ma_transect_sigma)\n  ) |&gt;\n  ungroup() |&gt;\n  mutate(\n    HCC1 = HCC + SiteEffects_HCC +\n      TransectEffects_HCC +\n      rnorm(n(), 0, hcc_sigma),\n    HCC2 = 100*plogis(HCC1),\n    SC1 = SC + SiteEffects_SC + TransectEffects_SC +\n      rnorm(n(), 0, sc_sigma),\n    SC2 = 100*plogis(SC1),\n    MA1 = MA + SiteEffects_MA + TransectEffects_MA\n    + rnorm(n(), 0, ma_sigma),\n    MA2 = 100*plogis(MA1)\n  ) |&gt;\n  arrange(Reef, Site, Transect, Year) |&gt;\n  dplyr::select(Reef, Longitude, Latitude, Site,\n    Transect, Year, HCC = HCC2, SC = SC2, MA = MA2) |&gt;\n  mutate(Year = 2021 - max(years) + Year,\n    Date = as.POSIXct(paste0(Year, \"-01-01 14:00:00\")))\n\n\n## The following are on a fold scale.\n## Hence a value of 0.8, indicates that \nDepth_effect_multiplier &lt;- 2\n\ndata_fixed_locs_obs &lt;-\n  data_fixed_locs_obs |&gt;\n  tidyr::crossing(Depth = seq(3, 10, length = Depths)) |&gt;\n  pivot_longer(cols = c(HCC, SC, MA),\n    names_to = \"Group\",\n    values_to = \"Value\") |&gt;\n  group_by(Reef, Site, Transect, Year, Date) |&gt;\n  mutate(Value = Value + rev(sort(Depth_effect_multiplier *\n                                    scale(rnorm(Depths))))) |&gt;\n  ungroup()\ndata_fixed_locs_obs |&gt; head()\n\n# A tibble: 6 × 10\n  Reef   Longitude Latitude Site  Transect  Year Date                Depth Group\n  &lt;chr&gt;      &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;    &lt;dbl&gt; &lt;dttm&gt;              &lt;dbl&gt; &lt;chr&gt;\n1 Reef1…      3.96    -20.3 S1    T1        2010 2010-01-01 14:00:00     3 HCC  \n2 Reef1…      3.96    -20.3 S1    T1        2010 2010-01-01 14:00:00     3 SC   \n3 Reef1…      3.96    -20.3 S1    T1        2010 2010-01-01 14:00:00     3 MA   \n4 Reef1…      3.96    -20.3 S1    T1        2010 2010-01-01 14:00:00    10 HCC  \n5 Reef1…      3.96    -20.3 S1    T1        2010 2010-01-01 14:00:00    10 SC   \n6 Reef1…      3.96    -20.3 S1    T1        2010 2010-01-01 14:00:00    10 MA   \n# ℹ 1 more variable: Value &lt;dbl&gt;\n\nsave(data_fixed_locs_obs, file = \"../data/data_fixed_locs_obs.RData\")\n\n\n## Need to split the percentage cover into point and frames\nload(file = \"../data/data_fixed_locs_obs.RData\")\ndata_fixed_locs_obs &lt;- data_fixed_locs_obs |&gt;\n  group_by(Reef,Site,Transect,Year,Depth,Date) |&gt;\n  mutate(Points = round(Number_of_frames_per_transect *\n                          Points_per_frame *\n                          (Value/sum(Value)),0),\n    Points = ifelse(Points&lt;0, 0, Points)) |&gt;\n  tidyr::uncount(Points) |&gt;\n  sample_n(n(), replace=FALSE) |&gt;\n  mutate(POINT_NO = rep_len(1:Points_per_frame, length = n()),\n    ## FRAME = 1 + cumsum(POINT_NO) %/% (sum(1:Points_per_frame) + 1e-10)) |&gt;\n    FRAME = rep(1:Number_of_frames_per_transect, each=Points_per_frame, length = n())) |&gt;\n  ungroup() \n\n## a |&gt; group_by(Reef, Site, Transect, Year, Depth, Group) |&gt;\n##     summarise(Count = n()) |&gt;\n##     ungroup(Group) |&gt;\n##     mutate(Total=sum(Count),\n##            Cover = Count/Total)\n\nreef_data_synthetic_fixed &lt;-\n  data_fixed_locs_obs |&gt;\n  mutate(\n    project_id = 1,\n    project_name = \"synthetic_fixed\",\n    SITE_NO = str_replace(Site, \"^S\", \"Site \"),\n    TRANSECT_NO = str_replace(Transect, \"^T\", \"Transect \"),\n    site_name = factor(paste(Reef, SITE_NO)),\n    site_id = as.numeric(site_name),\n    site_latitude = Latitude,\n    site_longitude = Longitude,\n    site_depth = Depth,\n    site_country = \"synthetic Country\",\n    site_reef_name = factor(Reef),\n    site_reef_type = NA,\n    site_reef_zone = NA,\n    site_code = NA,\n    site_management = NA,\n    survey_title = factor(paste(Reef, SITE_NO, TRANSECT_NO, format(Date, \"%Y-%m-%d\"))),\n    survey_id = as.numeric(survey_title),\n    survey_start_date = Date,\n    survey_depth = Depth,\n    survey_transect_number = as.numeric(str_replace(TRANSECT_NO, \"Transect \", \"\")),\n    image_name = factor(paste(survey_title, FRAME)),\n    image_id = as.numeric(image_name),\n    image_quality = 100,\n    point_no = POINT_NO,\n    point_id = as.numeric(factor(paste(image_name, POINT_NO))),\n    point_machine_classification = Group\n  ) |&gt;\n  dplyr::select(\n    project_id,\n    project_name,\n    site_id,\n    site_name,\n    site_latitude,\n    site_longitude,\n    site_depth,\n    site_country,\n    site_reef_name,\n    site_reef_type,\n    site_reef_zone,\n    site_code,\n    site_management,\n    survey_id,\n    survey_title,\n    survey_start_date,\n    survey_depth,\n    survey_transect_number,\n    image_id,\n    image_name,\n    image_quality,\n    point_id,\n    point_no,\n    point_machine_classification\n  )\n  ##   PCODE = \"SYNTHETIC-fixed\",\n  ##   ID = 1:n(),\n  ##   CRUISE_CODE = paste0(\"SYNTHETIC\",Year),\n  ##   REEF_NAME = Reef,\n  ##   AIMS_REEF_NAME = Reef,\n  ##   SECTOR = \"synthetic\",\n  ##   LATITUDE = Latitude,\n  ##   LONGITUDE = Longitude,\n  ##   SITE_NO = Site,\n  ##   TRANSECT_NO = Transect,\n  ##   SITE_DEPTH = Depth,\n  ##   REEF_ZONE = \"-\",\n  ##   REPORT_YEAR = Year,\n  ##   SURVEY_DATE = Date,\n  ##   FRAME = paste0(PCODE, \"/\", REEF_NAME, \"/\",\n  ##     REEF_ZONE, \"/\", SITE_NO, \"/\", SITE_DEPTH,\n  ##     \"/\", TRANSECT_NO, \"/\", REPORT_YEAR, \"/\", FRAME),\n  ##   POINT_NO = POINT_NO,\n  ##   FAMILY = NA,\n  ##   GROUP_DESC = Group,\n  ##   REEFPAGE_CATEGORY = paste0(Group,\"_alt\")\n  ## ) |&gt;\n  ## dplyr::select(PCODE, ID, CRUISE_CODE, REEF_NAME,\n  ##   AIMS_REEF_NAME, SECTOR,\n  ##   LATITUDE, LONGITUDE, SITE_NO, TRANSECT_NO, SITE_DEPTH,\n  ##   REEF_ZONE, REPORT_YEAR, SURVEY_DATE, FRAME, POINT_NO,\n  ##   FAMILY, GROUP_DESC, REEFPAGE_CATEGORY)\n\nwrite_csv(reef_data_synthetic_fixed,\n  file = \"../data/reef_data_synthetic_fixed.csv\"\n)\nrmarkdown::paged_table(reef_data_synthetic_fixed |&gt; head()) \n\n\n  \n\n\n\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n\nThe design comprises of:\n\na single region (synthetic)\n25 Reefs (of which Reef118 is the first) in each of 12 Years (2010 – 2021). Colours indicate different years. Note the selection of Reefs differs each Year.\n2 Sites (S1 and ~S2) within each Reef\n5 Transects (T1 – T5) within each Site\n100 photos are collected along each Transect. Although photos represent a spatial scale under Transect, it is not possible to gaurantee that Photo 1 is in exactly the same location each year - hence Photos are treated as random within the transects.\n5 Points per Photo (not depicted in diagram).\n\n\nset.seed(seed)\n \ndata_random_locs_obs &lt;- data_random_locs_sf |&gt;\n  bind_cols(data_random_locs_sf |&gt;\n              st_coordinates() |&gt;\n              as.data.frame() |&gt;\n              dplyr::rename(Longitude = X, Latitude = Y)) |&gt;\n  st_drop_geometry() |&gt;\n  as.data.frame() |&gt;\n  group_by(Longitude, Latitude, Reef) |&gt;\n  crossing(Transect = paste0(\"T\",\n    1:Number_of_transects_per_site)) |&gt;\n  group_by(Site, .add = TRUE) |&gt;\n  mutate(\n    SiteEffects_HCC = rnorm(1, 0, hcc_site_sigma),\n    SiteEffects_SC = rnorm(1, 0, sc_site_sigma),\n    SiteEffects_MA = rnorm(1, 0, ma_site_sigma)\n  ) |&gt;\n  group_by(Transect, .add = TRUE) |&gt;\n  mutate(\n    TransectEffects_HCC = rnorm(1, 0, hcc_transect_sigma),\n    TransectEffects_SC = rnorm(1, 0, sc_transect_sigma),\n    TransectEffects_MA = rnorm(1, 0, ma_transect_sigma)\n  ) |&gt;\n  ungroup() |&gt;\n  mutate(\n    HCC1 = HCC + SiteEffects_HCC + TransectEffects_HCC +\n      rnorm(n(), 0, hcc_sigma),\n    HCC2 = 100*plogis(HCC1),\n    SC1 = SC + SiteEffects_SC + TransectEffects_SC +\n      rnorm(n(), 0, sc_sigma),\n    SC2 = 100*plogis(SC1),\n    MA1 = MA + SiteEffects_MA + TransectEffects_MA +\n      rnorm(n(), 0, ma_sigma),\n    MA2 = 100*plogis(MA1)\n  ) |&gt;\n  arrange(Reef, Site, Transect, Year) |&gt;\n  dplyr::select(Reef, Longitude, Latitude, Site, Transect,\n    Year, HCC = HCC2, SC = SC2, MA = MA2) |&gt;\n  mutate(Year = 2021 - max(years) + Year,\n    Date = as.POSIXct(paste0(Year, \"-01-01 14:00:00\")))\n\n\n## The following are on a fold scale.\n## Hence a value of 0.8, indicates that \nDepth_effect_multiplier &lt;- 2\n\ndata_random_locs_obs &lt;-\n  data_random_locs_obs |&gt;\n  tidyr::crossing(Depth = seq(3, 10, length = Depths)) |&gt;\n  pivot_longer(\n    cols = c(HCC, SC, MA),\n    names_to = \"Group\",\n    values_to = \"Value\"\n  ) |&gt;\n  group_by(Reef, Site, Transect, Year, Date) |&gt;\n  mutate(Value = Value +\n    rev(sort(Depth_effect_multiplier *\n      scale(rnorm(Depths))))) |&gt;\n  ungroup()\nsave(data_random_locs_obs, file = \"../data/data_random_locs_obs.RData\")\n\n\n## Need to split the percentage cover into point and frames\nload(file = \"../data/data_random_locs_obs.RData\")\ndata_random_locs_obs &lt;- data_random_locs_obs |&gt;\n  group_by(Reef, Site, Transect, Year, Depth, Date) |&gt;\n  mutate(\n    Points = round(Number_of_frames_per_transect *\n      Points_per_frame * (Value / sum(Value)), 0),\n    Points = ifelse(Points &lt; 0, 0, Points)\n  ) |&gt;\n  tidyr::uncount(Points) |&gt;\n  sample_n(n(), replace = FALSE) |&gt;\n  mutate(\n    POINT_NO = rep_len(1:Points_per_frame, length = n()),\n    ## FRAME = 1 + cumsum(POINT_NO) %/% (sum(1:Points_per_frame) + 1e-10)) |&gt;\n    FRAME = rep(1:Number_of_frames_per_transect,\n      each = Points_per_frame, length = n()\n    )\n  ) |&gt;\n  ungroup()\n\nreef_data_synthetic_random &lt;-\n  data_random_locs_obs |&gt;\n  mutate(\n    project_id = 1,\n    project_name = \"synthetic_fixed\",\n    SITE_NO = str_replace(Site, \"^S\", \"Site \"),\n    TRANSECT_NO = str_replace(Transect, \"^T\", \"Transect \"),\n    site_name = factor(paste(Reef, SITE_NO)),\n    site_id = as.numeric(site_name),\n    site_latitude = Latitude,\n    site_longitude = Longitude,\n    site_depth = Depth,\n    site_country = \"synthetic Country\",\n    site_reef_name = factor(Reef),\n    site_reef_type = NA,\n    site_reef_zone = NA,\n    site_code = NA,\n    site_management = NA,\n    survey_title = factor(paste(Reef, SITE_NO, TRANSECT_NO, format(Date, \"%Y-%m-%d\"))),\n    survey_id = as.numeric(survey_title),\n    survey_start_date = Date,\n    survey_depth = Depth,\n    survey_transect_number = as.numeric(str_replace(TRANSECT_NO, \"Transect \", \"\")),\n    image_name = factor(paste(survey_title, FRAME)),\n    image_id = as.numeric(image_name),\n    image_quality = 100,\n    point_no = POINT_NO,\n    point_id = as.numeric(factor(paste(image_name, POINT_NO))),\n    point_machine_classification = Group\n  ) |&gt;\n  dplyr::select(\n    project_id,\n    project_name,\n    site_id,\n    site_name,\n    site_latitude,\n    site_longitude,\n    site_depth,\n    site_country,\n    site_reef_name,\n    site_reef_type,\n    site_reef_zone,\n    site_code,\n    site_management,\n    survey_id,\n    survey_title,\n    survey_start_date,\n    survey_depth,\n    survey_transect_number,\n    image_id,\n    image_name,\n    image_quality,\n    point_id,\n    point_no,\n    point_machine_classification\n  )\n  ##   PCODE = \"SYNTHETIC-random\",\n  ##   ID = 1:n(),\n  ##   CRUISE_CODE = paste0(\"SYNTHETIC\", Year),\n  ##   REEF_NAME = Reef,\n  ##   AIMS_REEF_NAME = Reef,\n  ##   SECTOR = \"synthetic\",\n  ##   LATITUDE = Latitude,\n  ##   LONGITUDE = Longitude,\n  ##   SITE_NO = Site,\n  ##   TRANSECT_NO = Transect,\n  ##   SITE_DEPTH = Depth,\n  ##   REEF_ZONE = \"-\",\n  ##   REPORT_YEAR = Year,\n  ##   SURVEY_DATE = Date,\n  ##   FRAME = paste0(PCODE, \"/\", REEF_NAME, \"/\", REEF_ZONE,\n  ##     \"/\", SITE_NO, \"/\", SITE_DEPTH, \"/\", TRANSECT_NO,\n  ##     \"/\", REPORT_YEAR, \"/\", FRAME),\n  ##   POINT_NO = POINT_NO,\n  ##   FAMILY = NA,\n  ##   GROUP_DESC = Group,\n  ##   REEFPAGE_CATEGORY = paste0(Group, \"_alt\")\n  ## ) |&gt;\n  ## dplyr::select(\n  ##   PCODE, ID, CRUISE_CODE, REEF_NAME, AIMS_REEF_NAME, SECTOR,\n  ##   LATITUDE, LONGITUDE, SITE_NO, TRANSECT_NO, SITE_DEPTH,\n  ##   REEF_ZONE, REPORT_YEAR, SURVEY_DATE, FRAME, POINT_NO,\n  ##   FAMILY, GROUP_DESC, REEFPAGE_CATEGORY\n  ## )\n\nwrite_csv(reef_data_synthetic_random,\n  file = \"../data/reef_data_synthetic_random.csv\"\n  )\nrmarkdown::paged_table(reef_data_synthetic_random |&gt; head())\n\n\n  \n\n\n\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\n\n\nThe end",
    "crumbs": [
      "Analyses",
      "Synthetic world and data"
    ]
  }
]